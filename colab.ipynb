{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MiyokoPang/IR/blob/master/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Prerequisite"
      ],
      "metadata": {
        "id": "VAsgLGuHDEpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MiyokoPang/IR.git\n",
        "!pip install pymysql"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63_U84RQBmTZ",
        "outputId": "90406777-3929-4023-e0c6-f90793fe8d1f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IR'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 24 (delta 5), reused 17 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (24/24), 871.37 KiB | 25.63 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3B. FYP_Deep_Learning_Models"
      ],
      "metadata": {
        "id": "YMPBWcOLDNJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Import Libraries and Configuration\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Database\n",
        "from sqlalchemy import create_engine\n",
        "import pymysql\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, Conv1D, GRU, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "print(\"Libraries imported.\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "# Database Configuration\n",
        "db_config = {\n",
        "    'host': '127.0.0.1',\n",
        "    'user': 'root',\n",
        "    'password': '',\n",
        "    'database': 'trading_system'\n",
        "}\n",
        "db_url = f\"mysql+pymysql://{db_config['user']}:{db_config['password']}@{db_config['host']}/{db_config['database']}\"\n",
        "engine = create_engine(db_url)\n",
        "\n",
        "# Path Definition\n",
        "base_dir = Path.cwd().parent\n",
        "output_dir = base_dir / \"content\" / \"IR\" / \"4_Results\" / \"Deep_Learning_Results\"\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Results will be saved to: {output_dir}\")"
      ],
      "metadata": {
        "id": "o3CMhxUrDcDB",
        "outputId": "29d99214-767d-41ab-9111-8a3284e3d7f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported.\n",
            "TensorFlow version: 2.19.0\n",
            "Results will be saved to: /content/IR/4_Results/Deep_Learning_Results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Load Sequence Data\n",
        "print(\"Loading sequence data from database.\")\n",
        "\n",
        "try:\n",
        "    df = pd.read_sql(\"SELECT * FROM data_daily_merged\", con=engine)\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df = df.sort_values(by=['ticker', 'date'])\n",
        "\n",
        "    print(f\"Successfully loaded {len(df):,} rows.\")\n",
        "    print(f\"Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
        "    print(f\"Number of unique tickers: {df['ticker'].nunique()}\")\n",
        "\n",
        "    print(\"\\nAvailable columns:\")\n",
        "    print(df.columns.tolist())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERROR loading data: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "5n2fsLYMDnAD",
        "outputId": "d3d5f202-aa16-48f8-a532-69c90517e5f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading sequence data from database.\n",
            "ERROR loading data: (pymysql.err.OperationalError) (2003, \"Can't connect to MySQL server on '127.0.0.1' ([Errno 111] Connection refused)\")\n",
            "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OperationalError",
          "evalue": "(pymysql.err.OperationalError) (2003, \"Can't connect to MySQL server on '127.0.0.1' ([Errno 111] Connection refused)\")\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pymysql/connections.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self, sock)\u001b[0m\n\u001b[1;32m    660\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m                             sock = socket.create_connection(\n\u001b[0m\u001b[1;32m    662\u001b[0m                                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mExceptionGroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"create_connection failed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dbapi_connection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mdialect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaded_dbapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36mraw_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3300\u001b[0m         \"\"\"\n\u001b[0;32m-> 3301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/pool/base.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \"\"\"\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ConnectionFairy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/pool/base.py\u001b[0m in \u001b[0;36m_checkout\u001b[0;34m(cls, pool, threadconns, fairy)\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfairy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m             \u001b[0mfairy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ConnectionRecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/pool/base.py\u001b[0m in \u001b[0;36mcheckout\u001b[0;34m(cls, pool)\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/pool/impl.py\u001b[0m in \u001b[0;36m_do_get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_reraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dec_overflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/util/langhelpers.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# remove potential circular references\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/pool/impl.py\u001b[0m in \u001b[0;36m_do_get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/pool/base.py\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ConnectionRecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/pool/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pool, connect)\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/pool/base.py\u001b[0m in \u001b[0;36m__connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_reraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m                 \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error on connect(): %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/util/langhelpers.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# remove potential circular references\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/pool/base.py\u001b[0m in \u001b[0;36m__connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    894\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarttime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbapi_connection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Created new connection %r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/create.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(connection_record)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdialect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/default.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self, *cargs, **cparams)\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;31m# inherits the docstring from interfaces.Dialect.connect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaded_dbapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcparams\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]  # NOQA: E501\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pymysql/connections.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, user, password, host, database, unix_socket, port, charset, collation, sql_mode, read_default_file, conv, use_unicode, client_flag, cursorclass, init_command, connect_timeout, read_default_group, autocommit, local_infile, max_allowed_packet, defer_connect, auth_plugin_map, read_timeout, write_timeout, bind_address, binary_prefix, program_name, server_public_key, ssl, ssl_ca, ssl_cert, ssl_disabled, ssl_key, ssl_key_password, ssl_verify_cert, ssl_verify_identity, compress, named_pipe, passwd, db)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pymysql/connections.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self, sock)\u001b[0m\n\u001b[1;32m    722\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperationalError\u001b[0m: (2003, \"Can't connect to MySQL server on '127.0.0.1' ([Errno 111] Connection refused)\")",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-52074910.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM data_daily_merged\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ticker'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mdtype_backend\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mpandasSQL_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpandas_sql\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpandas_sql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSQLiteDatabase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m             return pandas_sql.read_query(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mpandasSQL_builder\u001b[0;34m(con, schema, need_transaction)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msqlalchemy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqlalchemy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConnectable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSQLDatabase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_transaction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m     \u001b[0madbc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"adbc_driver_manager.dbapi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, con, schema, need_transaction)\u001b[0m\n\u001b[1;32m   1634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEngine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1636\u001b[0;31m             \u001b[0mcon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1637\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mneed_transaction\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_transaction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3275\u001b[0m         \"\"\"\n\u001b[1;32m   3276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mraw_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mPoolProxiedConnection\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dbapi_connection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mdialect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaded_dbapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 Connection._handle_dbapi_exception_noconnection(\n\u001b[0m\u001b[1;32m    146\u001b[0m                     \u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdialect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_handle_dbapi_exception_noconnection\u001b[0;34m(cls, e, dialect, engine, is_disconnect, invalidate_pool_on_disconnect, is_pre_ping)\u001b[0m\n\u001b[1;32m   2438\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mshould_wrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2439\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0msqlalchemy_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2440\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0msqlalchemy_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2442\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dbapi_connection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mdialect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaded_dbapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 Connection._handle_dbapi_exception_noconnection(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36mraw_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m         \"\"\"\n\u001b[0;32m-> 3301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/pool/base.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \"\"\"\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ConnectionFairy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_return_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConnectionPoolEntry\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/pool/base.py\u001b[0m in \u001b[0;36m_checkout\u001b[0;34m(cls, pool, threadconns, fairy)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     ) -> _ConnectionFairy:\n\u001b[1;32m   1263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfairy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m             \u001b[0mfairy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ConnectionRecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mthreadconns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/pool/base.py\u001b[0m in \u001b[0;36mcheckout\u001b[0;34m(cls, pool)\u001b[0m\n\u001b[1;32m    709\u001b[0m             \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConnectionRecord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/pool/impl.py\u001b[0m in \u001b[0;36m_do_get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_reraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dec_overflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/util/langhelpers.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mexc_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# remove potential circular references\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# remove potential circular references\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/pool/impl.py\u001b[0m in \u001b[0;36m_do_get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inc_overflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_reraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/pool/base.py\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;34m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ConnectionRecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     def _invalidate(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/pool/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pool, connect)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/pool/base.py\u001b[0m in \u001b[0;36m__connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_reraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m                 \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error on connect(): %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/util/langhelpers.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mexc_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# remove potential circular references\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# remove potential circular references\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/pool/base.py\u001b[0m in \u001b[0;36m__connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarttime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbapi_connection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Created new connection %r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/create.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(connection_record)\u001b[0m\n\u001b[1;32m    659\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdialect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0mcreator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpop_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"creator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/default.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self, *cargs, **cparams)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcparams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDBAPIConnection\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;31m# inherits the docstring from interfaces.Dialect.connect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaded_dbapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcparams\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]  # NOQA: E501\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_connect_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mURL\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mConnectArgsType\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pymysql/connections.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, user, password, host, database, unix_socket, port, charset, collation, sql_mode, read_default_file, conv, use_unicode, client_flag, cursorclass, init_command, connect_timeout, read_default_group, autocommit, local_infile, max_allowed_packet, defer_connect, auth_plugin_map, read_timeout, write_timeout, bind_address, binary_prefix, program_name, server_public_key, ssl, ssl_ca, ssl_cert, ssl_disabled, ssl_key, ssl_key_password, ssl_verify_cert, ssl_verify_identity, compress, named_pipe, passwd, db)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pymysql/connections.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self, sock)\u001b[0m\n\u001b[1;32m    721\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0;31m# If e is neither DatabaseError or IOError, It's a bug.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperationalError\u001b[0m: (pymysql.err.OperationalError) (2003, \"Can't connect to MySQL server on '127.0.0.1' ([Errno 111] Connection refused)\")\n(Background on this error at: https://sqlalche.me/e/20/e3q8)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Define Feature Sets for Deep Learning\n",
        "print(\"Defining feature sets for sequence models.\")\n",
        "\n",
        "# 1. Price_Only - Technical features\n",
        "price_features = ['close', 'volume']\n",
        "\n",
        "# 2. Price_Base_Sentiment - Price + basic sentiment scores\n",
        "base_sentiment_features = price_features + [\n",
        "    'textblob_polarity',\n",
        "    'vader_compound',\n",
        "    'finbert_compound'\n",
        "]\n",
        "\n",
        "# 3. Price_Full_Sentiment - Price + all sentiment features\n",
        "full_sentiment_features = price_features + [\n",
        "    'textblob_polarity',\n",
        "    'vader_compound',\n",
        "    'finbert_compound',\n",
        "    'news_count',\n",
        "    'has_news',\n",
        "    'days_since_news',\n",
        "    'sentiment_freshness',\n",
        "    'textblob_polarity_volatility',\n",
        "    'vader_compound_volatility',\n",
        "    'finbert_compound_volatility'\n",
        "]\n",
        "\n",
        "feature_sets = {\n",
        "    \"Price_Only\": price_features,\n",
        "    \"Price_Base_Sentiment\": base_sentiment_features,\n",
        "    \"Price_Full_Sentiment\": full_sentiment_features\n",
        "}\n",
        "\n",
        "print(f\"Total feature sets: {len(feature_sets)}\")\n",
        "print(f\"- Price_Only: {len(price_features)} features\")\n",
        "print(f\"- Price_Base_Sentiment: {len(base_sentiment_features)} features\")\n",
        "print(f\"- Price_Full_Sentiment: {len(full_sentiment_features)} features\")"
      ],
      "metadata": {
        "id": "cVhckTxaDpFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Sequence Generation Functions\n",
        "def create_sequences(data, feature_cols, target_col='close', sequence_length=60):\n",
        "    \"\"\"\n",
        "    Create sequences for time series prediction.\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "\n",
        "    for i in range(sequence_length, len(data)):\n",
        "        X.append(data[feature_cols].iloc[i-sequence_length:i].values)\n",
        "        current_close = data[target_col].iloc[i-1]\n",
        "        next_close = data[target_col].iloc[i]\n",
        "        return_val = (next_close - current_close) / current_close\n",
        "        y.append(return_val)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def prepare_data_for_ticker(ticker, df, feature_cols, sequence_length=60, train_ratio=0.8):\n",
        "    \"\"\"\n",
        "    Prepare train/test sequences for a single ticker.\n",
        "    \"\"\"\n",
        "    ticker_df = df[df['ticker'] == ticker].copy()\n",
        "    ticker_df = ticker_df.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "    if len(ticker_df) < sequence_length + 50:\n",
        "        return None\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    ticker_df[feature_cols] = scaler.fit_transform(ticker_df[feature_cols])\n",
        "\n",
        "    X, y = create_sequences(ticker_df, feature_cols, sequence_length=sequence_length)\n",
        "    split_idx = int(len(X) * train_ratio)\n",
        "\n",
        "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "    if len(X_test) < 5:\n",
        "        return None\n",
        "\n",
        "    return {'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test}\n",
        "\n",
        "print(\"Sequence generation functions defined.\")"
      ],
      "metadata": {
        "id": "lUuKwPPaDpL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Model Architecture Builders\n",
        "def build_lstm_model(input_shape, units=50):\n",
        "    model = Sequential([\n",
        "        LSTM(units, input_shape=input_shape, return_sequences=False),\n",
        "        Dropout(0.2),\n",
        "        Dense(25, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "def build_bilstm_model(input_shape, units=50):\n",
        "    model = Sequential([\n",
        "        Bidirectional(LSTM(units, return_sequences=False), input_shape=input_shape),\n",
        "        Dropout(0.2),\n",
        "        Dense(25, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "def build_cnn1d_model(input_shape, filters=64):\n",
        "    model = Sequential([\n",
        "        Conv1D(filters=filters, kernel_size=3, activation='relu', input_shape=input_shape),\n",
        "        Conv1D(filters=filters//2, kernel_size=3, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        keras.layers.Flatten(),\n",
        "        Dense(50, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "def build_gru_model(input_shape, units=50):\n",
        "    model = Sequential([\n",
        "        GRU(units, input_shape=input_shape, return_sequences=False),\n",
        "        Dropout(0.2),\n",
        "        Dense(25, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "model_builders = {\n",
        "    'LSTM': build_lstm_model,\n",
        "    'BiLSTM': build_bilstm_model,\n",
        "    'CNN1D': build_cnn1d_model,\n",
        "    'GRU': build_gru_model\n",
        "}\n",
        "\n",
        "print(\"Model architecture builders defined.\")\n",
        "print(f\"Available architectures: {list(model_builders.keys())}\")"
      ],
      "metadata": {
        "id": "aWOl-9lcDvh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Helper Functions\n",
        "def calculate_metrics(y_true, y_pred, model_name, ticker, feature_set_name):\n",
        "    mask = ~(np.isnan(y_true) | np.isnan(y_pred) | np.isinf(y_true) | np.isinf(y_pred))\n",
        "    y_true_clean = y_true[mask]\n",
        "    y_pred_clean = y_pred[mask]\n",
        "\n",
        "    if len(y_true_clean) == 0:\n",
        "        return None\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))\n",
        "    mae = mean_absolute_error(y_true_clean, y_pred_clean)\n",
        "    r2 = r2_score(y_true_clean, y_pred_clean)\n",
        "    correct_direction = np.sign(y_true_clean) == np.sign(y_pred_clean)\n",
        "    directional_accuracy = np.mean(correct_direction) * 100\n",
        "\n",
        "    return {\n",
        "        'ticker': ticker,\n",
        "        'model': f\"{model_name}_{feature_set_name}\",\n",
        "        'feature_set': feature_set_name,\n",
        "        'rmse': rmse,\n",
        "        'mae': mae,\n",
        "        'r2': r2,\n",
        "        'directional_accuracy': directional_accuracy\n",
        "    }\n",
        "\n",
        "print(\"Helper functions defined.\")"
      ],
      "metadata": {
        "id": "OgN6obquDwQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Main Training Loop (OPTIMIZED - Prevents Retracing)\n",
        "print(\"Starting Deep Learning Model Training Loop.\")\n",
        "\n",
        "all_results = []\n",
        "sequence_length = 60\n",
        "epochs = 50\n",
        "batch_size = 32\n",
        "\n",
        "# Get ticker list\n",
        "ticker_list = sorted(df['ticker'].unique())\n",
        "print(f\"Total available tickers: {len(ticker_list)}\")\n",
        "\n",
        "# Setting ticker limit\n",
        "ticker_list = ticker_list[:100]  # Reduced to 100 for faster testing\n",
        "print(f\"Training on: {len(ticker_list)} tickers.\")\n",
        "\n",
        "# Early stopping callback\n",
        "early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True, verbose=0)\n",
        "\n",
        "# Prefilter tickers by data availability\n",
        "print(\"\\nPre-filtering tickers by data availability.\")\n",
        "valid_tickers = []\n",
        "for ticker in tqdm(ticker_list, desc=\"Validating Tickers\"):\n",
        "    ticker_df = df[df['ticker'] == ticker]\n",
        "    if len(ticker_df) >= sequence_length + 100:  # Ensure enough data\n",
        "        valid_tickers.append(ticker)\n",
        "\n",
        "print(f\"Valid tickers after filtering: {len(valid_tickers)} (removed {len(ticker_list) - len(valid_tickers)} with insufficient data)\")\n",
        "ticker_list = valid_tickers\n",
        "\n",
        "# Adjust model weights rather than rebuilding\n",
        "print(\"\\nPre-building model templates.\")\n",
        "model_templates = {}\n",
        "\n",
        "for set_name, feature_cols in feature_sets.items():\n",
        "    n_features = len(feature_cols)\n",
        "    input_shape = (sequence_length, n_features)\n",
        "\n",
        "    for model_name, builder_func in model_builders.items():\n",
        "        cache_key = f\"{model_name}_{set_name}\"\n",
        "        # Build model and save initial weights\n",
        "        model = builder_func(input_shape)\n",
        "        initial_weights = model.get_weights()\n",
        "\n",
        "        model_templates[cache_key] = {\n",
        "            'model': model,\n",
        "            'initial_weights': initial_weights,\n",
        "            'input_shape': input_shape\n",
        "        }\n",
        "\n",
        "print(f\"Created {len(model_templates)} model templates.\")\n",
        "print(f\"Expected total models: {len(ticker_list) * 12} = {len(ticker_list) * 12}\")\n",
        "\n",
        "# Training Loop\n",
        "for ticker in tqdm(ticker_list, desc=\"Processing Tickers\"):\n",
        "\n",
        "    for set_name, feature_cols in feature_sets.items():\n",
        "\n",
        "        data = prepare_data_for_ticker(\n",
        "            ticker, df, feature_cols,\n",
        "            sequence_length=sequence_length\n",
        "        )\n",
        "\n",
        "        if data is None:\n",
        "            continue\n",
        "\n",
        "        X_train = data['X_train']\n",
        "        X_test = data['X_test']\n",
        "        y_train = data['y_train']\n",
        "        y_test = data['y_test']\n",
        "\n",
        "        # Skip if unexpected shapes\n",
        "        if len(X_test) < 5 or X_train.shape[1] != sequence_length:\n",
        "            continue\n",
        "\n",
        "        for model_name in model_builders.keys():\n",
        "            try:\n",
        "                cache_key = f\"{model_name}_{set_name}\"\n",
        "                template = model_templates[cache_key]\n",
        "                model = template['model']\n",
        "\n",
        "                # Reset to initial weights\n",
        "                model.set_weights(template['initial_weights'])\n",
        "\n",
        "                # Train\n",
        "                history = model.fit(\n",
        "                    X_train, y_train,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    callbacks=[early_stop],\n",
        "                    verbose=0,\n",
        "                    validation_split=0.1\n",
        "                )\n",
        "\n",
        "                # Predict\n",
        "                y_pred = model.predict(X_test, verbose=0).flatten()\n",
        "\n",
        "                # Calculate metrics\n",
        "                metrics = calculate_metrics(\n",
        "                    y_test, y_pred,\n",
        "                    model_name, ticker, set_name\n",
        "                )\n",
        "\n",
        "                if metrics:\n",
        "                    all_results.append(metrics)\n",
        "\n",
        "                del history\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "    # Clear session every 10 tickers to prevent memory buildup\n",
        "    if (ticker_list.index(ticker) + 1) % 10 == 0:\n",
        "        tf.keras.backend.clear_session()\n",
        "\n",
        "print(f\"\\nDeep Learning Training Complete. Total results: {len(all_results)}\")\n",
        "print(f\"Expected: ~{len(ticker_list) * 12}, Actual: {len(all_results)}\")\n",
        "\n",
        "# Final cleanup\n",
        "for template in model_templates.values():\n",
        "    del template['model']\n",
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "A4CvQgGfD0V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Save Results\n",
        "print(\"Saving results.\")\n",
        "\n",
        "if all_results:\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "    results_df['timestamp'] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    csv_path = output_dir / \"03B_Deep_Learning_Performance.csv\"\n",
        "    results_df.to_csv(csv_path, index=False)\n",
        "    print(f\"Successfully saved {len(results_df)} results to {csv_path}\")\n",
        "\n",
        "    print(\"Saving results to 'results_deep_learning_models' table in database.\")\n",
        "    try:\n",
        "        results_df.to_sql(\n",
        "            'results_deep_learning_models',\n",
        "            con=engine,\n",
        "            if_exists='replace',\n",
        "            index=False,\n",
        "            chunksize=1000\n",
        "        )\n",
        "        print(\"Successfully wrote results to database!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing results to database: {e}\")\n",
        "else:\n",
        "    print(\"No results were generated. Check for errors.\")"
      ],
      "metadata": {
        "id": "7mNR9J6sD1Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Continue Training on Additional Tickers (Batch 2)\n",
        "print(\"Continuing Deep Learning Training (Batch 2).\")\n",
        "\n",
        "# Load existing results to avoid retraining\n",
        "existing_results_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
        "trained_tickers = set(existing_results_df['ticker'].unique())\n",
        "print(f\"Already trained on {len(trained_tickers)} tickers.\")\n",
        "\n",
        "# Get next batch of tickers\n",
        "all_tickers = sorted(df['ticker'].unique())\n",
        "remaining_tickers = [t for t in all_tickers if t not in trained_tickers]\n",
        "print(f\"Remaining tickers available: {len(remaining_tickers)}\")\n",
        "\n",
        "# Set batch size (start conservative)\n",
        "BATCH_SIZE = 100  # Start with 100, can increase to 150-200 if stable\n",
        "next_batch = remaining_tickers[:BATCH_SIZE]\n",
        "print(f\"Training on next {len(next_batch)} tickers.\")\n",
        "\n",
        "# Pre-filter for data availability\n",
        "valid_tickers = []\n",
        "for ticker in tqdm(next_batch, desc=\"Validating Tickers\"):\n",
        "    ticker_df = df[df['ticker'] == ticker]\n",
        "    if len(ticker_df) >= sequence_length + 100:\n",
        "        valid_tickers.append(ticker)\n",
        "\n",
        "print(f\"Valid tickers: {len(valid_tickers)}\")\n",
        "\n",
        "# Rebuild model templates (same as before)\n",
        "print(\"\\nRebuilding model templates.\")\n",
        "model_templates = {}\n",
        "\n",
        "for set_name, feature_cols in feature_sets.items():\n",
        "    n_features = len(feature_cols)\n",
        "    input_shape = (sequence_length, n_features)\n",
        "\n",
        "    for model_name, builder_func in model_builders.items():\n",
        "        cache_key = f\"{model_name}_{set_name}\"\n",
        "        model = builder_func(input_shape)\n",
        "        initial_weights = model.get_weights()\n",
        "\n",
        "        model_templates[cache_key] = {\n",
        "            'model': model,\n",
        "            'initial_weights': initial_weights,\n",
        "            'input_shape': input_shape\n",
        "        }\n",
        "\n",
        "print(f\"Created {len(model_templates)} model templates.\")\n",
        "\n",
        "# Training Loop\n",
        "batch_results = []\n",
        "\n",
        "for ticker in tqdm(valid_tickers, desc=\"Processing Additional Tickers\"):\n",
        "\n",
        "    for set_name, feature_cols in feature_sets.items():\n",
        "\n",
        "        data = prepare_data_for_ticker(\n",
        "            ticker, df, feature_cols,\n",
        "            sequence_length=sequence_length\n",
        "        )\n",
        "\n",
        "        if data is None:\n",
        "            continue\n",
        "\n",
        "        X_train = data['X_train']\n",
        "        X_test = data['X_test']\n",
        "        y_train = data['y_train']\n",
        "        y_test = data['y_test']\n",
        "\n",
        "        if len(X_test) < 5 or X_train.shape[1] != sequence_length:\n",
        "            continue\n",
        "\n",
        "        for model_name in model_builders.keys():\n",
        "            try:\n",
        "                cache_key = f\"{model_name}_{set_name}\"\n",
        "                template = model_templates[cache_key]\n",
        "                model = template['model']\n",
        "\n",
        "                model.set_weights(template['initial_weights'])\n",
        "\n",
        "                history = model.fit(\n",
        "                    X_train, y_train,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    callbacks=[early_stop],\n",
        "                    verbose=0,\n",
        "                    validation_split=0.1\n",
        "                )\n",
        "\n",
        "                y_pred = model.predict(X_test, verbose=0).flatten()\n",
        "\n",
        "                metrics = calculate_metrics(\n",
        "                    y_test, y_pred,\n",
        "                    model_name, ticker, set_name\n",
        "                )\n",
        "\n",
        "                if metrics:\n",
        "                    batch_results.append(metrics)\n",
        "\n",
        "                del history\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "    # Clear session every 10 tickers\n",
        "    if (valid_tickers.index(ticker) + 1) % 10 == 0:\n",
        "        tf.keras.backend.clear_session()\n",
        "\n",
        "print(f\"\\nBatch 2 Training Complete. New results: {len(batch_results)}\")\n",
        "\n",
        "# Cleanup\n",
        "for template in model_templates.values():\n",
        "    del template['model']\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Append to existing results\n",
        "if batch_results:\n",
        "    new_results_df = pd.DataFrame(batch_results)\n",
        "    new_results_df['timestamp'] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    # Combine with existing\n",
        "    combined_df = pd.concat([existing_results_df, new_results_df], ignore_index=True)\n",
        "\n",
        "    # Save updated CSV\n",
        "    combined_df.to_csv(output_dir / \"03B_Deep_Learning_Performance.csv\", index=False)\n",
        "    print(f\"Updated results file. Total results: {len(combined_df)}\")\n",
        "\n",
        "    # Update database\n",
        "    try:\n",
        "        combined_df.to_sql(\n",
        "            'results_deep_learning_models',\n",
        "            con=engine,\n",
        "            if_exists='replace',\n",
        "            index=False,\n",
        "            chunksize=1000\n",
        "        )\n",
        "        print(\"Successfully updated database!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Database update error: {e}\")\n",
        "else:\n",
        "    print(\"No new results generated.\")"
      ],
      "metadata": {
        "id": "z9z7nda2D5TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Fix Database Upload (Run this first)\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "print(\"Attempting to fix database connection and upload existing results...\")\n",
        "\n",
        "# 1. Force close the existing bad connection\n",
        "try:\n",
        "    engine.dispose()\n",
        "    print(\"Old engine disposed.\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# 2. Re-create the engine\n",
        "db_url = f\"mysql+pymysql://{db_config['user']}:{db_config['password']}@{db_config['host']}/{db_config['database']}\"\n",
        "engine = create_engine(db_url)\n",
        "print(\"New database engine created.\")\n",
        "\n",
        "# 3. Load the CSV that was successfully saved on disk\n",
        "try:\n",
        "    csv_path = output_dir / \"03B_Deep_Learning_Performance.csv\"\n",
        "    current_results = pd.read_csv(csv_path)\n",
        "    print(f\"Loaded {len(current_results)} rows from CSV.\")\n",
        "\n",
        "    # 4. Upload to Database\n",
        "    current_results.to_sql(\n",
        "        'results_deep_learning_models',\n",
        "        con=engine,\n",
        "        if_exists='replace',\n",
        "        index=False,\n",
        "        chunksize=1000\n",
        "    )\n",
        "    print(\"SUCCESS: All results successfully uploaded to the database.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"CRITICAL ERROR: Could not upload to database. Error: {e}\")"
      ],
      "metadata": {
        "id": "mZ1OY4smD59-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Train Batch 3 (Tickers 200-300)\n",
        "print(\"Starting Batch 3 Training.\")\n",
        "\n",
        "# 1. Refresh the list of what is already finished\n",
        "existing_results_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
        "trained_tickers = set(existing_results_df['ticker'].unique())\n",
        "print(f\"Currently trained on {len(trained_tickers)} tickers.\")\n",
        "\n",
        "# 2. Calculate the next batch\n",
        "all_tickers = sorted(df['ticker'].unique())\n",
        "remaining_tickers = [t for t in all_tickers if t not in trained_tickers]\n",
        "\n",
        "# Grab the next 100\n",
        "BATCH_SIZE = 100\n",
        "next_batch = remaining_tickers[:BATCH_SIZE]\n",
        "print(f\"Targeting next {len(next_batch)} tickers.\")\n",
        "\n",
        "# 3. Pre-filter for data availability\n",
        "valid_tickers = []\n",
        "for ticker in tqdm(next_batch, desc=\"Validating Tickers\"):\n",
        "    ticker_df = df[df['ticker'] == ticker]\n",
        "    if len(ticker_df) >= sequence_length + 100:\n",
        "        valid_tickers.append(ticker)\n",
        "\n",
        "print(f\"Valid tickers to train: {len(valid_tickers)}\")\n",
        "\n",
        "# 4. Rebuild Templates (Fresh start for memory safety)\n",
        "tf.keras.backend.clear_session()\n",
        "model_templates = {}\n",
        "for set_name, feature_cols in feature_sets.items():\n",
        "    input_shape = (sequence_length, len(feature_cols))\n",
        "    for model_name, builder_func in model_builders.items():\n",
        "        model = builder_func(input_shape)\n",
        "        model_templates[f\"{model_name}_{set_name}\"] = {\n",
        "            'model': model,\n",
        "            'initial_weights': model.get_weights(),\n",
        "            'input_shape': input_shape\n",
        "        }\n",
        "\n",
        "# 5. Training Loop\n",
        "batch_results = []\n",
        "for ticker in tqdm(valid_tickers, desc=\"Training Batch 3\"):\n",
        "    for set_name, feature_cols in feature_sets.items():\n",
        "        data = prepare_data_for_ticker(ticker, df, feature_cols, sequence_length=sequence_length)\n",
        "        if data is None: continue\n",
        "\n",
        "        # Skip if validation set is too small\n",
        "        if len(data['X_test']) < 5: continue\n",
        "\n",
        "        for model_name in model_builders.keys():\n",
        "            try:\n",
        "                cache_key = f\"{model_name}_{set_name}\"\n",
        "                template = model_templates[cache_key]\n",
        "                model = template['model']\n",
        "                model.set_weights(template['initial_weights']) # Reset weights\n",
        "\n",
        "                history = model.fit(\n",
        "                    data['X_train'], data['y_train'],\n",
        "                    epochs=50, batch_size=32, callbacks=[early_stop],\n",
        "                    verbose=0, validation_split=0.1\n",
        "                )\n",
        "\n",
        "                y_pred = model.predict(data['X_test'], verbose=0).flatten()\n",
        "                metrics = calculate_metrics(data['y_test'], y_pred, model_name, ticker, set_name)\n",
        "\n",
        "                if metrics: batch_results.append(metrics)\n",
        "                del history\n",
        "            except: continue\n",
        "\n",
        "    # Memory management\n",
        "    if (valid_tickers.index(ticker) + 1) % 10 == 0:\n",
        "        tf.keras.backend.clear_session()\n",
        "\n",
        "# 6. Save and Upload\n",
        "print(f\"Batch 3 Complete. Generated {len(batch_results)} new results.\")\n",
        "\n",
        "if batch_results:\n",
        "    # Append to CSV\n",
        "    new_results_df = pd.DataFrame(batch_results)\n",
        "    new_results_df['timestamp'] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    # Reload full CSV to be safe, append, and save back\n",
        "    full_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
        "    combined_df = pd.concat([full_df, new_results_df], ignore_index=True)\n",
        "    combined_df.to_csv(output_dir / \"03B_Deep_Learning_Performance.csv\", index=False)\n",
        "    print(f\"CSV updated. Total rows: {len(combined_df)}\")\n",
        "\n",
        "    # Upload to DB\n",
        "    try:\n",
        "        # Re-dispose engine one last time to be safe\n",
        "        engine.dispose()\n",
        "        engine = create_engine(db_url)\n",
        "\n",
        "        combined_df.to_sql(\n",
        "            'results_deep_learning_models',\n",
        "            con=engine,\n",
        "            if_exists='replace',\n",
        "            index=False,\n",
        "            chunksize=1000\n",
        "        )\n",
        "        print(\"Database successfully updated with Batch 3.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error uploading Batch 3 to DB: {e}\")\n",
        "\n",
        "# Cleanup\n",
        "for template in model_templates.values(): del template['model']\n",
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "6Groa8qfD9-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Train Batch 4\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "import tensorflow as tf\n",
        "from tqdm.notebook import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"Starting Batch 4 Training...\")\n",
        "\n",
        "# 1. Refresh the list of what is already finished\n",
        "# We reload the CSV to ensure we skip everything done in Batch 3\n",
        "existing_results_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
        "trained_tickers = set(existing_results_df['ticker'].unique())\n",
        "print(f\"Currently trained on {len(trained_tickers)} tickers.\")\n",
        "\n",
        "# 2. Calculate the next batch\n",
        "all_tickers = sorted(df['ticker'].unique())\n",
        "remaining_tickers = [t for t in all_tickers if t not in trained_tickers]\n",
        "\n",
        "if not remaining_tickers:\n",
        "    print(\"No tickers left to train!\")\n",
        "else:\n",
        "    # Grab the next 100\n",
        "    BATCH_SIZE = 100\n",
        "    next_batch = remaining_tickers[:BATCH_SIZE]\n",
        "    print(f\"Targeting next {len(next_batch)} tickers (Batch 4).\")\n",
        "\n",
        "    # 3. Pre-filter for data availability\n",
        "    valid_tickers = []\n",
        "    for ticker in tqdm(next_batch, desc=\"Validating Batch 4 Tickers\"):\n",
        "        ticker_df = df[df['ticker'] == ticker]\n",
        "        if len(ticker_df) >= sequence_length + 100:\n",
        "            valid_tickers.append(ticker)\n",
        "\n",
        "    print(f\"Valid tickers to train: {len(valid_tickers)}\")\n",
        "\n",
        "    # 4. Rebuild Templates (Clear session to prevent memory leaks)\n",
        "    tf.keras.backend.clear_session()\n",
        "    model_templates = {}\n",
        "    for set_name, feature_cols in feature_sets.items():\n",
        "        input_shape = (sequence_length, len(feature_cols))\n",
        "        for model_name, builder_func in model_builders.items():\n",
        "            model = builder_func(input_shape)\n",
        "            model_templates[f\"{model_name}_{set_name}\"] = {\n",
        "                'model': model,\n",
        "                'initial_weights': model.get_weights(),\n",
        "                'input_shape': input_shape\n",
        "            }\n",
        "\n",
        "    # 5. Training Loop\n",
        "    batch_results = []\n",
        "    for ticker in tqdm(valid_tickers, desc=\"Training Batch 4\"):\n",
        "        for set_name, feature_cols in feature_sets.items():\n",
        "            data = prepare_data_for_ticker(ticker, df, feature_cols, sequence_length=sequence_length)\n",
        "            if data is None: continue\n",
        "\n",
        "            if len(data['X_test']) < 5: continue\n",
        "\n",
        "            for model_name in model_builders.keys():\n",
        "                try:\n",
        "                    cache_key = f\"{model_name}_{set_name}\"\n",
        "                    template = model_templates[cache_key]\n",
        "                    model = template['model']\n",
        "                    model.set_weights(template['initial_weights'])\n",
        "\n",
        "                    history = model.fit(\n",
        "                        data['X_train'], data['y_train'],\n",
        "                        epochs=50, batch_size=32, callbacks=[early_stop],\n",
        "                        verbose=0, validation_split=0.1\n",
        "                    )\n",
        "\n",
        "                    y_pred = model.predict(data['X_test'], verbose=0).flatten()\n",
        "                    metrics = calculate_metrics(data['y_test'], y_pred, model_name, ticker, set_name)\n",
        "\n",
        "                    if metrics: batch_results.append(metrics)\n",
        "                    del history\n",
        "                except: continue\n",
        "\n",
        "        # Periodic memory clear\n",
        "        if (valid_tickers.index(ticker) + 1) % 10 == 0:\n",
        "            tf.keras.backend.clear_session()\n",
        "\n",
        "    # 6. Save and Upload\n",
        "    print(f\"Batch 4 Complete. Generated {len(batch_results)} new results.\")\n",
        "\n",
        "    if batch_results:\n",
        "        # Append new results to CSV\n",
        "        new_results_df = pd.DataFrame(batch_results)\n",
        "        new_results_df['timestamp'] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        # Combine with full dataset\n",
        "        full_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
        "        combined_df = pd.concat([full_df, new_results_df], ignore_index=True)\n",
        "        combined_df.to_csv(output_dir / \"03B_Deep_Learning_Performance.csv\", index=False)\n",
        "        print(f\"CSV updated. Total rows: {len(combined_df)}\")\n",
        "\n",
        "        # Upload to DB (Re-initializing engine to prevent transaction errors)\n",
        "        try:\n",
        "            engine.dispose()\n",
        "            engine = create_engine(db_url)\n",
        "\n",
        "            combined_df.to_sql(\n",
        "                'results_deep_learning_models',\n",
        "                con=engine,\n",
        "                if_exists='replace',\n",
        "                index=False,\n",
        "                chunksize=1000\n",
        "            )\n",
        "            print(\"Database successfully updated with Batch 4.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error uploading Batch 4 to DB: {e}\")\n",
        "\n",
        "    # Final Cleanup\n",
        "    for template in model_templates.values(): del template['model']\n",
        "    tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "Zq195dXvEAms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Assess Performance (Current Progress ~400 tickers)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Assessing Deep Learning Model Performance so far...\")\n",
        "\n",
        "# 1. Load Results from CSV\n",
        "try:\n",
        "    results_path = output_dir / \"03B_Deep_Learning_Performance.csv\"\n",
        "    results_df = pd.read_csv(results_path)\n",
        "    print(f\"Loaded {len(results_df)} results from {results_path}\")\n",
        "    print(f\"Unique tickers processed: {results_df['ticker'].nunique()}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading results: {e}\")\n",
        "    results_df = pd.DataFrame()\n",
        "\n",
        "if not results_df.empty:\n",
        "    # 2. Summary Statistics by Model Type\n",
        "    # We group by model to see which architecture performs best on average\n",
        "    summary = results_df.groupby('model').agg({\n",
        "        'rmse': ['mean', 'std', 'min'],\n",
        "        'mae': ['mean', 'std'],\n",
        "        'directional_accuracy': ['mean', 'std', 'max']\n",
        "    }).round(4)\n",
        "\n",
        "    print(\"\\n--- Performance by Model Type ---\")\n",
        "    print(summary.to_string())\n",
        "\n",
        "    # 3. Top 15 Models by RMSE\n",
        "    print(\"\\n--- Top 15 Models by RMSE ---\")\n",
        "    top_models = results_df.nsmallest(15, 'rmse')[['model', 'ticker', 'rmse', 'directional_accuracy']]\n",
        "    print(top_models.to_string(index=False))\n",
        "\n",
        "    # 4. Comparison with Traditional Models (if available in DB)\n",
        "    try:\n",
        "        trad_results = pd.read_sql(\"SELECT * FROM results_ir_models\", con=engine)\n",
        "\n",
        "        print(\"\\n--- Comparison with Traditional Models ---\")\n",
        "        print(f\"{'Metric':<25} | {'Deep Learning':<15} | {'Traditional':<15}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # RMSE Comparison\n",
        "        dl_min_rmse = results_df['rmse'].min()\n",
        "        trad_min_rmse = trad_results['rmse'].min()\n",
        "        print(f\"{'Best RMSE':<25} | {dl_min_rmse:<15.4f} | {trad_min_rmse:<15.4f}\")\n",
        "\n",
        "        # Directional Accuracy Comparison\n",
        "        dl_avg_acc = results_df['directional_accuracy'].mean()\n",
        "        trad_avg_acc = trad_results['directional_accuracy'].mean()\n",
        "        print(f\"{'Avg Directional Acc %':<25} | {dl_avg_acc:<15.2f} | {trad_avg_acc:<15.2f}\")\n",
        "\n",
        "        dl_best_acc = results_df['directional_accuracy'].max()\n",
        "        trad_best_acc = trad_results['directional_accuracy'].max()\n",
        "        print(f\"{'Best Directional Acc %':<25} | {dl_best_acc:<15.2f} | {trad_best_acc:<15.2f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n(Could not load traditional model results for comparison: {e})\")\n",
        "\n",
        "else:\n",
        "    print(\"No results found to analyze.\")"
      ],
      "metadata": {
        "id": "qfUOHrX2EDUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Train Batch 5 (Next 200 tickers -> Target Total ~600)\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "import tensorflow as tf\n",
        "from tqdm.notebook import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "sequence_length = 60\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True, verbose=0)\n",
        "print(\"Starting Batch 5 Training (Targeting next 200 tickers).\")\n",
        "\n",
        "# 1. Identify what is already done\n",
        "try:\n",
        "    existing_results_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
        "    trained_tickers = set(existing_results_df['ticker'].unique())\n",
        "except:\n",
        "    trained_tickers = set()\n",
        "    existing_results_df = pd.DataFrame()\n",
        "\n",
        "print(f\"Currently trained on {len(trained_tickers)} tickers.\")\n",
        "\n",
        "# 2. Select next batch\n",
        "all_tickers = sorted(df['ticker'].unique())\n",
        "remaining_tickers = [t for t in all_tickers if t not in trained_tickers]\n",
        "\n",
        "if not remaining_tickers:\n",
        "    print(\"All tickers have been trained!\")\n",
        "else:\n",
        "    BATCH_SIZE = 200  # Training 200 tickers in this batch\n",
        "    next_batch = remaining_tickers[:BATCH_SIZE]\n",
        "    print(f\"Targeting next {len(next_batch)} tickers.\")\n",
        "\n",
        "    # 3. Validate Data Availability\n",
        "    valid_tickers = []\n",
        "    for ticker in tqdm(next_batch, desc=\"Validating Tickers\"):\n",
        "        ticker_df = df[df['ticker'] == ticker]\n",
        "        # Check if enough data exists (Sequence length + buffer)\n",
        "        if len(ticker_df) >= sequence_length + 100:\n",
        "            valid_tickers.append(ticker)\n",
        "\n",
        "    print(f\"Valid tickers to train: {len(valid_tickers)}\")\n",
        "\n",
        "    # 4. Rebuild Model Templates (Fresh session)\n",
        "    tf.keras.backend.clear_session()\n",
        "    model_templates = {}\n",
        "    for set_name, feature_cols in feature_sets.items():\n",
        "        input_shape = (sequence_length, len(feature_cols))\n",
        "        for model_name, builder_func in model_builders.items():\n",
        "            model = builder_func(input_shape)\n",
        "            model_templates[f\"{model_name}_{set_name}\"] = {\n",
        "                'model': model,\n",
        "                'initial_weights': model.get_weights(),\n",
        "                'input_shape': input_shape\n",
        "            }\n",
        "\n",
        "    # 5. Training Loop\n",
        "    batch_results = []\n",
        "    for ticker in tqdm(valid_tickers, desc=\"Training Batch 5\"):\n",
        "        for set_name, feature_cols in feature_sets.items():\n",
        "            data = prepare_data_for_ticker(ticker, df, feature_cols, sequence_length=sequence_length)\n",
        "            if data is None: continue\n",
        "            if len(data['X_test']) < 5: continue # Skip small test sets\n",
        "\n",
        "            for model_name in model_builders.keys():\n",
        "                try:\n",
        "                    cache_key = f\"{model_name}_{set_name}\"\n",
        "                    template = model_templates[cache_key]\n",
        "                    model = template['model']\n",
        "                    model.set_weights(template['initial_weights']) # Reset\n",
        "\n",
        "                    history = model.fit(\n",
        "                        data['X_train'], data['y_train'],\n",
        "                        epochs=50, batch_size=32, callbacks=[early_stop],\n",
        "                        verbose=0, validation_split=0.1\n",
        "                    )\n",
        "\n",
        "                    y_pred = model.predict(data['X_test'], verbose=0).flatten()\n",
        "                    metrics = calculate_metrics(data['y_test'], y_pred, model_name, ticker, set_name)\n",
        "\n",
        "                    if metrics: batch_results.append(metrics)\n",
        "                    del history\n",
        "                except: continue\n",
        "\n",
        "        # Memory Cleanup (every 10 tickers)\n",
        "        if (valid_tickers.index(ticker) + 1) % 10 == 0:\n",
        "            tf.keras.backend.clear_session()\n",
        "\n",
        "    # 6. Save Results\n",
        "    print(f\"Batch 5 Complete. Generated {len(batch_results)} new results.\")\n",
        "\n",
        "    if batch_results:\n",
        "        new_results_df = pd.DataFrame(batch_results)\n",
        "        new_results_df['timestamp'] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        # Merge and Save CSV\n",
        "        full_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
        "        combined_df = pd.concat([full_df, new_results_df], ignore_index=True)\n",
        "        combined_df.to_csv(output_dir / \"03B_Deep_Learning_Performance.csv\", index=False)\n",
        "        print(f\"CSV updated. Total rows: {len(combined_df)}\")\n",
        "\n",
        "        # Upload to DB (Re-init engine to act as a fresh transaction)\n",
        "        try:\n",
        "            engine.dispose()\n",
        "            engine = create_engine(db_url)\n",
        "            combined_df.to_sql(\n",
        "                'results_deep_learning_models',\n",
        "                con=engine,\n",
        "                if_exists='replace',\n",
        "                index=False,\n",
        "                chunksize=1000\n",
        "            )\n",
        "            print(\"Database successfully updated.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error uploading to DB: {e}\")\n",
        "\n",
        "    # Final Cleanup\n",
        "    for template in model_templates.values(): del template['model']\n",
        "    tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "k1X07jGkEF1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Train Batch 6 (Next 200 tickers)\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "import tensorflow as tf\n",
        "from tqdm.notebook import tqdm\n",
        "from datetime import datetime\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Configuration\n",
        "sequence_length = 60\n",
        "BATCH_SIZE = 200\n",
        "early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True, verbose=0)\n",
        "\n",
        "print(\"Starting Batch 6 Training (Targeting next 200 tickers).\")\n",
        "\n",
        "# 1. Identify what is already done\n",
        "try:\n",
        "    existing_results_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
        "    trained_tickers = set(existing_results_df['ticker'].unique())\n",
        "except Exception as e:\n",
        "    print(f\"Could not load existing results: {e}\")\n",
        "    trained_tickers = set()\n",
        "    existing_results_df = pd.DataFrame()\n",
        "\n",
        "print(f\"Currently trained on {len(trained_tickers)} tickers.\")\n",
        "\n",
        "# 2. Select next batch\n",
        "all_tickers = sorted(df['ticker'].unique())\n",
        "remaining_tickers = [t for t in all_tickers if t not in trained_tickers]\n",
        "\n",
        "if not remaining_tickers:\n",
        "    print(\"All tickers have been trained! No more batches needed.\")\n",
        "else:\n",
        "    next_batch = remaining_tickers[:BATCH_SIZE]\n",
        "    print(f\"Targeting next {len(next_batch)} tickers (Batch 6).\")\n",
        "\n",
        "    # 3. Validate Data Availability\n",
        "    valid_tickers = []\n",
        "    for ticker in tqdm(next_batch, desc=\"Validating Batch 6 Tickers\"):\n",
        "        ticker_df = df[df['ticker'] == ticker]\n",
        "        if len(ticker_df) >= sequence_length + 100:\n",
        "            valid_tickers.append(ticker)\n",
        "\n",
        "    print(f\"Valid tickers to train: {len(valid_tickers)}\")\n",
        "\n",
        "    # 4. Rebuild Model Templates (Fresh session to clear memory)\n",
        "    tf.keras.backend.clear_session()\n",
        "    model_templates = {}\n",
        "    for set_name, feature_cols in feature_sets.items():\n",
        "        input_shape = (sequence_length, len(feature_cols))\n",
        "        for model_name, builder_func in model_builders.items():\n",
        "            model = builder_func(input_shape)\n",
        "            model_templates[f\"{model_name}_{set_name}\"] = {\n",
        "                'model': model,\n",
        "                'initial_weights': model.get_weights(),\n",
        "                'input_shape': input_shape\n",
        "            }\n",
        "\n",
        "    # 5. Training Loop\n",
        "    batch_results = []\n",
        "    for ticker in tqdm(valid_tickers, desc=\"Training Batch 6\"):\n",
        "        for set_name, feature_cols in feature_sets.items():\n",
        "            data = prepare_data_for_ticker(ticker, df, feature_cols, sequence_length=sequence_length)\n",
        "            if data is None: continue\n",
        "            if len(data['X_test']) < 5: continue\n",
        "\n",
        "            for model_name in model_builders.keys():\n",
        "                try:\n",
        "                    cache_key = f\"{model_name}_{set_name}\"\n",
        "                    template = model_templates[cache_key]\n",
        "                    model = template['model']\n",
        "\n",
        "                    # Reset weights to initial state\n",
        "                    model.set_weights(template['initial_weights'])\n",
        "\n",
        "                    history = model.fit(\n",
        "                        data['X_train'], data['y_train'],\n",
        "                        epochs=50, batch_size=32, callbacks=[early_stop],\n",
        "                        verbose=0, validation_split=0.1\n",
        "                    )\n",
        "\n",
        "                    y_pred = model.predict(data['X_test'], verbose=0).flatten()\n",
        "                    metrics = calculate_metrics(data['y_test'], y_pred, model_name, ticker, set_name)\n",
        "\n",
        "                    if metrics: batch_results.append(metrics)\n",
        "                    del history\n",
        "                except: continue\n",
        "\n",
        "        # Memory Cleanup (every 10 tickers)\n",
        "        if (valid_tickers.index(ticker) + 1) % 10 == 0:\n",
        "            tf.keras.backend.clear_session()\n",
        "\n",
        "    # 6. Save Results\n",
        "    print(f\"Batch 6 Complete. Generated {len(batch_results)} new results.\")\n",
        "\n",
        "    if batch_results:\n",
        "        new_results_df = pd.DataFrame(batch_results)\n",
        "        new_results_df['timestamp'] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        # Merge and Save CSV\n",
        "        try:\n",
        "            full_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
        "            combined_df = pd.concat([full_df, new_results_df], ignore_index=True)\n",
        "            combined_df.to_csv(output_dir / \"03B_Deep_Learning_Performance.csv\", index=False)\n",
        "            print(f\"CSV updated. Total rows: {len(combined_df)}\")\n",
        "\n",
        "            # Upload to DB (Re-init engine to act as a fresh transaction)\n",
        "            engine.dispose()\n",
        "            engine = create_engine(db_url)\n",
        "            combined_df.to_sql(\n",
        "                'results_deep_learning_models',\n",
        "                con=engine,\n",
        "                if_exists='replace',\n",
        "                index=False,\n",
        "                chunksize=1000\n",
        "            )\n",
        "            print(\"Database successfully updated.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving/uploading results: {e}\")\n",
        "\n",
        "    # Final Cleanup\n",
        "    for template in model_templates.values(): del template['model']\n",
        "    tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "RypoR5E3EIpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Train Batch 7 (Tickers 800-1000)\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "import tensorflow as tf\n",
        "from tqdm.notebook import tqdm\n",
        "from datetime import datetime\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Configuration\n",
        "sequence_length = 60\n",
        "BATCH_SIZE = 200  # Target tickers 800-1000\n",
        "early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True, verbose=0)\n",
        "\n",
        "print(\"Starting Batch 7 Training (Targeting next 200 tickers).\")\n",
        "\n",
        "# 1. Identify what is already done\n",
        "try:\n",
        "    existing_results_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
        "    trained_tickers = set(existing_results_df['ticker'].unique())\n",
        "except Exception as e:\n",
        "    print(f\"Could not load existing results: {e}\")\n",
        "    trained_tickers = set()\n",
        "    existing_results_df = pd.DataFrame()\n",
        "\n",
        "print(f\"Currently trained on {len(trained_tickers)} tickers.\")\n",
        "\n",
        "# 2. Select next batch\n",
        "all_tickers = sorted(df['ticker'].unique())\n",
        "remaining_tickers = [t for t in all_tickers if t not in trained_tickers]\n",
        "\n",
        "if not remaining_tickers:\n",
        "    print(\"All tickers have been trained! No more batches needed.\")\n",
        "else:\n",
        "    next_batch = remaining_tickers[:BATCH_SIZE]\n",
        "    print(f\"Targeting next {len(next_batch)} tickers (Batch 7).\")\n",
        "\n",
        "    # 3. Validate Data Availability\n",
        "    valid_tickers = []\n",
        "    for ticker in tqdm(next_batch, desc=\"Validating Batch 7 Tickers\"):\n",
        "        ticker_df = df[df['ticker'] == ticker]\n",
        "        if len(ticker_df) >= sequence_length + 100:\n",
        "            valid_tickers.append(ticker)\n",
        "\n",
        "    print(f\"Valid tickers to train: {len(valid_tickers)}\")\n",
        "\n",
        "    # 4. Rebuild Model Templates (Fresh session to clear memory)\n",
        "    tf.keras.backend.clear_session()\n",
        "    model_templates = {}\n",
        "    for set_name, feature_cols in feature_sets.items():\n",
        "        input_shape = (sequence_length, len(feature_cols))\n",
        "        for model_name, builder_func in model_builders.items():\n",
        "            model = builder_func(input_shape)\n",
        "            model_templates[f\"{model_name}_{set_name}\"] = {\n",
        "                'model': model,\n",
        "                'initial_weights': model.get_weights(),\n",
        "                'input_shape': input_shape\n",
        "            }\n",
        "\n",
        "    # 5. Training Loop\n",
        "    batch_results = []\n",
        "    for ticker in tqdm(valid_tickers, desc=\"Training Batch 7\"):\n",
        "        for set_name, feature_cols in feature_sets.items():\n",
        "            data = prepare_data_for_ticker(ticker, df, feature_cols, sequence_length=sequence_length)\n",
        "            if data is None: continue\n",
        "            if len(data['X_test']) < 5: continue\n",
        "\n",
        "            for model_name in model_builders.keys():\n",
        "                try:\n",
        "                    cache_key = f\"{model_name}_{set_name}\"\n",
        "                    template = model_templates[cache_key]\n",
        "                    model = template['model']\n",
        "\n",
        "                    # Reset weights to initial state\n",
        "                    model.set_weights(template['initial_weights'])\n",
        "\n",
        "                    history = model.fit(\n",
        "                        data['X_train'], data['y_train'],\n",
        "                        epochs=50, batch_size=32, callbacks=[early_stop],\n",
        "                        verbose=0, validation_split=0.1\n",
        "                    )\n",
        "\n",
        "                    y_pred = model.predict(data['X_test'], verbose=0).flatten()\n",
        "                    metrics = calculate_metrics(data['y_test'], y_pred, model_name, ticker, set_name)\n",
        "\n",
        "                    if metrics: batch_results.append(metrics)\n",
        "                    del history\n",
        "                except: continue\n",
        "\n",
        "        # Memory Cleanup (every 10 tickers)\n",
        "        if (valid_tickers.index(ticker) + 1) % 10 == 0:\n",
        "            tf.keras.backend.clear_session()\n",
        "\n",
        "    # 6. Save Results\n",
        "    print(f\"Batch 7 Complete. Generated {len(batch_results)} new results.\")\n",
        "\n",
        "    if batch_results:\n",
        "        new_results_df = pd.DataFrame(batch_results)\n",
        "        new_results_df['timestamp'] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        # Merge and Save CSV\n",
        "        try:\n",
        "            full_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
        "            combined_df = pd.concat([full_df, new_results_df], ignore_index=True)\n",
        "            combined_df.to_csv(output_dir / \"03B_Deep_Learning_Performance.csv\", index=False)\n",
        "            print(f\"CSV updated. Total rows: {len(combined_df)}\")\n",
        "\n",
        "            # Upload to DB (Re-init engine to act as a fresh transaction)\n",
        "            engine.dispose()\n",
        "            engine = create_engine(db_url)\n",
        "            combined_df.to_sql(\n",
        "                'results_deep_learning_models',\n",
        "                con=engine,\n",
        "                if_exists='replace',\n",
        "                index=False,\n",
        "                chunksize=1000\n",
        "            )\n",
        "            print(\"Database successfully updated.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving/uploading results: {e}\")\n",
        "\n",
        "    # Final Cleanup\n",
        "    for template in model_templates.values(): del template['model']\n",
        "    tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "PueeU_OgEK0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Train Batch 8 (Tickers 1000-1200)\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "import tensorflow as tf\n",
        "from tqdm.notebook import tqdm\n",
        "from datetime import datetime\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Configuration\n",
        "sequence_length = 60\n",
        "BATCH_SIZE = 200  # Target next 200 tickers\n",
        "early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True, verbose=0)\n",
        "\n",
        "print(\"Starting Batch 8 Training (Targeting next 200 tickers).\")\n",
        "\n",
        "# 1. Identify what is already done\n",
        "try:\n",
        "    existing_results_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
        "    trained_tickers = set(existing_results_df['ticker'].unique())\n",
        "except Exception as e:\n",
        "    print(f\"Could not load existing results: {e}\")\n",
        "    trained_tickers = set()\n",
        "    existing_results_df = pd.DataFrame()\n",
        "\n",
        "print(f\"Currently trained on {len(trained_tickers)} tickers.\")\n",
        "\n",
        "# 2. Select next batch\n",
        "all_tickers = sorted(df['ticker'].unique())\n",
        "remaining_tickers = [t for t in all_tickers if t not in trained_tickers]\n",
        "\n",
        "if not remaining_tickers:\n",
        "    print(\"All tickers have been trained! No more batches needed.\")\n",
        "else:\n",
        "    next_batch = remaining_tickers[:BATCH_SIZE]\n",
        "    print(f\"Targeting next {len(next_batch)} tickers (Batch 8).\")\n",
        "\n",
        "    # 3. Validate Data Availability\n",
        "    valid_tickers = []\n",
        "    for ticker in tqdm(next_batch, desc=\"Validating Batch 8 Tickers\"):\n",
        "        ticker_df = df[df['ticker'] == ticker]\n",
        "        if len(ticker_df) >= sequence_length + 100:\n",
        "            valid_tickers.append(ticker)\n",
        "\n",
        "    print(f\"Valid tickers to train: {len(valid_tickers)}\")\n",
        "\n",
        "    # 4. Rebuild Model Templates (Fresh session to clear memory)\n",
        "    tf.keras.backend.clear_session()\n",
        "    model_templates = {}\n",
        "    for set_name, feature_cols in feature_sets.items():\n",
        "        input_shape = (sequence_length, len(feature_cols))\n",
        "        for model_name, builder_func in model_builders.items():\n",
        "            model = builder_func(input_shape)\n",
        "            model_templates[f\"{model_name}_{set_name}\"] = {\n",
        "                'model': model,\n",
        "                'initial_weights': model.get_weights(),\n",
        "                'input_shape': input_shape\n",
        "            }\n",
        "\n",
        "    # 5. Training Loop\n",
        "    batch_results = []\n",
        "    for ticker in tqdm(valid_tickers, desc=\"Training Batch 8\"):\n",
        "        for set_name, feature_cols in feature_sets.items():\n",
        "            data = prepare_data_for_ticker(ticker, df, feature_cols, sequence_length=sequence_length)\n",
        "            if data is None: continue\n",
        "            if len(data['X_test']) < 5: continue\n",
        "\n",
        "            for model_name in model_builders.keys():\n",
        "                try:\n",
        "                    cache_key = f\"{model_name}_{set_name}\"\n",
        "                    template = model_templates[cache_key]\n",
        "                    model = template['model']\n",
        "\n",
        "                    # Reset weights to initial state\n",
        "                    model.set_weights(template['initial_weights'])\n",
        "\n",
        "                    history = model.fit(\n",
        "                        data['X_train'], data['y_train'],\n",
        "                        epochs=50, batch_size=32, callbacks=[early_stop],\n",
        "                        verbose=0, validation_split=0.1\n",
        "                    )\n",
        "\n",
        "                    y_pred = model.predict(data['X_test'], verbose=0).flatten()\n",
        "                    metrics = calculate_metrics(data['y_test'], y_pred, model_name, ticker, set_name)\n",
        "\n",
        "                    if metrics: batch_results.append(metrics)\n",
        "                    del history\n",
        "                except: continue\n",
        "\n",
        "        # Memory Cleanup (every 10 tickers)\n",
        "        if (valid_tickers.index(ticker) + 1) % 10 == 0:\n",
        "            tf.keras.backend.clear_session()\n",
        "\n",
        "    # 6. Save Results\n",
        "    print(f\"Batch 8 Complete. Generated {len(batch_results)} new results.\")\n",
        "\n",
        "    if batch_results:\n",
        "        new_results_df = pd.DataFrame(batch_results)\n",
        "        new_results_df['timestamp'] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        # Merge and Save CSV\n",
        "        try:\n",
        "            full_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
        "            combined_df = pd.concat([full_df, new_results_df], ignore_index=True)\n",
        "            combined_df.to_csv(output_dir / \"03B_Deep_Learning_Performance.csv\", index=False)\n",
        "            print(f\"CSV updated. Total rows: {len(combined_df)}\")\n",
        "\n",
        "            # Upload to DB (Re-init engine to act as a fresh transaction)\n",
        "            engine.dispose()\n",
        "            engine = create_engine(db_url)\n",
        "            combined_df.to_sql(\n",
        "                'results_deep_learning_models',\n",
        "                con=engine,\n",
        "                if_exists='replace',\n",
        "                index=False,\n",
        "                chunksize=1000\n",
        "            )\n",
        "            print(\"Database successfully updated.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving/uploading results: {e}\")\n",
        "\n",
        "    # Final Cleanup\n",
        "    for template in model_templates.values(): del template['model']\n",
        "    tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "K61MwVr_ENEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Train Batch 9 (Tickers 1200-1400)\n",
        "import pandas as pd\n",
        "import gc\n",
        "from sqlalchemy import create_engine\n",
        "import tensorflow as tf\n",
        "from tqdm.notebook import tqdm\n",
        "from datetime import datetime\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Configuration\n",
        "sequence_length = 60\n",
        "BATCH_SIZE = 200\n",
        "early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True, verbose=0)\n",
        "\n",
        "print(\"Starting Batch 9 Training (Targeting next 200 tickers)\")\n",
        "\n",
        "# 1. Identify what is already done\n",
        "try:\n",
        "    existing_results_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
        "    trained_tickers = set(existing_results_df['ticker'].unique())\n",
        "except Exception as e:\n",
        "    trained_tickers = set()\n",
        "\n",
        "print(f\"Currently trained on {len(trained_tickers)} tickers.\")\n",
        "\n",
        "# 2. Select next batch\n",
        "all_tickers = sorted(df['ticker'].unique())\n",
        "remaining_tickers = [t for t in all_tickers if t not in trained_tickers]\n",
        "\n",
        "if not remaining_tickers:\n",
        "    print(\"All tickers have been trained!\")\n",
        "else:\n",
        "    next_batch = remaining_tickers[:BATCH_SIZE]\n",
        "    print(f\"Targeting next {len(next_batch)} tickers (Batch 9).\")\n",
        "\n",
        "    # 3. Validate Data\n",
        "    valid_tickers = []\n",
        "    for ticker in tqdm(next_batch, desc=\"Validating Batch 9 Tickers\"):\n",
        "        if len(df[df['ticker'] == ticker]) >= sequence_length + 100:\n",
        "            valid_tickers.append(ticker)\n",
        "    print(f\"Valid tickers: {len(valid_tickers)}\")\n",
        "\n",
        "    # 4. Prepare Weights (Store weights only, not model objects)\n",
        "    tf.keras.backend.clear_session()\n",
        "    weight_templates = {}\n",
        "    for set_name, feature_cols in feature_sets.items():\n",
        "        input_shape = (sequence_length, len(feature_cols))\n",
        "        for model_name, builder_func in model_builders.items():\n",
        "            # Build temp model just to get initial random weights\n",
        "            temp_model = builder_func(input_shape)\n",
        "            weight_templates[f\"{model_name}_{set_name}\"] = {\n",
        "                'weights': temp_model.get_weights(),\n",
        "                'input_shape': input_shape,\n",
        "                'builder': builder_func\n",
        "            }\n",
        "            del temp_model\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    # 5. Training Loop (Constructing FRESH models per ticker to stop retracing)\n",
        "    batch_results = []\n",
        "    for ticker in tqdm(valid_tickers, desc=\"Training Batch 9\"):\n",
        "\n",
        "        # Prepare data\n",
        "        for set_name, feature_cols in feature_sets.items():\n",
        "            data = prepare_data_for_ticker(ticker, df, feature_cols, sequence_length=sequence_length)\n",
        "            if data is None or len(data['X_test']) < 5: continue\n",
        "\n",
        "            for model_name in model_builders.keys():\n",
        "                try:\n",
        "                    cache_key = f\"{model_name}_{set_name}\"\n",
        "                    template = weight_templates[cache_key]\n",
        "\n",
        "                    # FIX: Build a FRESH model instance every time\n",
        "                    # This prevents TensorFlow from trying to reuse the graph for different X_test sizes\n",
        "                    model = template['builder'](template['input_shape'])\n",
        "                    model.set_weights(template['weights'])\n",
        "\n",
        "                    history = model.fit(\n",
        "                        data['X_train'], data['y_train'],\n",
        "                        epochs=50, batch_size=32, callbacks=[early_stop],\n",
        "                        verbose=0, validation_split=0.1\n",
        "                    )\n",
        "\n",
        "                    y_pred = model.predict(data['X_test'], verbose=0).flatten()\n",
        "                    metrics = calculate_metrics(data['y_test'], y_pred, model_name, ticker, set_name)\n",
        "                    if metrics: batch_results.append(metrics)\n",
        "\n",
        "                    # Aggressive cleanup\n",
        "                    del model\n",
        "                    del history\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "        # Clear session after EVERY ticker to prevent memory bloat from fresh models\n",
        "        tf.keras.backend.clear_session()\n",
        "        gc.collect()\n",
        "\n",
        "    # 6. Save Results\n",
        "    print(f\"Batch 9 Complete. Generated {len(batch_results)} new results.\")\n",
        "    if batch_results:\n",
        "        new_results_df = pd.DataFrame(batch_results)\n",
        "        new_results_df['timestamp'] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        try:\n",
        "            full_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
        "            combined_df = pd.concat([full_df, new_results_df], ignore_index=True)\n",
        "            combined_df.to_csv(output_dir / \"03B_Deep_Learning_Performance.csv\", index=False)\n",
        "            print(f\"CSV updated. Total rows: {len(combined_df)}\")\n",
        "\n",
        "            engine.dispose()\n",
        "            engine = create_engine(db_url)\n",
        "            combined_df.to_sql('results_deep_learning_models', con=engine, if_exists='replace', index=False, chunksize=1000)\n",
        "            print(\"Database updated.\")\n",
        "        except Exception as e: print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "e3xhyzzmEPBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Train Batch 10 (Tickers 1400-1600)\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "import tensorflow as tf\n",
        "from tqdm.notebook import tqdm\n",
        "from datetime import datetime\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Configuration\n",
        "sequence_length = 60\n",
        "BATCH_SIZE = 200\n",
        "early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True, verbose=0)\n",
        "\n",
        "print(\"Starting Batch 10 Training (Targeting next 200 tickers).\")\n",
        "\n",
        "# 1. Identify what is already done\n",
        "try:\n",
        "    existing_results_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
        "    trained_tickers = set(existing_results_df['ticker'].unique())\n",
        "except Exception as e:\n",
        "    print(f\"Could not load existing results: {e}\")\n",
        "    trained_tickers = set()\n",
        "    existing_results_df = pd.DataFrame()\n",
        "\n",
        "print(f\"Currently trained on {len(trained_tickers)} tickers.\")\n",
        "\n",
        "# 2. Select next batch\n",
        "all_tickers = sorted(df['ticker'].unique())\n",
        "remaining_tickers = [t for t in all_tickers if t not in trained_tickers]\n",
        "\n",
        "if not remaining_tickers:\n",
        "    print(\"All tickers have been trained! No more batches needed.\")\n",
        "else:\n",
        "    next_batch = remaining_tickers[:BATCH_SIZE]\n",
        "    print(f\"Targeting next {len(next_batch)} tickers (Batch 10).\")\n",
        "\n",
        "    # 3. Validate Data Availability\n",
        "    valid_tickers = []\n",
        "    for ticker in tqdm(next_batch, desc=\"Validating Batch 10 Tickers\"):\n",
        "        ticker_df = df[df['ticker'] == ticker]\n",
        "        if len(ticker_df) >= sequence_length + 100:\n",
        "            valid_tickers.append(ticker)\n",
        "\n",
        "    print(f\"Valid tickers to train: {len(valid_tickers)}\")\n",
        "\n",
        "    # 4. Rebuild Model Templates\n",
        "    tf.keras.backend.clear_session()\n",
        "    model_templates = {}\n",
        "    for set_name, feature_cols in feature_sets.items():\n",
        "        input_shape = (sequence_length, len(feature_cols))\n",
        "        for model_name, builder_func in model_builders.items():\n",
        "            model = builder_func(input_shape)\n",
        "            model_templates[f\"{model_name}_{set_name}\"] = {\n",
        "                'model': model,\n",
        "                'initial_weights': model.get_weights(),\n",
        "                'input_shape': input_shape\n",
        "            }\n",
        "\n",
        "    # 5. Training Loop\n",
        "    batch_results = []\n",
        "    for ticker in tqdm(valid_tickers, desc=\"Training Batch 10\"):\n",
        "        for set_name, feature_cols in feature_sets.items():\n",
        "            data = prepare_data_for_ticker(ticker, df, feature_cols, sequence_length=sequence_length)\n",
        "            if data is None: continue\n",
        "            if len(data['X_test']) < 5: continue\n",
        "\n",
        "            for model_name in model_builders.keys():\n",
        "                try:\n",
        "                    cache_key = f\"{model_name}_{set_name}\"\n",
        "                    template = model_templates[cache_key]\n",
        "                    model = template['model']\n",
        "                    model.set_weights(template['initial_weights'])\n",
        "\n",
        "                    history = model.fit(\n",
        "                        data['X_train'], data['y_train'],\n",
        "                        epochs=50, batch_size=32, callbacks=[early_stop],\n",
        "                        verbose=0, validation_split=0.1\n",
        "                    )\n",
        "\n",
        "                    y_pred = model.predict(data['X_test'], verbose=0).flatten()\n",
        "                    metrics = calculate_metrics(data['y_test'], y_pred, model_name, ticker, set_name)\n",
        "                    if metrics: batch_results.append(metrics)\n",
        "                    del history\n",
        "                except: continue\n",
        "\n",
        "        if (valid_tickers.index(ticker) + 1) % 10 == 0:\n",
        "            tf.keras.backend.clear_session()\n",
        "\n",
        "    # 6. Save Results\n",
        "    print(f\"Batch 10 Complete. Generated {len(batch_results)} new results.\")\n",
        "    if batch_results:\n",
        "        new_results_df = pd.DataFrame(batch_results)\n",
        "        new_results_df['timestamp'] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        try:\n",
        "            full_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
        "            combined_df = pd.concat([full_df, new_results_df], ignore_index=True)\n",
        "            combined_df.to_csv(output_dir / \"03B_Deep_Learning_Performance.csv\", index=False)\n",
        "            print(f\"CSV updated. Total rows: {len(combined_df)}\")\n",
        "\n",
        "            engine.dispose()\n",
        "            engine = create_engine(db_url)\n",
        "            combined_df.to_sql('results_deep_learning_models', con=engine, if_exists='replace', index=False, chunksize=1000)\n",
        "            print(\"Database successfully updated.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving results: {e}\")\n",
        "\n",
        "    for template in model_templates.values(): del template['model']\n",
        "    tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "0Q3YDAiRERyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Train Batch 11 (Tickers 1600-1800)\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "import tensorflow as tf\n",
        "from tqdm.notebook import tqdm\n",
        "from datetime import datetime\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "sequence_length = 60\n",
        "BATCH_SIZE = 200\n",
        "early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True, verbose=0)\n",
        "\n",
        "print(\"Starting Batch 11 Training (Targeting next 200 tickers).\")\n",
        "\n",
        "try:\n",
        "    existing_results_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
        "    trained_tickers = set(existing_results_df['ticker'].unique())\n",
        "except:\n",
        "    trained_tickers = set()\n",
        "\n",
        "print(f\"Currently trained on {len(trained_tickers)} tickers.\")\n",
        "all_tickers = sorted(df['ticker'].unique())\n",
        "remaining_tickers = [t for t in all_tickers if t not in trained_tickers]\n",
        "\n",
        "if not remaining_tickers:\n",
        "    print(\"All tickers trained.\")\n",
        "else:\n",
        "    next_batch = remaining_tickers[:BATCH_SIZE]\n",
        "    print(f\"Targeting next {len(next_batch)} tickers (Batch 11).\")\n",
        "\n",
        "    valid_tickers = []\n",
        "    for ticker in tqdm(next_batch, desc=\"Validating Batch 11 Tickers\"):\n",
        "        if len(df[df['ticker'] == ticker]) >= sequence_length + 100:\n",
        "            valid_tickers.append(ticker)\n",
        "    print(f\"Valid tickers: {len(valid_tickers)}\")\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    model_templates = {}\n",
        "    for set_name, feature_cols in feature_sets.items():\n",
        "        input_shape = (sequence_length, len(feature_cols))\n",
        "        for model_name, builder_func in model_builders.items():\n",
        "            model = builder_func(input_shape)\n",
        "            model_templates[f\"{model_name}_{set_name}\"] = {\n",
        "                'model': model,\n",
        "                'initial_weights': model.get_weights(),\n",
        "                'input_shape': input_shape\n",
        "            }\n",
        "\n",
        "    batch_results = []\n",
        "    for ticker in tqdm(valid_tickers, desc=\"Training Batch 11\"):\n",
        "        for set_name, feature_cols in feature_sets.items():\n",
        "            data = prepare_data_for_ticker(ticker, df, feature_cols, sequence_length=sequence_length)\n",
        "            if data is None or len(data['X_test']) < 5: continue\n",
        "\n",
        "            for model_name in model_builders.keys():\n",
        "                try:\n",
        "                    cache_key = f\"{model_name}_{set_name}\"\n",
        "                    template = model_templates[cache_key]\n",
        "                    model = template['model']\n",
        "                    model.set_weights(template['initial_weights'])\n",
        "\n",
        "                    history = model.fit(data['X_train'], data['y_train'], epochs=50, batch_size=32,\n",
        "                                      callbacks=[early_stop], verbose=0, validation_split=0.1)\n",
        "\n",
        "                    y_pred = model.predict(data['X_test'], verbose=0).flatten()\n",
        "                    metrics = calculate_metrics(data['y_test'], y_pred, model_name, ticker, set_name)\n",
        "                    if metrics: batch_results.append(metrics)\n",
        "                    del history\n",
        "                except: continue\n",
        "\n",
        "        if (valid_tickers.index(ticker) + 1) % 10 == 0:\n",
        "            tf.keras.backend.clear_session()\n",
        "\n",
        "    print(f\"Batch 11 Complete. New results: {len(batch_results)}\")\n",
        "    if batch_results:\n",
        "        new_results_df = pd.DataFrame(batch_results)\n",
        "        new_results_df['timestamp'] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        try:\n",
        "            full_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
        "            combined_df = pd.concat([full_df, new_results_df], ignore_index=True)\n",
        "            combined_df.to_csv(output_dir / \"03B_Deep_Learning_Performance.csv\", index=False)\n",
        "            print(f\"CSV updated. Total rows: {len(combined_df)}\")\n",
        "            engine.dispose()\n",
        "            engine = create_engine(db_url)\n",
        "            combined_df.to_sql('results_deep_learning_models', con=engine, if_exists='replace', index=False, chunksize=1000)\n",
        "            print(\"Database updated.\")\n",
        "        except Exception as e: print(f\"Error: {e}\")\n",
        "\n",
        "    for template in model_templates.values(): del template['model']\n",
        "    tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "wQSkoA4qETpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Train Batch 12 (Tickers 1800-2000)\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "import tensorflow as tf\n",
        "from tqdm.notebook import tqdm\n",
        "from datetime import datetime\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "sequence_length = 60\n",
        "BATCH_SIZE = 200\n",
        "early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True, verbose=0)\n",
        "\n",
        "print(\"Starting Batch 12 Training (Targeting next 200 tickers).\")\n",
        "\n",
        "try:\n",
        "    existing_results_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
        "    trained_tickers = set(existing_results_df['ticker'].unique())\n",
        "except:\n",
        "    trained_tickers = set()\n",
        "\n",
        "print(f\"Currently trained on {len(trained_tickers)} tickers.\")\n",
        "all_tickers = sorted(df['ticker'].unique())\n",
        "remaining_tickers = [t for t in all_tickers if t not in trained_tickers]\n",
        "\n",
        "if not remaining_tickers:\n",
        "    print(\"All tickers trained.\")\n",
        "else:\n",
        "    next_batch = remaining_tickers[:BATCH_SIZE]\n",
        "    print(f\"Targeting next {len(next_batch)} tickers (Batch 12).\")\n",
        "\n",
        "    valid_tickers = []\n",
        "    for ticker in tqdm(next_batch, desc=\"Validating Batch 12 Tickers\"):\n",
        "        if len(df[df['ticker'] == ticker]) >= sequence_length + 100:\n",
        "            valid_tickers.append(ticker)\n",
        "    print(f\"Valid tickers: {len(valid_tickers)}\")\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    model_templates = {}\n",
        "    for set_name, feature_cols in feature_sets.items():\n",
        "        input_shape = (sequence_length, len(feature_cols))\n",
        "        for model_name, builder_func in model_builders.items():\n",
        "            model = builder_func(input_shape)\n",
        "            model_templates[f\"{model_name}_{set_name}\"] = {\n",
        "                'model': model,\n",
        "                'initial_weights': model.get_weights(),\n",
        "                'input_shape': input_shape\n",
        "            }\n",
        "\n",
        "    batch_results = []\n",
        "    for ticker in tqdm(valid_tickers, desc=\"Training Batch 12\"):\n",
        "        for set_name, feature_cols in feature_sets.items():\n",
        "            data = prepare_data_for_ticker(ticker, df, feature_cols, sequence_length=sequence_length)\n",
        "            if data is None or len(data['X_test']) < 5: continue\n",
        "\n",
        "            for model_name in model_builders.keys():\n",
        "                try:\n",
        "                    cache_key = f\"{model_name}_{set_name}\"\n",
        "                    template = model_templates[cache_key]\n",
        "                    model = template['model']\n",
        "                    model.set_weights(template['initial_weights'])\n",
        "\n",
        "                    history = model.fit(data['X_train'], data['y_train'], epochs=50, batch_size=32,\n",
        "                                      callbacks=[early_stop], verbose=0, validation_split=0.1)\n",
        "\n",
        "                    y_pred = model.predict(data['X_test'], verbose=0).flatten()\n",
        "                    metrics = calculate_metrics(data['y_test'], y_pred, model_name, ticker, set_name)\n",
        "                    if metrics: batch_results.append(metrics)\n",
        "                    del history\n",
        "                except: continue\n",
        "\n",
        "        if (valid_tickers.index(ticker) + 1) % 10 == 0:\n",
        "            tf.keras.backend.clear_session()\n",
        "\n",
        "    print(f\"Batch 12 Complete. New results: {len(batch_results)}\")\n",
        "    if batch_results:\n",
        "        new_results_df = pd.DataFrame(batch_results)\n",
        "        new_results_df['timestamp'] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        try:\n",
        "            full_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
        "            combined_df = pd.concat([full_df, new_results_df], ignore_index=True)\n",
        "            combined_df.to_csv(output_dir / \"03B_Deep_Learning_Performance.csv\", index=False)\n",
        "            print(f\"CSV updated. Total rows: {len(combined_df)}\")\n",
        "            engine.dispose()\n",
        "            engine = create_engine(db_url)\n",
        "            combined_df.to_sql('results_deep_learning_models', con=engine, if_exists='replace', index=False, chunksize=1000)\n",
        "            print(\"Database updated.\")\n",
        "        except Exception as e: print(f\"Error: {e}\")\n",
        "\n",
        "    for template in model_templates.values(): del template['model']\n",
        "    tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "-MaMguEsEVll"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}