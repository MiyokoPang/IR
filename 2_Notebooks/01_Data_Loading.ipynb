{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aff7e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and connection configured.\n",
      "Project Base Directory: c:\\Users\\18kyu\\Desktop\\Unishit\\IR\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Database Connection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import math\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Libraries imported and connection configured.\")\n",
    "\n",
    "# Define Base Directory\n",
    "base_dir = Path.cwd().parent \n",
    "\n",
    "# Database Configuration\n",
    "db_config = {\n",
    "    'host': '127.0.0.1',\n",
    "    'user': 'root',\n",
    "    'password': '',  \n",
    "    'database': 'trading_system'\n",
    "}\n",
    "db_url = f\"mysql+pymysql://{db_config['user']}:{db_config['password']}@{db_config['host']}/{db_config['database']}\"\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "print(f\"Project Base Directory: {base_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a115ffc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and Writing Headlines\n",
      "Loaded 1147268 headlines from CSV.\n",
      "Writing 1147268 headlines to 'headlines' table.\n",
      "Headlines table successfully written.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load and Write Headlines\n",
    "print(\"Loading and Writing Headlines\")\n",
    "try:\n",
    "    final_headlines_path = base_dir / \"1_IR_Data\" / \"2_Cleaned_Data\" / \"Cleaned_Sentiment_Final.csv\"\n",
    "    df_headlines_final = pd.read_csv(final_headlines_path)\n",
    "    print(f\"Loaded {len(df_headlines_final)} headlines from CSV.\")\n",
    "    \n",
    "    # Prepare for database\n",
    "    df_headlines_to_db = df_headlines_final[['index', 'headline', 'date', 'stock']]\n",
    "    df_headlines_to_db.columns = ['original_index', 'headline', 'date', 'stock'] \n",
    "    \n",
    "    print(f\"Writing {len(df_headlines_to_db)} headlines to 'headlines' table.\")\n",
    "    df_headlines_to_db.to_sql('headlines', con=engine, if_exists='replace', index=False, chunksize=1000)\n",
    "    print(\"Headlines table successfully written.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error writing headlines: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f84b739d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and Writing Stock Prices\n",
      "Loading prices from c:\\Users\\18kyu\\Desktop\\Unishit\\IR\\1_IR_Data\\3_Price_Data\\Stock_Price_20250727_150046.csv...\n",
      "Total rows to write: 8,986,458. Total chunks: 180.\n",
      "\n",
      "Writing stock prices to 'stock_prices' table (in chunks)...\n",
      "Saved chunk 1 of 180 (Table Replaced).\n",
      "Saved chunk 2 of 180.\n",
      "Saved chunk 3 of 180.\n",
      "Saved chunk 4 of 180.\n",
      "Saved chunk 5 of 180.\n",
      "Saved chunk 6 of 180.\n",
      "Saved chunk 7 of 180.\n",
      "Saved chunk 8 of 180.\n",
      "Saved chunk 9 of 180.\n",
      "Saved chunk 10 of 180.\n",
      "Saved chunk 11 of 180.\n",
      "Saved chunk 12 of 180.\n",
      "Saved chunk 13 of 180.\n",
      "Saved chunk 14 of 180.\n",
      "Saved chunk 15 of 180.\n",
      "Saved chunk 16 of 180.\n",
      "Saved chunk 17 of 180.\n",
      "Saved chunk 18 of 180.\n",
      "Saved chunk 19 of 180.\n",
      "Saved chunk 20 of 180.\n",
      "Saved chunk 21 of 180.\n",
      "Saved chunk 22 of 180.\n",
      "Saved chunk 23 of 180.\n",
      "Saved chunk 24 of 180.\n",
      "Saved chunk 25 of 180.\n",
      "Saved chunk 26 of 180.\n",
      "Saved chunk 27 of 180.\n",
      "Saved chunk 28 of 180.\n",
      "Saved chunk 29 of 180.\n",
      "Saved chunk 30 of 180.\n",
      "Saved chunk 31 of 180.\n",
      "Saved chunk 32 of 180.\n",
      "Saved chunk 33 of 180.\n",
      "Saved chunk 34 of 180.\n",
      "Saved chunk 35 of 180.\n",
      "Saved chunk 36 of 180.\n",
      "Saved chunk 37 of 180.\n",
      "Saved chunk 38 of 180.\n",
      "Saved chunk 39 of 180.\n",
      "Saved chunk 40 of 180.\n",
      "Saved chunk 41 of 180.\n",
      "Saved chunk 42 of 180.\n",
      "Saved chunk 43 of 180.\n",
      "Saved chunk 44 of 180.\n",
      "Saved chunk 45 of 180.\n",
      "Saved chunk 46 of 180.\n",
      "Saved chunk 47 of 180.\n",
      "Saved chunk 48 of 180.\n",
      "Saved chunk 49 of 180.\n",
      "Saved chunk 50 of 180.\n",
      "Saved chunk 51 of 180.\n",
      "Saved chunk 52 of 180.\n",
      "Saved chunk 53 of 180.\n",
      "Saved chunk 54 of 180.\n",
      "Saved chunk 55 of 180.\n",
      "Saved chunk 56 of 180.\n",
      "Saved chunk 57 of 180.\n",
      "Saved chunk 58 of 180.\n",
      "Saved chunk 59 of 180.\n",
      "Saved chunk 60 of 180.\n",
      "Saved chunk 61 of 180.\n",
      "Saved chunk 62 of 180.\n",
      "Saved chunk 63 of 180.\n",
      "Saved chunk 64 of 180.\n",
      "Saved chunk 65 of 180.\n",
      "Saved chunk 66 of 180.\n",
      "Saved chunk 67 of 180.\n",
      "Saved chunk 68 of 180.\n",
      "Saved chunk 69 of 180.\n",
      "Saved chunk 70 of 180.\n",
      "Saved chunk 71 of 180.\n",
      "Saved chunk 72 of 180.\n",
      "Saved chunk 73 of 180.\n",
      "Saved chunk 74 of 180.\n",
      "Saved chunk 75 of 180.\n",
      "Saved chunk 76 of 180.\n",
      "Saved chunk 77 of 180.\n",
      "Saved chunk 78 of 180.\n",
      "Saved chunk 79 of 180.\n",
      "Saved chunk 80 of 180.\n",
      "Saved chunk 81 of 180.\n",
      "Saved chunk 82 of 180.\n",
      "Saved chunk 83 of 180.\n",
      "Saved chunk 84 of 180.\n",
      "Saved chunk 85 of 180.\n",
      "Saved chunk 86 of 180.\n",
      "Saved chunk 87 of 180.\n",
      "Saved chunk 88 of 180.\n",
      "Saved chunk 89 of 180.\n",
      "Saved chunk 90 of 180.\n",
      "Saved chunk 91 of 180.\n",
      "Saved chunk 92 of 180.\n",
      "Saved chunk 93 of 180.\n",
      "Saved chunk 94 of 180.\n",
      "Saved chunk 95 of 180.\n",
      "Saved chunk 96 of 180.\n",
      "Saved chunk 97 of 180.\n",
      "Saved chunk 98 of 180.\n",
      "Saved chunk 99 of 180.\n",
      "Saved chunk 100 of 180.\n",
      "Saved chunk 101 of 180.\n",
      "Saved chunk 102 of 180.\n",
      "Saved chunk 103 of 180.\n",
      "Saved chunk 104 of 180.\n",
      "Saved chunk 105 of 180.\n",
      "Saved chunk 106 of 180.\n",
      "Saved chunk 107 of 180.\n",
      "Saved chunk 108 of 180.\n",
      "Saved chunk 109 of 180.\n",
      "Saved chunk 110 of 180.\n",
      "Saved chunk 111 of 180.\n",
      "Saved chunk 112 of 180.\n",
      "Saved chunk 113 of 180.\n",
      "Saved chunk 114 of 180.\n",
      "Saved chunk 115 of 180.\n",
      "Saved chunk 116 of 180.\n",
      "Saved chunk 117 of 180.\n",
      "Saved chunk 118 of 180.\n",
      "Saved chunk 119 of 180.\n",
      "Saved chunk 120 of 180.\n",
      "Saved chunk 121 of 180.\n",
      "Saved chunk 122 of 180.\n",
      "Saved chunk 123 of 180.\n",
      "Saved chunk 124 of 180.\n",
      "Saved chunk 125 of 180.\n",
      "Saved chunk 126 of 180.\n",
      "Saved chunk 127 of 180.\n",
      "Saved chunk 128 of 180.\n",
      "Saved chunk 129 of 180.\n",
      "Saved chunk 130 of 180.\n",
      "Saved chunk 131 of 180.\n",
      "Saved chunk 132 of 180.\n",
      "Saved chunk 133 of 180.\n",
      "Saved chunk 134 of 180.\n",
      "Saved chunk 135 of 180.\n",
      "Saved chunk 136 of 180.\n",
      "Saved chunk 137 of 180.\n",
      "Saved chunk 138 of 180.\n",
      "Saved chunk 139 of 180.\n",
      "Saved chunk 140 of 180.\n",
      "Saved chunk 141 of 180.\n",
      "Saved chunk 142 of 180.\n",
      "Saved chunk 143 of 180.\n",
      "Saved chunk 144 of 180.\n",
      "Saved chunk 145 of 180.\n",
      "Saved chunk 146 of 180.\n",
      "Saved chunk 147 of 180.\n",
      "Saved chunk 148 of 180.\n",
      "Saved chunk 149 of 180.\n",
      "Saved chunk 150 of 180.\n",
      "Saved chunk 151 of 180.\n",
      "Saved chunk 152 of 180.\n",
      "Saved chunk 153 of 180.\n",
      "Saved chunk 154 of 180.\n",
      "Saved chunk 155 of 180.\n",
      "Saved chunk 156 of 180.\n",
      "Saved chunk 157 of 180.\n",
      "Saved chunk 158 of 180.\n",
      "Saved chunk 159 of 180.\n",
      "Saved chunk 160 of 180.\n",
      "Saved chunk 161 of 180.\n",
      "Saved chunk 162 of 180.\n",
      "Saved chunk 163 of 180.\n",
      "Saved chunk 164 of 180.\n",
      "Saved chunk 165 of 180.\n",
      "Saved chunk 166 of 180.\n",
      "Saved chunk 167 of 180.\n",
      "Saved chunk 168 of 180.\n",
      "Saved chunk 169 of 180.\n",
      "Saved chunk 170 of 180.\n",
      "Saved chunk 171 of 180.\n",
      "Saved chunk 172 of 180.\n",
      "Saved chunk 173 of 180.\n",
      "Saved chunk 174 of 180.\n",
      "Saved chunk 175 of 180.\n",
      "Saved chunk 176 of 180.\n",
      "Saved chunk 177 of 180.\n",
      "Saved chunk 178 of 180.\n",
      "Saved chunk 179 of 180.\n",
      "Saved chunk 180 of 180.\n",
      "\n",
      "Stock prices table successfully written. Total rows: 8,986,458\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load and Write Stock Prices\n",
    "print(\"Loading and Writing Stock Prices\")\n",
    "try:\n",
    "    price_file_path = base_dir / \"1_IR_Data\" / \"3_Price_Data\" / \"Stock_Price_20250727_150046.csv\"\n",
    "    \n",
    "    print(f\"Loading prices from {price_file_path}...\")\n",
    "    \n",
    "    chunk_size = 50000\n",
    "    with open(price_file_path, 'r', encoding='utf-8') as f:\n",
    "        total_rows = sum(1 for line in f) - 1 \n",
    "    total_chunks = math.ceil(total_rows / chunk_size)\n",
    "    print(f\"Total rows to write: {total_rows:,}. Total chunks: {total_chunks}.\")\n",
    "    \n",
    "    print(f\"\\nWriting stock prices to 'stock_prices' table (in chunks)...\")\n",
    "    chunk_iter = pd.read_csv(price_file_path, chunksize=chunk_size, on_bad_lines='skip') \n",
    "    \n",
    "    first_chunk = True\n",
    "    chunk_count = 0 \n",
    "    rows_written = 0\n",
    "\n",
    "    for chunk in chunk_iter:\n",
    "        chunk_count += 1 \n",
    "        \n",
    "        chunk.replace([np.inf, -np.inf], np.nan, inplace=True) \n",
    "        chunk.rename(columns={'Date': 'date', 'Open': 'open', 'High': 'high', 'Low': 'low', 'Close': 'close', 'Volume': 'volume'}, inplace=True)\n",
    "        final_chunk = chunk[['date', 'ticker', 'open', 'high', 'low', 'close', 'volume']]\n",
    "        \n",
    "        if first_chunk:\n",
    "            final_chunk.to_sql('stock_prices', con=engine, if_exists='replace', index=False, chunksize=1000)\n",
    "            print(f\"Saved chunk {chunk_count} of {total_chunks} (Table Replaced).\")\n",
    "            first_chunk = False\n",
    "        else:\n",
    "            final_chunk.to_sql('stock_prices', con=engine, if_exists='append', index=False, chunksize=1000)\n",
    "            print(f\"Saved chunk {chunk_count} of {total_chunks}.\")\n",
    "            \n",
    "        rows_written += len(chunk)\n",
    "\n",
    "    print(f\"\\nStock prices table successfully written. Total rows: {rows_written:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error writing stock prices to database: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed6e18fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and Writing Sentiment Scores\n",
      "Loaded 1274576 scores from CSV.\n",
      "Writing 1141860 scores to 'sentiment_scores' table.\n",
      "Sentiment scores table successfully written.\n",
      "\n",
      " Database loading complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load and Write Sentiment Scores\n",
    "print(\"Loading and Writing Sentiment Scores\")\n",
    "try:\n",
    "    sentiment_file_path = base_dir / \"1_IR_Data\" / \"4_Sentiment_Scores\" / \"Sentiment_Scores_Complete.csv\"\n",
    "    df_scores = pd.read_csv(sentiment_file_path)\n",
    "    print(f\"Loaded {len(df_scores)} scores from CSV.\")\n",
    "    \n",
    "    mapping_df = pd.read_sql(\"SELECT id, original_index FROM headlines\", con=engine)\n",
    "    mapping_dict = pd.Series(mapping_df.id.values, index=mapping_df.original_index).to_dict()\n",
    "    \n",
    "    df_scores['headline_id'] = df_scores['index'].map(mapping_dict)\n",
    "    \n",
    "    final_scores = df_scores[[\n",
    "        'headline_id', 'textblob_polarity', 'textblob_subjectivity', \n",
    "        'vader_compound', 'vader_positive', 'vader_negative', 'vader_neutral', \n",
    "        'finbert_compound', 'finbert_positive', 'finbert_negative', 'finbert_neutral', 'finbert_label'\n",
    "    ]]\n",
    "    final_scores = final_scores.dropna(subset=['headline_id']) \n",
    "    \n",
    "    print(f\"Writing {len(final_scores)} scores to 'sentiment_scores' table.\")\n",
    "    final_scores.to_sql('sentiment_scores', con=engine, if_exists='replace', index=False, chunksize=1000)\n",
    "    print(\"Sentiment scores table successfully written.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error writing sentiment scores: {e}\")\n",
    "\n",
    "print(\"\\n Database loading complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
