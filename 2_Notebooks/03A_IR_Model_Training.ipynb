{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73b89a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n",
      "Results will be saved to: c:\\Users\\18kyu\\Desktop\\Unishit\\IR\\4_Results\\Ablation_Runs\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports & Configuration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import math\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Database\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "\n",
    "# Metrics and Preprocessing\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"Libraries imported.\")\n",
    "\n",
    "# Database Configuration\n",
    "db_config = {\n",
    "    'host': '127.0.0.1',\n",
    "    'user': 'root',\n",
    "    'password': '',  \n",
    "    'database': 'trading_system'\n",
    "}\n",
    "db_url = f\"mysql+pymysql://{db_config['user']}:{db_config['password']}@{db_config['host']}/{db_config['database']}\"\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "# Path Definition\n",
    "base_dir = Path.cwd().parent\n",
    "output_dir = base_dir / \"4_Results\" / \"Ablation_Runs\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Results will be saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6a65f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 'model_features' table from database.\n",
      "Successfully loaded 1,855,010 rows.\n",
      "Data date range: 2010-04-11 to 2020-05-31\n",
      "Number of unique tickers: 4028\n",
      "\n",
      " Available columns in model_features:\n",
      "['ticker', 'date', 'adj_close', 'volume', 'textblob_polarity', 'vader_compound', 'finbert_compound', 'news_count', 'has_news', 'days_since_news', 'sentiment_freshness', 'textblob_polarity_volatility', 'vader_compound_volatility', 'finbert_compound_volatility', 'prev_close', 'return', 'log_return', 'ma_3', 'ma_5', 'ma_10', 'volatility', 'rsi', 'momentum_3', 'momentum_5', 'finbert_momentum_3', 'vader_momentum_3', 'textblob_momentum_3', 'target_return']\n",
      "\n",
      "=== Data Quality Check ===\n",
      "NaN in target_return: 0 (0.00%)\n",
      "NaN in adj_close: 0\n",
      "\n",
      "=== Sample Data (First Ticker) ===\n",
      "  ticker       date  adj_close  target_return  textblob_polarity  finbert_compound\n",
      "0      A 2010-04-11    21.7974       0.030384           0.053013          0.038326\n",
      "1      A 2010-04-18    22.4597       0.038843           0.053013          0.038326\n",
      "2      A 2010-04-25    23.3321      -0.010372           0.053013          0.038326\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load and Validate 'model_features' table from database\n",
    "print(\"Loading 'model_features' table from database.\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_sql(\"SELECT * FROM model_features\", con=engine)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values(by=['ticker', 'date'])\n",
    "    \n",
    "    print(f\"Successfully loaded {len(df):,} rows.\")\n",
    "    print(f\"Data date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
    "    print(f\"Number of unique tickers: {df['ticker'].nunique()}\")\n",
    "    \n",
    "    # Display available columns to verify\n",
    "    print(f\"\\n Available columns in model_features:\")\n",
    "    print(df.columns.tolist())\n",
    "    \n",
    "    # Check for required columns\n",
    "    required_cols = ['ticker', 'date', 'adj_close', 'target_return']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"ERROR: Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    # Check for NaN values in critical columns\n",
    "    print(f\"\\n=== Data Quality Check ===\")\n",
    "    print(f\"NaN in target_return: {df['target_return'].isna().sum()} ({df['target_return'].isna().sum()/len(df)*100:.2f}%)\")\n",
    "    print(f\"NaN in adj_close: {df['adj_close'].isna().sum()}\")\n",
    "    \n",
    "    # Display sample data\n",
    "    print(f\"\\n=== Sample Data (First Ticker) ===\")\n",
    "    first_ticker = df['ticker'].iloc[0]\n",
    "    print(df[df['ticker'] == first_ticker].head(3)[['ticker', 'date', 'adj_close', 'target_return', 'textblob_polarity', 'finbert_compound']].to_string())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ca60f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Defining feature sets for model training.\n",
      "Total feature sets: 3\n",
      "- Price_Only: 9 features\n",
      "- Price_Base_Sentiment: 12 features\n",
      "- Price_Full_Sentiment: 22 features\n",
      "\n",
      " Validating features against available columns.\n",
      "All features are available in the dataset.\n",
      "\n",
      " Feature Breakdown\n",
      "Price_Only features:\n",
      "prev_close is available.\n",
      "ma_3 is available.\n",
      "ma_5 is available.\n",
      "ma_10 is available.\n",
      "volatility is available.\n",
      "rsi is available.\n",
      "momentum_3 is available.\n",
      "momentum_5 is available.\n",
      "volume is available.\n",
      "\n",
      "Sentiment features added in Base:\n",
      "textblob_polarity is available.\n",
      "vader_compound is available.\n",
      "finbert_compound is available.\n",
      "\n",
      "Additional features in Full:\n",
      "news_count is available.\n",
      "has_news is available.\n",
      "days_since_news is available.\n",
      "sentiment_freshness is available.\n",
      "textblob_polarity_volatility is available.\n",
      "vader_compound_volatility is available.\n",
      "finbert_compound_volatility is available.\n",
      "finbert_momentum_3 is available.\n",
      "vader_momentum_3 is available.\n",
      "textblob_momentum_3 is available.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Define Feature Sets\n",
    "print(\"\\n Defining feature sets for model training.\")\n",
    "\n",
    "# 1. Price_Only - Technical features\n",
    "price_features = [\n",
    "    'prev_close',\n",
    "    'ma_3',\n",
    "    'ma_5',\n",
    "    'ma_10',\n",
    "    'volatility',\n",
    "    'rsi',\n",
    "    'momentum_3',\n",
    "    'momentum_5',\n",
    "    'volume'\n",
    "]\n",
    "\n",
    "# 2. Price_Base_Sentiment - Price + basic sentiment scores\n",
    "base_sentiment_features = price_features + [\n",
    "    'textblob_polarity',    # TextBlob sentiment\n",
    "    'vader_compound',       # VADER sentiment\n",
    "    'finbert_compound'      # FinBERT sentiment\n",
    "]\n",
    "\n",
    "# 3. Price_Full_Sentiment - Price + all sentiment features\n",
    "full_sentiment_features = price_features + [\n",
    "    # Base sentiment scores\n",
    "    'textblob_polarity', \n",
    "    'vader_compound', \n",
    "    'finbert_compound',\n",
    "    \n",
    "    # News metadata\n",
    "    'news_count',           # Number of news articles\n",
    "    'has_news',             # Binary: any news this period\n",
    "    'days_since_news',      # Days since last news\n",
    "    'sentiment_freshness',  # Sentiment freshness score\n",
    "    \n",
    "    # Sentiment volatility\n",
    "    'textblob_polarity_volatility',\n",
    "    'vader_compound_volatility',\n",
    "    'finbert_compound_volatility',\n",
    "    \n",
    "    # Sentiment momentum\n",
    "    'finbert_momentum_3',\n",
    "    'vader_momentum_3',\n",
    "    'textblob_momentum_3'\n",
    "]\n",
    "\n",
    "# Store feature sets\n",
    "feature_sets = {\n",
    "    \"Price_Only\": price_features,\n",
    "    \"Price_Base_Sentiment\": base_sentiment_features,\n",
    "    \"Price_Full_Sentiment\": full_sentiment_features\n",
    "}\n",
    "\n",
    "# Summary of feature sets\n",
    "print(f\"Total feature sets: {len(feature_sets)}\")\n",
    "print(f\"- Price_Only: {len(price_features)} features\")\n",
    "print(f\"- Price_Base_Sentiment: {len(base_sentiment_features)} features\")\n",
    "print(f\"- Price_Full_Sentiment: {len(full_sentiment_features)} features\")\n",
    "\n",
    "# Check if all features exist \n",
    "print(f\"\\n Validating features against available columns.\")\n",
    "all_features = set(full_sentiment_features)\n",
    "available_cols = set(df.columns)\n",
    "missing_features = all_features - available_cols\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"The following features are not in the data:\")\n",
    "    for feat in sorted(missing_features):\n",
    "        print(f\"  - {feat}\")\n",
    "    print(\"\\n Need to update your feature lists.\")\n",
    "else:\n",
    "    print(\"All features are available in the dataset.\")\n",
    "    \n",
    "# Show feature breakdown\n",
    "print(f\"\\n Feature Breakdown\")\n",
    "print(f\"Price_Only features:\")\n",
    "for feat in price_features:\n",
    "    print(f\"{feat} is available.\")\n",
    "    \n",
    "print(f\"\\nSentiment features added in Base:\")\n",
    "for feat in base_sentiment_features:\n",
    "    if feat not in price_features:\n",
    "        print(f\"{feat} is available.\")\n",
    "        \n",
    "print(f\"\\nAdditional features in Full:\")\n",
    "for feat in full_sentiment_features:\n",
    "    if feat not in base_sentiment_features:\n",
    "        print(f\"{feat} is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77073102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Define Helper Functions\n",
    "def calculate_metrics(y_true, y_pred, model_name, ticker, feature_set_name):\n",
    "    \"\"\"Calculate metrics with safety checks including directional accuracy\"\"\"\n",
    "    mask = ~(np.isnan(y_true) | np.isnan(y_pred) | np.isinf(y_true) | np.isinf(y_pred))\n",
    "    y_true_clean = y_true[mask]\n",
    "    y_pred_clean = y_pred[mask]\n",
    "    \n",
    "    if len(y_true_clean) == 0:\n",
    "        return None\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))\n",
    "    mae = mean_absolute_error(y_true_clean, y_pred_clean)\n",
    "    r2 = r2_score(y_true_clean, y_pred_clean)\n",
    "    \n",
    "    # Correct directional accuracy for returns\n",
    "    correct_direction = np.sign(y_true_clean) == np.sign(y_pred_clean)\n",
    "    directional_accuracy = np.mean(correct_direction) * 100\n",
    "    \n",
    "    return {\n",
    "        'ticker': ticker,\n",
    "        'model': f\"{model_name}_{feature_set_name}\",\n",
    "        'feature_set': feature_set_name,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'directional_accuracy': directional_accuracy\n",
    "    }\n",
    "    \n",
    "print(\"\\n Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb1a5d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All model-training functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Model Training Functions\n",
    "# 1. Linear Models\n",
    "def run_linear_models(train_df, test_df, feature_cols, feature_set_name, ticker):\n",
    "    results = []\n",
    "    models = {\n",
    "        \"LinearRegression\": LinearRegression(),\n",
    "        \"Ridge\": Ridge(random_state=42),\n",
    "        \"Lasso\": Lasso(random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(train_df[feature_cols])\n",
    "    X_test = scaler.transform(test_df[feature_cols])\n",
    "    y_train = train_df['target_return'].values\n",
    "    y_test = test_df['target_return'].values\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            metrics = calculate_metrics(y_test, y_pred, name, ticker, feature_set_name)\n",
    "            if metrics:\n",
    "                results.append(metrics)\n",
    "        except Exception as e:\n",
    "            pass \n",
    "    return results\n",
    "\n",
    "# 2. Tree-Based Models \n",
    "def run_tree_models(train_df, test_df, feature_cols, feature_set_name, ticker):\n",
    "    results = []\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),\n",
    "        \"XGBoost_Light\": xgb.XGBRegressor(n_estimators=50, max_depth=3, learning_rate=0.1, objective='reg:squarederror', random_state=42),\n",
    "        \"XGBoost_Medium\": xgb.XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, objective='reg:squarederror', random_state=42),\n",
    "    }\n",
    "    \n",
    "    # Scaling data for XGBoost\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(train_df[feature_cols])\n",
    "    X_test = scaler.transform(test_df[feature_cols])\n",
    "    y_train = train_df['target_return'].values\n",
    "    y_test = test_df['target_return'].values\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            metrics = calculate_metrics(y_test, y_pred, name, ticker, feature_set_name)\n",
    "            if metrics:\n",
    "                results.append(metrics)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    return results\n",
    "\n",
    "# 3. SVM Models\n",
    "def run_svm_models(train_df, test_df, feature_cols, feature_set_name, ticker):\n",
    "    results = []\n",
    "    model = SVR(kernel='rbf', C=1.0, gamma='scale')\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(train_df[feature_cols])\n",
    "    X_test = scaler.transform(test_df[feature_cols])\n",
    "    y_train = train_df['target_return'].values\n",
    "    y_test = test_df['target_return'].values\n",
    "    \n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        metrics = calculate_metrics(y_test, y_pred, \"SVR\", ticker, feature_set_name)\n",
    "        if metrics:\n",
    "            results.append(metrics)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    return results\n",
    "\n",
    "# 4. MLP Models \n",
    "def run_mlp_models(train_df, test_df, feature_cols, feature_set_name, ticker):\n",
    "    results = []\n",
    "    configs = [\n",
    "        {'hidden_layer_sizes': (50,), 'name': 'MLP_Small'},\n",
    "        {'hidden_layer_sizes': (64, 32), 'name': 'MLP_Medium'},\n",
    "    ]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(train_df[feature_cols])\n",
    "    X_test = scaler.transform(test_df[feature_cols])\n",
    "    y_train = train_df['target_return'].values\n",
    "    y_test = test_df['target_return'].values\n",
    "    \n",
    "    for config in configs:\n",
    "        try:\n",
    "            model = MLPRegressor(\n",
    "                hidden_layer_sizes=config['hidden_layer_sizes'], \n",
    "                max_iter=500, random_state=42, early_stopping=True\n",
    "            )\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            metrics = calculate_metrics(y_test, y_pred, config['name'], ticker, feature_set_name)\n",
    "            if metrics:\n",
    "                results.append(metrics)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    return results\n",
    "\n",
    "# 5. ARIMA Models\n",
    "def run_arima_models(train_df, test_df, ticker):\n",
    "    results = []\n",
    "    # Target variable\n",
    "    y_train = train_df['target_return']\n",
    "    y_test = test_df['target_return']\n",
    "    \n",
    "    # 1. ARIMA (Price Only)\n",
    "    try:\n",
    "        model_arima = ARIMA(y_train, order=(5, 1, 0)) # Using a standard p,d,q\n",
    "        model_fit = model_arima.fit()\n",
    "        forecast = model_fit.forecast(steps=len(y_test))\n",
    "        metrics = calculate_metrics(y_test.values, forecast.values, \"ARIMA(5,1,0)\", ticker, \"Price_Only\")\n",
    "        if metrics:\n",
    "            results.append(metrics)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        \n",
    "    # 2. ARIMAX (Full Sentiment Features)\n",
    "    try:\n",
    "        exog_cols = [c for c in full_sentiment_features if c not in price_features]\n",
    "        exog_train = train_df[exog_cols]\n",
    "        exog_test = test_df[exog_cols]\n",
    "        \n",
    "        model_arimax = ARIMA(y_train, order=(5, 1, 0), exog=exog_train)\n",
    "        model_fit = model_arimax.fit()\n",
    "        forecast = model_fit.forecast(steps=len(y_test), exog=exog_test)\n",
    "        metrics = calculate_metrics(y_test.values, forecast.values, \"ARIMAX(5,1,0)\", ticker, \"Price_Full_Sentiment\")\n",
    "        if metrics:\n",
    "            results.append(metrics)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        \n",
    "    return results\n",
    "\n",
    "print(\"All model-training functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6189ff2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Model Training Loop.\n",
      "Will train models for 4028 tickers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b516032b1523420289dbc6be0159af5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing All Tickers:   0%|          | 0/4028 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model Training Complete. Total results generated: 95064.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Main Training Loop\n",
    "print(\"Starting Model Training Loop.\")\n",
    "\n",
    "all_results = []\n",
    "# Get the unique, sorted list of all tickers\n",
    "ticker_list = sorted(df['ticker'].unique())\n",
    "print(f\"Will train models for {len(ticker_list)} tickers.\")\n",
    "\n",
    "for ticker in tqdm(ticker_list, desc=\"Processing All Tickers\"):\n",
    "    ticker_df = df[df['ticker'] == ticker]\n",
    "    \n",
    "    # 1. Split data for this ticker\n",
    "    split_idx = int(len(ticker_df) * 0.8)\n",
    "    train_df = ticker_df.iloc[:split_idx]\n",
    "    test_df = ticker_df.iloc[split_idx:]\n",
    "    \n",
    "    # Skip tickers with too little data\n",
    "    if len(train_df) < 50 or len(test_df) < 5:\n",
    "        continue\n",
    "        \n",
    "    # 2. Iterate through our 3 feature sets\n",
    "    for set_name, feature_cols in feature_sets.items():\n",
    "        \n",
    "        # 3. Run all model functions\n",
    "        all_results.extend( run_linear_models(train_df, test_df, feature_cols, set_name, ticker) )\n",
    "        all_results.extend( run_svm_models(train_df, test_df, feature_cols, set_name, ticker) )\n",
    "        \n",
    "        if set_name != \"Price_Base_Sentiment\":\n",
    "             all_results.extend( run_tree_models(train_df, test_df, feature_cols, set_name, ticker) )\n",
    "             all_results.extend( run_mlp_models(train_df, test_df, feature_cols, set_name, ticker) )\n",
    "    \n",
    "    # 4. Run ARIMA models\n",
    "    all_results.extend( run_arima_models(train_df, test_df, ticker) )\n",
    "\n",
    "print(f\"\\n Model Training Complete. Total results generated: {len(all_results)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42a6ea0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving all model results.\n",
      "Successfully saved 95064 results to c:\\Users\\18kyu\\Desktop\\Unishit\\IR\\4_Results\\Ablation_Runs\\03A_IR_Model_Performance.csv\n",
      "Saving results to 'results_ir_models' table in database.\n",
      "Successfully wrote results to database!\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Save Results to CSV\n",
    "print(\"Saving all model results.\")\n",
    "\n",
    "if all_results:\n",
    "    # 1. Convert to DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df['timestamp'] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # 2. Save to new CSV folder\n",
    "    csv_path = output_dir / \"03A_IR_Model_Performance.csv\"\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "    print(f\"Successfully saved {len(results_df)} results to {csv_path}\")\n",
    "    \n",
    "    # 3. Save to Database\n",
    "    print(\"Saving results to 'results_ir_models' table in database.\")\n",
    "    try:\n",
    "        results_df.to_sql(\n",
    "            'results_ir_models',\n",
    "            con=engine,\n",
    "            if_exists='replace',\n",
    "            index=False,\n",
    "            chunksize=1000\n",
    "        )\n",
    "        print(\"Successfully wrote results to database!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing results to database: {e}\")\n",
    "else:\n",
    "    print(\"No results were generated. Skipping save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3115719d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Cleaning Results for R analysis.\n",
      "Successfully cleaned and saved summary to c:\\Users\\18kyu\\Desktop\\Unishit\\IR\\4_Results\\cleaned_ablation_results.csv\n",
      "\n",
      " Phase 3A Complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Clean Results for R Analysis\n",
    "print(\"\\n Cleaning Results for R analysis.\")\n",
    "\n",
    "# Load saved file\n",
    "csv_path = output_dir / \"03A_IR_Model_Performance.csv\"\n",
    "df_raw_results = pd.read_csv(csv_path)\n",
    "\n",
    "# 1. Create a summary by model (mean of all tickers)\n",
    "model_summary = df_raw_results.groupby('model').agg(\n",
    "    rmse_mean=('rmse', 'mean'),\n",
    "    mae_mean=('mae', 'mean'),\n",
    "    r2_mean=('r2', 'mean'),\n",
    "    directional_accuracy_mean=('directional_accuracy', 'mean'),\n",
    "    model_count=('ticker', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# 2. Port the logic from clean_ablation_data\n",
    "def extract_model_info(model_name):\n",
    "    parts = model_name.split('_')\n",
    "    model_type = parts[0]\n",
    "    \n",
    "    if 'ARIMA' in model_type:\n",
    "        model_family = 'ARIMA+ARIMAX'\n",
    "        config = parts[1]\n",
    "        feature_set = '_'.join(parts[2:])\n",
    "    elif 'MLP' in model_type or 'XGBoost' in model_type:\n",
    "        model_family = model_type\n",
    "        config = parts[1]\n",
    "        feature_set = '_'.join(parts[2:])\n",
    "    else:\n",
    "        model_family = model_type.replace('LinearRegression', 'Linear Regression')\n",
    "        config = 'Standard'\n",
    "        feature_set = '_'.join(parts[1:])\n",
    "        \n",
    "    if 'Price_Full_Sentiment' in feature_set:\n",
    "        sentiment_type = 'Full Sentiment'\n",
    "    elif 'Price_Base_Sentiment' in feature_set:\n",
    "        sentiment_type = 'Base Sentiment'\n",
    "    elif 'Price_Only' in feature_set:\n",
    "        sentiment_type = 'Price Only'\n",
    "    else:\n",
    "        sentiment_type = 'Other'\n",
    "        \n",
    "    return model_family, config, sentiment_type\n",
    "\n",
    "model_info = model_summary['model'].apply(extract_model_info)\n",
    "model_summary['model_family'] = [info[0] for info in model_info]\n",
    "model_summary['model_config'] = [info[1] for info in model_info]\n",
    "model_summary['sentiment_type'] = [info[2] for info in model_info]\n",
    "model_summary['model_variation'] = model_summary.apply(\n",
    "    lambda row: f\"{row['model_family']}_{row['model_config']}\" if row['model_config'] != 'Standard' \n",
    "    else row['model_family'], axis=1\n",
    ")\n",
    "\n",
    "# 3. Calculate Improvements (relative to Price_Only)\n",
    "base_models = model_summary[model_summary['sentiment_type'] == 'Price Only'].copy()\n",
    "base_models = base_models.set_index('model_variation')[['rmse_mean', 'r2_mean']]\n",
    "\n",
    "def calculate_improvements(row):\n",
    "    model_var = row['model_variation']\n",
    "    if model_var in base_models.index and row['sentiment_type'] != 'Price Only':\n",
    "        base_rmse = base_models.loc[model_var, 'rmse_mean']\n",
    "        base_r2 = base_models.loc[model_var, 'r2_mean']\n",
    "        \n",
    "        rmse_imp = ((base_rmse - row['rmse_mean']) / base_rmse) * 100\n",
    "        r2_imp = ((row['r2_mean'] - base_r2) / abs(base_r2)) * 100\n",
    "        return rmse_imp, r2_imp\n",
    "    return 0, 0\n",
    "\n",
    "improvements = model_summary.apply(calculate_improvements, axis=1)\n",
    "model_summary['rmse_improvement_pct'] = [imp[0] for imp in improvements]\n",
    "model_summary['r2_improvement_pct'] = [imp[1] for imp in improvements]\n",
    "model_summary['rmse_improved'] = (model_summary['rmse_improvement_pct'] > 0).astype(int)\n",
    "model_summary['r2_improved'] = (model_summary['r2_improvement_pct'] > 0).astype(int)\n",
    "summary_csv_path = base_dir / \"4_Results\" / \"cleaned_ablation_results.csv\"\n",
    "model_summary.to_csv(summary_csv_path, index=False)\n",
    "\n",
    "print(f\"Successfully cleaned and saved summary to {summary_csv_path}\")\n",
    "print(\"\\n Phase 3A Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "097bc726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Performance Validation ---\n",
      "Successfully loaded cleaned summary file.\n",
      "Total model variations tested: 24\n",
      "\n",
      "=== Top 10 Models by R-squared (R²) ===\n",
      "                                  model     r2_mean  directional_accuracy_mean\n",
      "0               ARIMA(5,1,0)_Price_Only   -0.149811                  50.782327\n",
      "19                       SVR_Price_Only   -0.805193                  47.981469\n",
      "18             SVR_Price_Full_Sentiment   -0.860055                  47.973120\n",
      "17             SVR_Price_Base_Sentiment   -1.928091                  47.917084\n",
      "20   XGBoost_Light_Price_Full_Sentiment   -3.010012                  49.120065\n",
      "22  XGBoost_Medium_Price_Full_Sentiment   -5.533572                  48.913564\n",
      "12    RandomForest_Price_Full_Sentiment  -29.887717                  48.692338\n",
      "13              RandomForest_Price_Only -102.311530                  48.607447\n",
      "4                      Lasso_Price_Only -108.777695                  52.770582\n",
      "16                     Ridge_Price_Only -198.111618                  49.770201\n",
      "\n",
      "=== Top 10 Models by RMSE Improvement (vs. Price_Only) ===\n",
      "                                  model  rmse_improvement_pct  r2_improvement_pct\n",
      "22  XGBoost_Medium_Price_Full_Sentiment              9.876143           99.911430\n",
      "20   XGBoost_Light_Price_Full_Sentiment              8.481934           99.926352\n",
      "12    RandomForest_Price_Full_Sentiment              2.951348           70.787538\n",
      "18             SVR_Price_Full_Sentiment              0.033009           -6.813511\n",
      "17             SVR_Price_Base_Sentiment              0.005954         -139.456901\n",
      "0               ARIMA(5,1,0)_Price_Only              0.000000            0.000000\n",
      "7           LinearRegression_Price_Only              0.000000            0.000000\n",
      "1    ARIMAX(5,1,0)_Price_Full_Sentiment              0.000000            0.000000\n",
      "19                       SVR_Price_Only              0.000000            0.000000\n",
      "16                     Ridge_Price_Only              0.000000            0.000000\n",
      "\n",
      "=== Bottom 5 Models by R-squared (R²) ===\n",
      "                              model       r2_mean\n",
      "10   MLP_Small_Price_Full_Sentiment -68780.778646\n",
      "8   MLP_Medium_Price_Full_Sentiment -41618.777630\n",
      "23        XGBoost_Medium_Price_Only  -6247.668298\n",
      "21         XGBoost_Light_Price_Only  -4087.015667\n",
      "11             MLP_Small_Price_Only  -2902.615576\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Model Performance Validation ---\")\n",
    "\n",
    "# Load the summary file you just created in Cell 8\n",
    "summary_path = base_dir / \"4_Results\" / \"cleaned_ablation_results.csv\"\n",
    "\n",
    "try:\n",
    "    summary_df = pd.read_csv(summary_path)\n",
    "\n",
    "    print(\"Successfully loaded cleaned summary file.\")\n",
    "    print(f\"Total model variations tested: {len(summary_df)}\")\n",
    "\n",
    "    # Sort by the best R-squared (higher is better)\n",
    "    print(\"\\n=== Top 10 Models by R-squared (R²) ===\")\n",
    "    print(summary_df.sort_values(by='r2_mean', ascending=False).head(10)[\n",
    "        ['model', 'r2_mean', 'directional_accuracy_mean']\n",
    "    ].to_string())\n",
    "\n",
    "    # Sort by the best RMSE Improvement (higher is better)\n",
    "    print(\"\\n=== Top 10 Models by RMSE Improvement (vs. Price_Only) ===\")\n",
    "    print(summary_df.sort_values(by='rmse_improvement_pct', ascending=False).head(10)[\n",
    "        ['model', 'rmse_improvement_pct', 'r2_improvement_pct']\n",
    "    ].to_string())\n",
    "\n",
    "    # Check the worst models\n",
    "    print(\"\\n=== Bottom 5 Models by R-squared (R²) ===\")\n",
    "    print(summary_df.sort_values(by='r2_mean', ascending=True).head(5)[\n",
    "        ['model', 'r2_mean']\n",
    "    ].to_string())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Could not find the summary file at {summary_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
