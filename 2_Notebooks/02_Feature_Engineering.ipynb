{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7a1885a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and connection configured.\n",
      "Project Base Directory: c:\\Users\\18kyu\\Desktop\\Unishit\\IR\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries and Database Configuration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import math\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()\n",
    "print(\"Libraries imported and connection configured.\")\n",
    "\n",
    "# Database Configuration \n",
    "db_config = {\n",
    "    'host': '127.0.0.1',\n",
    "    'user': 'root',\n",
    "    'password': '',\n",
    "    'database': 'trading_system'\n",
    "}\n",
    "db_url = f\"mysql+pymysql://{db_config['user']}:{db_config['password']}@{db_config['host']}/{db_config['database']}\"\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "# Define Base Directory\n",
    "base_dir = Path.cwd().parent\n",
    "print(f\"Project Base Directory: {base_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18e7a33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from database.\n",
      "Loading 'stock_prices'.\n",
      "Loaded 8,986,458 price records.\n",
      "Loading 'headlines'.\n",
      "Loaded 1,147,268 headline records.\n",
      "Loading 'sentiment_scores'.\n",
      "Loaded 1,141,860 score records.\n",
      "\n",
      " All data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Data from Database\n",
    "print(\"Loading data from database.\")\n",
    "\n",
    "# 1. Load Stock Prices\n",
    "print(\"Loading 'stock_prices'.\")\n",
    "try:\n",
    "    sql_prices = \"\"\"\n",
    "    SELECT \n",
    "        ticker, date, \n",
    "        CAST(close AS DECIMAL(10, 4)) AS close, \n",
    "        CAST(volume AS SIGNED) AS volume \n",
    "    FROM stock_prices\n",
    "    \"\"\"\n",
    "    df_prices = pd.read_sql(sql_prices, con=engine)\n",
    "    df_prices['date'] = pd.to_datetime(df_prices['date'], errors='coerce').dt.date\n",
    "    df_prices = df_prices.dropna(subset=['date', 'close', 'volume'])\n",
    "    print(f\"Loaded {len(df_prices):,} price records.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading prices: {e}\")\n",
    "\n",
    "# 2. Load Headlines\n",
    "print(\"Loading 'headlines'.\")\n",
    "try:\n",
    "    sql_headlines = \"SELECT id, date, stock, headline FROM headlines\"\n",
    "    df_headlines = pd.read_sql(sql_headlines, con=engine, index_col='id')\n",
    "    df_headlines['date'] = pd.to_datetime(df_headlines['date'], errors='coerce').dt.date\n",
    "    df_headlines = df_headlines.dropna(subset=['date'])\n",
    "    print(f\"Loaded {len(df_headlines):,} headline records.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading headlines: {e}\")\n",
    "\n",
    "# 3. Load Sentiment Scores\n",
    "print(\"Loading 'sentiment_scores'.\")\n",
    "try:\n",
    "    sql_scores = \"SELECT headline_id, textblob_polarity, vader_compound, finbert_compound FROM sentiment_scores\"\n",
    "    df_scores = pd.read_sql(sql_scores, con=engine, index_col='headline_id')\n",
    "    print(f\"Loaded {len(df_scores):,} score records.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading scores: {e}\")\n",
    "\n",
    "print(\"\\n All data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3caca532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging scores with headlines.\n",
      "Merged 1141860 sentiment records with dates/stocks.\n",
      "Aggregating sentiment scores by stock and day.\n",
      "Created 786414 daily aggregated sentiment records.\n",
      "\n",
      "Merging daily prices and sentiment.\n",
      "Performing imputation of missing sentiment scores.\n",
      "Processing sentiment imputation by ticker.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f459f904ec4824a72ca753b2d6346a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating market-wide sentiment baseline.\n",
      "  Filling 4133432 remaining NaNs in textblob_polarity with market baseline: 0.0530\n",
      "  Filling 4133432 remaining NaNs in vader_compound with market baseline: 0.0891\n",
      "  Filling 4133432 remaining NaNs in finbert_compound with market baseline: 0.0383\n",
      "\n",
      "Creating sentiment quality features.\n",
      "\n",
      "Created final daily merged dataset with 8986458 rows.\n",
      "\n",
      "=== Imputation Quality Analysis ===\n",
      "textblob_polarity:\n",
      "  Non-zero values: 8,429,893 (93.8%)\n",
      "  Mean: 0.0474\n",
      "  Std: 0.0944\n",
      "vader_compound:\n",
      "  Non-zero values: 8,593,816 (95.6%)\n",
      "  Mean: 0.0905\n",
      "  Std: 0.1265\n",
      "finbert_compound:\n",
      "  Non-zero values: 8,348,197 (92.9%)\n",
      "  Mean: 0.0275\n",
      "  Std: 0.1915\n",
      "\n",
      "Days with actual news: 685,462 (7.6%)\n",
      "\n",
      "Sample data (AAL ticker):\n",
      "   ticker       date   close  finbert_compound  has_news  days_since_news  sentiment_freshness\n",
      "0     AAL 2010-01-04  4.4969          0.038326         0               21             0.049787\n",
      "1     AAL 2010-01-05  5.0060          0.038326         0               21             0.049787\n",
      "2     AAL 2010-01-06  4.7986          0.038326         0               21             0.049787\n",
      "3     AAL 2010-01-07  4.9400          0.038326         0               21             0.049787\n",
      "4     AAL 2010-01-08  4.8457          0.038326         0               21             0.049787\n",
      "5     AAL 2010-01-11  4.7514          0.038326         0               21             0.049787\n",
      "6     AAL 2010-01-12  4.7891          0.038326         0               21             0.049787\n",
      "7     AAL 2010-01-13  5.1662          0.038326         0               21             0.049787\n",
      "8     AAL 2010-01-14  5.2699          0.038326         0               21             0.049787\n",
      "9     AAL 2010-01-15  5.1851          0.038326         0               21             0.049787\n",
      "10    AAL 2010-01-19  5.3171          0.038326         0               21             0.049787\n",
      "11    AAL 2010-01-20  5.4113          0.038326         0               21             0.049787\n",
      "12    AAL 2010-01-21  5.1474          0.038326         0               21             0.049787\n",
      "13    AAL 2010-01-22  4.9400          0.038326         0               21             0.049787\n",
      "14    AAL 2010-01-25  4.9211          0.038326         0               21             0.049787\n",
      "15    AAL 2010-01-26  4.7608          0.038326         0               21             0.049787\n",
      "16    AAL 2010-01-27  4.5817          0.038326         0               21             0.049787\n",
      "17    AAL 2010-01-28  4.8457          0.038326         0               21             0.049787\n",
      "18    AAL 2010-01-29  5.0060          0.038326         0               21             0.049787\n",
      "19    AAL 2010-02-01  5.2794          0.038326         0               21             0.049787\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Impute Sentiment Scores and Merge with Prices\n",
    "print(\"Merging scores with headlines.\")\n",
    "\n",
    "# 1. Join scores with headlines\n",
    "df_sentiment_daily = df_headlines.join(df_scores, how='inner')\n",
    "print(f\"Merged {len(df_sentiment_daily)} sentiment records with dates/stocks.\")\n",
    "\n",
    "# 2. Aggregate sentiment by day\n",
    "print(\"Aggregating sentiment scores by stock and day.\")\n",
    "df_sentiment_agg = df_sentiment_daily.groupby(['stock', 'date']).agg(\n",
    "    textblob_polarity = ('textblob_polarity', 'mean'),\n",
    "    vader_compound = ('vader_compound', 'mean'),\n",
    "    finbert_compound = ('finbert_compound', 'mean'),\n",
    "    news_count = ('headline', 'count')\n",
    ").reset_index()\n",
    "print(f\"Created {len(df_sentiment_agg)} daily aggregated sentiment records.\")\n",
    "\n",
    "# 3. Merge daily sentiment with prices\n",
    "print(\"\\nMerging daily prices and sentiment.\")\n",
    "merged_daily_df = pd.merge(\n",
    "    df_prices,\n",
    "    df_sentiment_agg,\n",
    "    left_on=['ticker', 'date'],\n",
    "    right_on=['stock', 'date'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 4. Impute missing sentiment scores\n",
    "merged_daily_df = merged_daily_df.drop(columns=['stock'])\n",
    "merged_daily_df = merged_daily_df.sort_values(by=['ticker', 'date']).reset_index(drop=True)\n",
    "\n",
    "print(\"Performing imputation of missing sentiment scores.\")\n",
    "\n",
    "sentiment_cols = ['textblob_polarity', 'vader_compound', 'finbert_compound']\n",
    "\n",
    "def smart_sentiment_imputation(group, decay_window=21, baseline_window=90):\n",
    "    group = group.copy()\n",
    "    group = group.reset_index(drop=True) \n",
    "    \n",
    "    # Create 'has_news' flag\n",
    "    group['has_news'] = (group['news_count'] > 0).fillna(False).astype(int)\n",
    "    group['days_since_news'] = decay_window\n",
    "    \n",
    "    for col in sentiment_cols:\n",
    "        # Step 1: Calculate stock's historical baseline\n",
    "        news_only = group.loc[group['has_news'] == 1, col].copy()\n",
    "        news_only_baseline = news_only.rolling(\n",
    "            window=baseline_window, min_periods=5\n",
    "        ).mean()\n",
    "        \n",
    "        group[f'{col}_baseline'] = np.nan\n",
    "        group.loc[group['has_news'] == 1, f'{col}_baseline'] = news_only_baseline.values\n",
    "        group[f'{col}_baseline'] = group[f'{col}_baseline'].ffill()\n",
    "        \n",
    "        # Step 2: Apply decay\n",
    "        imputed_values = group[col].copy()\n",
    "        last_news_idx = None\n",
    "        last_news_value = None\n",
    "        \n",
    "        for i in range(len(group)):\n",
    "            if group['has_news'].iloc[i] == 1 and pd.notna(group[col].iloc[i]):\n",
    "                last_news_idx = i\n",
    "                last_news_value = group[col].iloc[i]\n",
    "                group.loc[i, 'days_since_news'] = 0 # Reset counter\n",
    "            elif last_news_idx is not None:\n",
    "                # We have seen news before\n",
    "                days_since = i - last_news_idx\n",
    "                group.loc[i, 'days_since_news'] = min(days_since, decay_window)\n",
    "                \n",
    "                if pd.isna(group[col].iloc[i]):\n",
    "                    if days_since <= decay_window:\n",
    "                        decay_factor = np.exp(-days_since / 7)\n",
    "                        baseline = group[f'{col}_baseline'].iloc[i]\n",
    "                        \n",
    "                        if pd.notna(baseline):\n",
    "                            imputed_values.iloc[i] = (decay_factor * last_news_value + \n",
    "                                                     (1 - decay_factor) * baseline)\n",
    "                        else:\n",
    "                            imputed_values.iloc[i] = decay_factor * last_news_value\n",
    "                    else:\n",
    "                        baseline = group[f'{col}_baseline'].iloc[i]\n",
    "                        if pd.notna(baseline):\n",
    "                            imputed_values.iloc[i] = baseline\n",
    "            else:\n",
    "                pass\n",
    "                    \n",
    "        group[col] = imputed_values\n",
    "    \n",
    "    group = group.drop(columns=[f'{col}_baseline' for col in sentiment_cols])\n",
    "    return group\n",
    "\n",
    "# Apply smart imputation\n",
    "print(\"Processing sentiment imputation by ticker.\")\n",
    "merged_daily_df = merged_daily_df.groupby('ticker', group_keys=False).progress_apply(\n",
    "    lambda x: smart_sentiment_imputation(x, decay_window=21, baseline_window=90)\n",
    ")\n",
    "\n",
    "# 5. Calculate market-wide baseline for any remaining NaNs\n",
    "print(\"\\nCalculating market-wide sentiment baseline.\")\n",
    "for col in sentiment_cols:\n",
    "    market_baseline = merged_daily_df.loc[\n",
    "        merged_daily_df['has_news'] == 1, col\n",
    "    ].mean()\n",
    "    \n",
    "    remaining_nas = merged_daily_df[col].isna().sum()\n",
    "    if remaining_nas > 0:\n",
    "        print(f\"  Filling {remaining_nas} remaining NaNs in {col} with market baseline: {market_baseline:.4f}\")\n",
    "        merged_daily_df[col] = merged_daily_df[col].fillna(market_baseline)\n",
    "\n",
    "# Fill news_count\n",
    "merged_daily_df['news_count'] = merged_daily_df['news_count'].fillna(0)\n",
    "\n",
    "# 6. Add sentiment features\n",
    "print(\"\\nCreating sentiment quality features.\")\n",
    "merged_daily_df['sentiment_freshness'] = np.exp(-merged_daily_df['days_since_news'] / 7)\n",
    "\n",
    "for col in sentiment_cols:\n",
    "    merged_daily_df[f'{col}_volatility'] = merged_daily_df.groupby('ticker')[col].transform(\n",
    "        lambda x: x.rolling(window=30, min_periods=5).std()\n",
    "    )\n",
    "    merged_daily_df[f'{col}_volatility'] = merged_daily_df[f'{col}_volatility'].fillna(\n",
    "        merged_daily_df[f'{col}_volatility'].median()\n",
    "    )\n",
    "\n",
    "# 7. Convert date back to datetime\n",
    "merged_daily_df['date'] = pd.to_datetime(merged_daily_df['date'])\n",
    "print(f\"\\nCreated final daily merged dataset with {len(merged_daily_df)} rows.\")\n",
    "\n",
    "# 8. Analyze imputation results\n",
    "print(\"\\n=== Imputation Quality Analysis ===\")\n",
    "for col in sentiment_cols:\n",
    "    non_zero = (merged_daily_df[col] != 0).sum()\n",
    "    pct_non_zero = (non_zero / len(merged_daily_df)) * 100\n",
    "    mean_val = merged_daily_df[col].mean()\n",
    "    std_val = merged_daily_df[col].std()\n",
    "    print(f\"{col}:\")\n",
    "    print(f\"  Non-zero values: {non_zero:,} ({pct_non_zero:.1f}%)\")\n",
    "    print(f\"  Mean: {mean_val:.4f}\")\n",
    "    print(f\"  Std: {std_val:.4f}\")\n",
    "\n",
    "print(f\"\\nDays with actual news: {merged_daily_df['has_news'].sum():,} ({(merged_daily_df['has_news'].sum()/len(merged_daily_df)*100):.1f}%)\")\n",
    "\n",
    "print(\"\\nSample data (AAL ticker):\")\n",
    "sample_df = merged_daily_df.query(\"ticker == 'AAL' and date >= '2010-01-04'\").head(20)\n",
    "print(sample_df[['ticker', 'date', 'close', 'finbert_compound', 'has_news', \n",
    "                 'days_since_news', 'sentiment_freshness']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06b697b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Weekly Tabular Dataset.\n",
      "Aggregating daily data to weekly and creating technical features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8571c2a9f84e389be4b654e1d7ffc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created final tabular dataset with 1855010 model-ready rows.\n",
      "\n",
      "Saved tabular (weekly) data to: c:\\Users\\18kyu\\Desktop\\Unishit\\IR\\1_IR_Data\\5_Merged_Data\\model_ready_features.csv\n",
      "Saving tabular data to 'model_features' table...\n",
      "Successfully wrote features to database!\n",
      "\n",
      "=== Feature Summary ===\n",
      "Total features: 28\n",
      "\n",
      "Feature columns:\n",
      "  Price/Technical: ['adj_close', 'volume', 'textblob_polarity_volatility', 'vader_compound_volatility', 'finbert_compound_volatility', 'prev_close', 'return', 'log_return', 'ma_3', 'ma_5', 'ma_10', 'volatility', 'rsi', 'momentum_3', 'momentum_5', 'finbert_momentum_3', 'vader_momentum_3', 'textblob_momentum_3']\n",
      "  Sentiment Base: ['textblob_polarity', 'vader_compound', 'finbert_compound']\n",
      "  Sentiment Advanced: ['news_count', 'days_since_news', 'sentiment_freshness', 'textblob_polarity_volatility', 'vader_compound_volatility', 'finbert_compound_volatility', 'volatility', 'momentum_3', 'momentum_5', 'finbert_momentum_3', 'vader_momentum_3', 'textblob_momentum_3']\n",
      "\n",
      "Target variable: target_return\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Create Weekly Tabular Dataset\n",
    "print(\"Creating Weekly Tabular Dataset.\")\n",
    "\n",
    "# 1. Define Feature Functions\n",
    "def calculate_rsi(prices, window=14):\n",
    "    delta = prices.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def create_weekly_features(group):\n",
    "    # a) Resample daily data to weekly\n",
    "    group = group.set_index('date')\n",
    "    \n",
    "    # Add all sentiment and technical features to the aggregation dictionary\n",
    "    weekly_group = group.resample('W').agg(\n",
    "        # Price\n",
    "        adj_close = ('close', 'last'), \n",
    "        volume = ('volume', 'sum'),\n",
    "        \n",
    "        # Base Sentiment\n",
    "        textblob_polarity = ('textblob_polarity', 'mean'),\n",
    "        vader_compound = ('vader_compound', 'mean'),\n",
    "        finbert_compound = ('finbert_compound', 'mean'),\n",
    "        \n",
    "        # News Count / Freshness\n",
    "        news_count = ('news_count', 'sum'),\n",
    "        has_news = ('has_news', 'max'), # 1 if any news in week\n",
    "        days_since_news = ('days_since_news', 'mean'), # Avg staleness\n",
    "        sentiment_freshness = ('sentiment_freshness', 'mean'),\n",
    "        \n",
    "        # Sentiment Volatility\n",
    "        textblob_polarity_volatility = ('textblob_polarity_volatility', 'mean'),\n",
    "        vader_compound_volatility = ('vader_compound_volatility', 'mean'),\n",
    "        finbert_compound_volatility = ('finbert_compound_volatility', 'mean')\n",
    "    )\n",
    "    \n",
    "    # Drop weeks with no trading\n",
    "    weekly_group = weekly_group.dropna(subset=['adj_close']) \n",
    "    \n",
    "    # b) Create technical features \n",
    "    df = weekly_group.copy()\n",
    "    df['prev_close'] = df['adj_close'].shift(1)\n",
    "    df['return'] = df['adj_close'].pct_change()\n",
    "    df['log_return'] = np.log(df['adj_close'] / df['prev_close'])\n",
    "    df['ma_3'] = df['prev_close'].rolling(3).mean()\n",
    "    df['ma_5'] = df['prev_close'].rolling(5).mean()\n",
    "    df['ma_10'] = df['prev_close'].rolling(10).mean()\n",
    "    df['volatility'] = df['return'].rolling(5).std()\n",
    "    df['rsi'] = calculate_rsi(df['prev_close'])\n",
    "    df['momentum_3'] = df['adj_close'] / df['adj_close'].shift(3) - 1\n",
    "    df['momentum_5'] = df['adj_close'] / df['adj_close'].shift(5) - 1\n",
    "    \n",
    "    # c) Create Sentiment Momentum Features\n",
    "    # Captures whether sentiment is improving or deteriorating\n",
    "    df['finbert_momentum_3'] = df['finbert_compound'] - df['finbert_compound'].shift(3)\n",
    "    df['vader_momentum_3'] = df['vader_compound'] - df['vader_compound'].shift(3)\n",
    "    df['textblob_momentum_3'] = df['textblob_polarity'] - df['textblob_polarity'].shift(3)\n",
    "    \n",
    "    # d) Create Target Variable\n",
    "    df['target_return'] = df['return'].shift(-1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 2. Apply to all tickers\n",
    "print(\"Aggregating daily data to weekly and creating technical features...\")\n",
    "all_features_df = merged_daily_df.groupby('ticker').progress_apply(create_weekly_features)\n",
    "\n",
    "# 3. Clean and Save\n",
    "final_model_data = all_features_df.dropna()\n",
    "final_model_data = final_model_data.reset_index() \n",
    "print(f\"Created final tabular dataset with {len(final_model_data)} model-ready rows.\")\n",
    "\n",
    "# 4. Save to CSV\n",
    "output_dir = base_dir / \"1_IR_Data\" / \"5_Merged_Data\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "csv_path = output_dir / \"model_ready_features.csv\"\n",
    "final_model_data.to_csv(csv_path, index=False)\n",
    "print(f\"\\nSaved tabular (weekly) data to: {csv_path}\")\n",
    "\n",
    "# 5. Save to Database\n",
    "print(\"Saving tabular data to 'model_features' table...\")\n",
    "try:\n",
    "    # Rollback any pending transactions first\n",
    "    try:\n",
    "        engine.dispose()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Recreate engine connection\n",
    "    engine = create_engine(db_url)\n",
    "    \n",
    "    # Convert date to string before saving\n",
    "    final_model_data_db = final_model_data.copy()\n",
    "    final_model_data_db['date'] = pd.to_datetime(final_model_data_db['date']).dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    final_model_data_db.to_sql(\n",
    "        'model_features',\n",
    "        con=engine,\n",
    "        if_exists='replace',\n",
    "        index=False,\n",
    "        chunksize=1000\n",
    "    )\n",
    "    print(\"Successfully wrote features to database!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to database: {e}\")\n",
    "    print(\"Data was still saved to CSV successfully.\")\n",
    "\n",
    "# 6. Display Feature Summary\n",
    "print(\"\\n=== Feature Summary ===\")\n",
    "print(f\"Total features: {len(final_model_data.columns)}\")\n",
    "print(\"\\nFeature columns:\")\n",
    "feature_cols = [col for col in final_model_data.columns if col not in ['ticker', 'date', 'target_return']]\n",
    "print(f\"  Price/Technical: {[c for c in feature_cols if any(x in c for x in ['close', 'volume', 'return', 'ma_', 'rsi', 'momentum', 'volatility'])]}\")\n",
    "print(f\"  Sentiment Base: {[c for c in feature_cols if any(x in c for x in ['textblob_polarity', 'vader_compound', 'finbert_compound']) and 'volatility' not in c and 'momentum' not in c]}\")\n",
    "print(f\"  Sentiment Advanced: {[c for c in feature_cols if any(x in c for x in ['momentum', 'volatility', 'freshness', 'news_count', 'days_since'])]}\")\n",
    "print(f\"\\nTarget variable: target_return\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b030c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Creating Daily Sequence Datasets.\n",
      "Creating daily target variable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd31b383c5b54cc19f8aaaff9121e7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving individual ticker files to c:\\Users\\18kyu\\Desktop\\Unishit\\IR\\1_IR_Data\\6_Sequence_Data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0217330dea24e6a8696a507b6fd7505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving sequence files:   0%|          | 0/4041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4041 individual ticker files for sequence modeling.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Create Daily Sequence Datasets\n",
    "print(\"\\n Creating Daily Sequence Datasets.\")\n",
    "\n",
    "# 1. Define Daily Feature Function\n",
    "def create_daily_target(group):\n",
    "    \"\"\"Creates the target variable for sequence models\"\"\"\n",
    "    df = group.copy()\n",
    "    df['return'] = df['close'].pct_change()\n",
    "    df['target_return'] = df['return'].shift(-1)\n",
    "    return df\n",
    "\n",
    "print(\"Creating daily target variable.\")\n",
    "daily_features_df = merged_daily_df.groupby('ticker').progress_apply(create_daily_target)\n",
    "daily_features_df = daily_features_df.dropna()\n",
    "daily_features_df = daily_features_df.reset_index(drop=True)\n",
    "\n",
    "# 2. Save one CSV file per ticker\n",
    "output_seq_dir = base_dir / \"1_IR_Data\" / \"6_Sequence_Data\"\n",
    "output_seq_dir.mkdir(exist_ok=True)\n",
    "print(f\"Saving individual ticker files to {output_seq_dir}.\")\n",
    "\n",
    "tickers_with_data = daily_features_df['ticker'].unique()\n",
    "\n",
    "for ticker in tqdm(tickers_with_data, desc=\"Saving sequence files\"):\n",
    "    ticker_df = daily_features_df[daily_features_df['ticker'] == ticker]\n",
    "    \n",
    "    if len(ticker_df) > 50:\n",
    "        file_path = output_seq_dir / f\"{ticker}.csv\"\n",
    "        ticker_df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Saved {len(tickers_with_data)} individual ticker files for sequence modeling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa9a4356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Saving Daily Merged Data to Database.\n",
      "Writing 'data_daily_merged' to the database.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c546154355412685b2f1fa3f6fa469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing Daily Data:   0%|          | 1/1798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote 'data_daily_merged' to the database!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Save Daily Merged Data to Database\n",
    "print(\"\\n Saving Daily Merged Data to Database.\")\n",
    "\n",
    "try:\n",
    "    merged_daily_df['date'] = merged_daily_df['date'].dt.strftime('%Y-%m-%d')        \n",
    "    print(\"Writing 'data_daily_merged' to the database.\")\n",
    "\n",
    "    chunk_size = 5000 \n",
    "    total_chunks = math.ceil(len(merged_daily_df) / chunk_size)\n",
    "    chunks = np.array_split(merged_daily_df, total_chunks)\n",
    "    \n",
    "    # Write the first chunk (replace)\n",
    "    first_chunk = chunks[0]\n",
    "    first_chunk.to_sql(\n",
    "        'data_daily_merged', \n",
    "        con=engine, \n",
    "        if_exists='replace', \n",
    "        index=False, \n",
    "        chunksize=1000\n",
    "    )\n",
    "    \n",
    "    # Write remaining chunks (append)\n",
    "    for chunk in tqdm(chunks[1:], desc=\"Writing Daily Data\", total=total_chunks, initial=1):\n",
    "        chunk.to_sql(\n",
    "            'data_daily_merged', \n",
    "            con=engine, \n",
    "            if_exists='append', \n",
    "            index=False, \n",
    "            chunksize=1000\n",
    "        )\n",
    "    \n",
    "    print(\"Successfully wrote 'data_daily_merged' to the database!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error writing daily data to database: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
