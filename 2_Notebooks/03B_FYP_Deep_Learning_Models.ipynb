{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d84ab60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n",
      "TensorFlow version: 2.20.0\n",
      "Results will be saved to: c:\\Users\\18kyu\\Desktop\\Unishit\\IR\\4_Results\\Deep_Learning_Results\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries and Configuration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Database\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Deep Learning \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Conv1D, GRU, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"Libraries imported.\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Database Configuration\n",
    "db_config = {\n",
    "    'host': '127.0.0.1',\n",
    "    'user': 'root',\n",
    "    'password': '',\n",
    "    'database': 'trading_system'\n",
    "}\n",
    "db_url = f\"mysql+pymysql://{db_config['user']}:{db_config['password']}@{db_config['host']}/{db_config['database']}\"\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "# Path Definition\n",
    "base_dir = Path.cwd().parent\n",
    "output_dir = base_dir / \"4_Results\" / \"Deep_Learning_Results\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Results will be saved to: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d80e8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sequence data from database.\n",
      "Successfully loaded 8,986,458 rows.\n",
      "Date range: 2010-01-04 to 2020-06-03\n",
      "Number of unique tickers: 4041\n",
      "\n",
      "Available columns:\n",
      "['ticker', 'date', 'close', 'volume', 'textblob_polarity', 'vader_compound', 'finbert_compound', 'news_count', 'has_news', 'days_since_news', 'sentiment_freshness', 'textblob_polarity_volatility', 'vader_compound_volatility', 'finbert_compound_volatility']\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Sequence Data\n",
    "print(\"Loading sequence data from database.\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_sql(\"SELECT * FROM data_daily_merged\", con=engine)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values(by=['ticker', 'date'])\n",
    "    \n",
    "    print(f\"Successfully loaded {len(df):,} rows.\")\n",
    "    print(f\"Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
    "    print(f\"Number of unique tickers: {df['ticker'].nunique()}\")\n",
    "    \n",
    "    print(\"\\nAvailable columns:\")\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading data: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4092a480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining feature sets for sequence models.\n",
      "Total feature sets: 3\n",
      "- Price_Only: 2 features\n",
      "- Price_Base_Sentiment: 5 features\n",
      "- Price_Full_Sentiment: 12 features\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Define Feature Sets for Deep Learning\n",
    "print(\"Defining feature sets for sequence models.\")\n",
    "\n",
    "# 1. Price_Only - Technical features\n",
    "price_features = ['close', 'volume']\n",
    "\n",
    "# 2. Price_Base_Sentiment - Price + basic sentiment scores\n",
    "base_sentiment_features = price_features + [\n",
    "    'textblob_polarity',\n",
    "    'vader_compound',\n",
    "    'finbert_compound'\n",
    "]\n",
    "\n",
    "# 3. Price_Full_Sentiment - Price + all sentiment features\n",
    "full_sentiment_features = price_features + [\n",
    "    'textblob_polarity',\n",
    "    'vader_compound',\n",
    "    'finbert_compound',\n",
    "    'news_count',\n",
    "    'has_news',\n",
    "    'days_since_news',\n",
    "    'sentiment_freshness',\n",
    "    'textblob_polarity_volatility',\n",
    "    'vader_compound_volatility',\n",
    "    'finbert_compound_volatility'\n",
    "]\n",
    "\n",
    "feature_sets = {\n",
    "    \"Price_Only\": price_features,\n",
    "    \"Price_Base_Sentiment\": base_sentiment_features,\n",
    "    \"Price_Full_Sentiment\": full_sentiment_features\n",
    "}\n",
    "\n",
    "print(f\"Total feature sets: {len(feature_sets)}\")\n",
    "print(f\"- Price_Only: {len(price_features)} features\")\n",
    "print(f\"- Price_Base_Sentiment: {len(base_sentiment_features)} features\")\n",
    "print(f\"- Price_Full_Sentiment: {len(full_sentiment_features)} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8bcb7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence generation functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Sequence Generation Functions\n",
    "def create_sequences(data, feature_cols, target_col='close', sequence_length=60):\n",
    "    \"\"\"\n",
    "    Create sequences for time series prediction.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(sequence_length, len(data)):\n",
    "        X.append(data[feature_cols].iloc[i-sequence_length:i].values)\n",
    "        current_close = data[target_col].iloc[i-1]\n",
    "        next_close = data[target_col].iloc[i]\n",
    "        return_val = (next_close - current_close) / current_close\n",
    "        y.append(return_val)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def prepare_data_for_ticker(ticker, df, feature_cols, sequence_length=60, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Prepare train/test sequences for a single ticker.\n",
    "    \"\"\"\n",
    "    ticker_df = df[df['ticker'] == ticker].copy()\n",
    "    ticker_df = ticker_df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    if len(ticker_df) < sequence_length + 50:\n",
    "        return None\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    ticker_df[feature_cols] = scaler.fit_transform(ticker_df[feature_cols])\n",
    "    \n",
    "    X, y = create_sequences(ticker_df, feature_cols, sequence_length=sequence_length)\n",
    "    split_idx = int(len(X) * train_ratio)\n",
    "    \n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    if len(X_test) < 5:\n",
    "        return None\n",
    "    \n",
    "    return {'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test}\n",
    "\n",
    "print(\"Sequence generation functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cc282a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture builders defined.\n",
      "Available architectures: ['LSTM', 'BiLSTM', 'CNN1D', 'GRU']\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Model Architecture Builders\n",
    "def build_lstm_model(input_shape, units=50):\n",
    "    model = Sequential([\n",
    "        LSTM(units, input_shape=input_shape, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(25, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def build_bilstm_model(input_shape, units=50):\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(units, return_sequences=False), input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        Dense(25, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def build_cnn1d_model(input_shape, filters=64):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=filters, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        Conv1D(filters=filters//2, kernel_size=3, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        keras.layers.Flatten(),\n",
    "        Dense(50, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def build_gru_model(input_shape, units=50):\n",
    "    model = Sequential([\n",
    "        GRU(units, input_shape=input_shape, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(25, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "model_builders = {\n",
    "    'LSTM': build_lstm_model,\n",
    "    'BiLSTM': build_bilstm_model,\n",
    "    'CNN1D': build_cnn1d_model,\n",
    "    'GRU': build_gru_model\n",
    "}\n",
    "\n",
    "print(\"Model architecture builders defined.\")\n",
    "print(f\"Available architectures: {list(model_builders.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98242d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Helper Functions\n",
    "def calculate_metrics(y_true, y_pred, model_name, ticker, feature_set_name):\n",
    "    mask = ~(np.isnan(y_true) | np.isnan(y_pred) | np.isinf(y_true) | np.isinf(y_pred))\n",
    "    y_true_clean = y_true[mask]\n",
    "    y_pred_clean = y_pred[mask]\n",
    "    \n",
    "    if len(y_true_clean) == 0:\n",
    "        return None\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))\n",
    "    mae = mean_absolute_error(y_true_clean, y_pred_clean)\n",
    "    r2 = r2_score(y_true_clean, y_pred_clean)\n",
    "    correct_direction = np.sign(y_true_clean) == np.sign(y_pred_clean)\n",
    "    directional_accuracy = np.mean(correct_direction) * 100\n",
    "    \n",
    "    return {\n",
    "        'ticker': ticker,\n",
    "        'model': f\"{model_name}_{feature_set_name}\",\n",
    "        'feature_set': feature_set_name,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'directional_accuracy': directional_accuracy\n",
    "    }\n",
    "\n",
    "print(\"Helper functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e9fd423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Deep Learning Model Training Loop.\n",
      "Total available tickers: 4041\n",
      "Training on: 100 tickers.\n",
      "\n",
      "Pre-filtering tickers by data availability.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593e32fb03a6406ca5f31a1f1a8fe1f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Tickers:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid tickers after filtering: 98 (removed 2 with insufficient data)\n",
      "\n",
      "Pre-building model templates.\n",
      "Created 12 model templates.\n",
      "Expected total models: 1176 = 1176\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44f2c69f9294786a2043817de86a986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Tickers:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\18kyu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "\n",
      "Deep Learning Training Complete. Total results: 1176\n",
      "Expected: ~1176, Actual: 1176\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Main Training Loop (OPTIMIZED - Prevents Retracing)\n",
    "print(\"Starting Deep Learning Model Training Loop.\")\n",
    "\n",
    "all_results = []\n",
    "sequence_length = 60\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "# Get ticker list\n",
    "ticker_list = sorted(df['ticker'].unique())\n",
    "print(f\"Total available tickers: {len(ticker_list)}\")\n",
    "\n",
    "# Setting ticker limit\n",
    "ticker_list = ticker_list[:100]  # Reduced to 100 for faster testing\n",
    "print(f\"Training on: {len(ticker_list)} tickers.\")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True, verbose=0)\n",
    "\n",
    "# Prefilter tickers by data availability\n",
    "print(\"\\nPre-filtering tickers by data availability.\")\n",
    "valid_tickers = []\n",
    "for ticker in tqdm(ticker_list, desc=\"Validating Tickers\"):\n",
    "    ticker_df = df[df['ticker'] == ticker]\n",
    "    if len(ticker_df) >= sequence_length + 100:  # Ensure enough data\n",
    "        valid_tickers.append(ticker)\n",
    "\n",
    "print(f\"Valid tickers after filtering: {len(valid_tickers)} (removed {len(ticker_list) - len(valid_tickers)} with insufficient data)\")\n",
    "ticker_list = valid_tickers\n",
    "\n",
    "# Adjust model weights rather than rebuilding\n",
    "print(\"\\nPre-building model templates.\")\n",
    "model_templates = {}\n",
    "\n",
    "for set_name, feature_cols in feature_sets.items():\n",
    "    n_features = len(feature_cols)\n",
    "    input_shape = (sequence_length, n_features)\n",
    "    \n",
    "    for model_name, builder_func in model_builders.items():\n",
    "        cache_key = f\"{model_name}_{set_name}\"\n",
    "        # Build model and save initial weights\n",
    "        model = builder_func(input_shape)\n",
    "        initial_weights = model.get_weights()\n",
    "        \n",
    "        model_templates[cache_key] = {\n",
    "            'model': model,\n",
    "            'initial_weights': initial_weights,\n",
    "            'input_shape': input_shape\n",
    "        }\n",
    "\n",
    "print(f\"Created {len(model_templates)} model templates.\")\n",
    "print(f\"Expected total models: {len(ticker_list) * 12} = {len(ticker_list) * 12}\")\n",
    "\n",
    "# Training Loop\n",
    "for ticker in tqdm(ticker_list, desc=\"Processing Tickers\"):\n",
    "    \n",
    "    for set_name, feature_cols in feature_sets.items():\n",
    "        \n",
    "        data = prepare_data_for_ticker(\n",
    "            ticker, df, feature_cols, \n",
    "            sequence_length=sequence_length\n",
    "        )\n",
    "        \n",
    "        if data is None:\n",
    "            continue\n",
    "        \n",
    "        X_train = data['X_train']\n",
    "        X_test = data['X_test']\n",
    "        y_train = data['y_train']\n",
    "        y_test = data['y_test']\n",
    "        \n",
    "        # Skip if unexpected shapes\n",
    "        if len(X_test) < 5 or X_train.shape[1] != sequence_length:\n",
    "            continue\n",
    "        \n",
    "        for model_name in model_builders.keys():\n",
    "            try:\n",
    "                cache_key = f\"{model_name}_{set_name}\"\n",
    "                template = model_templates[cache_key]\n",
    "                model = template['model']\n",
    "                \n",
    "                # Reset to initial weights\n",
    "                model.set_weights(template['initial_weights'])\n",
    "                \n",
    "                # Train\n",
    "                history = model.fit(\n",
    "                    X_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stop],\n",
    "                    verbose=0,\n",
    "                    validation_split=0.1\n",
    "                )\n",
    "                \n",
    "                # Predict\n",
    "                y_pred = model.predict(X_test, verbose=0).flatten()\n",
    "                \n",
    "                # Calculate metrics\n",
    "                metrics = calculate_metrics(\n",
    "                    y_test, y_pred, \n",
    "                    model_name, ticker, set_name\n",
    "                )\n",
    "                \n",
    "                if metrics:\n",
    "                    all_results.append(metrics)\n",
    "                \n",
    "                del history\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    # Clear session every 10 tickers to prevent memory buildup\n",
    "    if (ticker_list.index(ticker) + 1) % 10 == 0:\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "print(f\"\\nDeep Learning Training Complete. Total results: {len(all_results)}\")\n",
    "print(f\"Expected: ~{len(ticker_list) * 12}, Actual: {len(all_results)}\")\n",
    "\n",
    "# Final cleanup\n",
    "for template in model_templates.values():\n",
    "    del template['model']\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb5f7a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results.\n",
      "Successfully saved 1176 results to c:\\Users\\18kyu\\Desktop\\Unishit\\IR\\4_Results\\Deep_Learning_Results\\03B_Deep_Learning_Performance.csv\n",
      "Saving results to 'results_deep_learning_models' table in database.\n",
      "Successfully wrote results to database!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Save Results\n",
    "print(\"Saving results.\")\n",
    "\n",
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df['timestamp'] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    csv_path = output_dir / \"03B_Deep_Learning_Performance.csv\"\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "    print(f\"Successfully saved {len(results_df)} results to {csv_path}\")\n",
    "    \n",
    "    print(\"Saving results to 'results_deep_learning_models' table in database.\")\n",
    "    try:\n",
    "        results_df.to_sql(\n",
    "            'results_deep_learning_models',\n",
    "            con=engine,\n",
    "            if_exists='replace',\n",
    "            index=False,\n",
    "            chunksize=1000\n",
    "        )\n",
    "        print(\"Successfully wrote results to database!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing results to database: {e}\")\n",
    "else:\n",
    "    print(\"No results were generated. Check for errors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0755c930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Deep Learning Model Performance Summary\n",
      "\n",
      " Performance by Model Type:\n",
      "                               rmse                      mae                    r2               directional_accuracy        \n",
      "                               mean      std     min    mean      std         mean           std                 mean     std\n",
      "model                                                                                                                        \n",
      "BiLSTM_Price_Base_Sentiment  5.4261  23.0592  0.0309  1.1558   3.4987   -3295.3750  3.003249e+04              48.3175  4.4621\n",
      "BiLSTM_Price_Full_Sentiment  5.8352  23.6693  0.0594  1.2271   4.2461    -991.7805  3.889152e+03              48.5631  4.7660\n",
      "BiLSTM_Price_Only            5.2559  23.1662  0.0498  1.0920   3.6893   -3383.7489  3.199807e+04              48.4020  5.3864\n",
      "CNN1D_Price_Base_Sentiment   5.9749  25.7129  0.0215  1.5577   9.6310  -61958.5291  6.096440e+05              48.0968  4.8229\n",
      "CNN1D_Price_Full_Sentiment   6.3604  28.1994  0.0181  1.5502  10.0899 -117012.2451  1.152569e+06              48.4627  4.9065\n",
      "CNN1D_Price_Only             8.0083  29.4903  0.0090  2.6263  16.0922 -140200.9286  1.143740e+06              48.4698  5.2204\n",
      "GRU_Price_Base_Sentiment     5.1182  22.7870  0.0508  0.9246   1.9381   -1714.7285  1.066276e+04              47.4720  4.6981\n",
      "GRU_Price_Full_Sentiment     6.4187  23.7402  0.0764  1.7889   6.0336  -10066.6552  6.934130e+04              49.2737  5.1719\n",
      "GRU_Price_Only               5.1677  23.3265  0.0251  1.0696   4.0424  -11666.6800  1.150533e+05              48.5973  5.3930\n",
      "LSTM_Price_Base_Sentiment    5.2580  22.8600  0.0499  0.8727   1.9074    -863.7980  4.217361e+03              48.4260  4.8544\n",
      "LSTM_Price_Full_Sentiment    5.4812  23.0642  0.0346  0.9070   2.3694   -3022.4314  2.689929e+04              48.0077  5.0869\n",
      "LSTM_Price_Only              4.5507  22.8005  0.0313  0.5614   1.2851     -42.2310  2.221836e+02              48.2010  5.0402\n",
      "\n",
      " Top 10 Models by RMSE\n",
      "                            model ticker      rmse  directional_accuracy\n",
      "302              CNN1D_Price_Only   ACFN  0.008976             35.294118\n",
      "470              CNN1D_Price_Only   ACUR  0.015357             42.544732\n",
      "1102   CNN1D_Price_Full_Sentiment    AGQ  0.018065             52.436647\n",
      "474    CNN1D_Price_Base_Sentiment   ACUR  0.021492             41.749503\n",
      "306    CNN1D_Price_Base_Sentiment   ACFN  0.021759             33.921569\n",
      "303                GRU_Price_Only   ACFN  0.025105             33.725490\n",
      "310    CNN1D_Price_Full_Sentiment   ACFN  0.026434             35.098039\n",
      "305   BiLSTM_Price_Base_Sentiment   ACFN  0.030945             34.705882\n",
      "300               LSTM_Price_Only   ACFN  0.031330             34.705882\n",
      "926              CNN1D_Price_Only   AFMD  0.033149             47.292419\n",
      "\n",
      " Comparison with Traditional Models\n",
      "Traditional Models - Best RMSE: 0.0002\n",
      "Deep Learning - Best RMSE: 0.0090\n",
      "\n",
      "Traditional Models - Avg Dir. Acc: 49.71%\n",
      "Deep Learning - Avg Dir. Acc: 48.36%\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Summary Statistics\n",
    "print(\"\\n Deep Learning Model Performance Summary\")\n",
    "\n",
    "if len(all_results) > 0:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    summary = results_df.groupby('model').agg({\n",
    "        'rmse': ['mean', 'std', 'min'],\n",
    "        'mae': ['mean', 'std'],\n",
    "        'r2': ['mean', 'std'],\n",
    "        'directional_accuracy': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    \n",
    "    print(\"\\n Performance by Model Type:\")\n",
    "    print(summary.to_string())\n",
    "    \n",
    "    print(\"\\n Top 10 Models by RMSE\")\n",
    "    top_models = results_df.nsmallest(10, 'rmse')[['model', 'ticker', 'rmse', 'directional_accuracy']]\n",
    "    print(top_models.to_string())\n",
    "    \n",
    "    try:\n",
    "        trad_results = pd.read_sql(\"SELECT * FROM results_ir_models\", con=engine)\n",
    "        print(\"\\n Comparison with Traditional Models\")\n",
    "        print(f\"Traditional Models - Best RMSE: {trad_results['rmse'].min():.4f}\")\n",
    "        print(f\"Deep Learning - Best RMSE: {results_df['rmse'].min():.4f}\")\n",
    "        print(f\"\\nTraditional Models - Avg Dir. Acc: {trad_results['directional_accuracy'].mean():.2f}%\")\n",
    "        print(f\"Deep Learning - Avg Dir. Acc: {results_df['directional_accuracy'].mean():.2f}%\")\n",
    "    except:\n",
    "        print(\"\\n(Traditional model results not found for comparison)\")\n",
    "else:\n",
    "    print(\"No results to summarize.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "163c31b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing Deep Learning Training (Batch 2).\n",
      "Already trained on 98 tickers.\n",
      "Remaining tickers available: 3943\n",
      "Training on next 100 tickers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd9741d760943f6b7ac15e18028961d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Tickers:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid tickers: 98\n",
      "\n",
      "Rebuilding model templates.\n",
      "Created 12 model templates.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1950bd0f96b4c379479c35f94a23c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Additional Tickers:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 2 Training Complete. New results: 1176\n",
      "Updated results file. Total results: 2352\n",
      "Database update error: Can't reconnect until invalid transaction is rolled back.  Please rollback() fully before proceeding (Background on this error at: https://sqlalche.me/e/20/8s2b)\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Continue Training on Additional Tickers (Batch 2)\n",
    "print(\"Continuing Deep Learning Training (Batch 2).\")\n",
    "\n",
    "# Load existing results to avoid retraining\n",
    "existing_results_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
    "trained_tickers = set(existing_results_df['ticker'].unique())\n",
    "print(f\"Already trained on {len(trained_tickers)} tickers.\")\n",
    "\n",
    "# Get next batch of tickers\n",
    "all_tickers = sorted(df['ticker'].unique())\n",
    "remaining_tickers = [t for t in all_tickers if t not in trained_tickers]\n",
    "print(f\"Remaining tickers available: {len(remaining_tickers)}\")\n",
    "\n",
    "# Set batch size (start conservative)\n",
    "BATCH_SIZE = 100  # Start with 100, can increase to 150-200 if stable\n",
    "next_batch = remaining_tickers[:BATCH_SIZE]\n",
    "print(f\"Training on next {len(next_batch)} tickers.\")\n",
    "\n",
    "# Pre-filter for data availability\n",
    "valid_tickers = []\n",
    "for ticker in tqdm(next_batch, desc=\"Validating Tickers\"):\n",
    "    ticker_df = df[df['ticker'] == ticker]\n",
    "    if len(ticker_df) >= sequence_length + 100:\n",
    "        valid_tickers.append(ticker)\n",
    "\n",
    "print(f\"Valid tickers: {len(valid_tickers)}\")\n",
    "\n",
    "# Rebuild model templates (same as before)\n",
    "print(\"\\nRebuilding model templates.\")\n",
    "model_templates = {}\n",
    "\n",
    "for set_name, feature_cols in feature_sets.items():\n",
    "    n_features = len(feature_cols)\n",
    "    input_shape = (sequence_length, n_features)\n",
    "    \n",
    "    for model_name, builder_func in model_builders.items():\n",
    "        cache_key = f\"{model_name}_{set_name}\"\n",
    "        model = builder_func(input_shape)\n",
    "        initial_weights = model.get_weights()\n",
    "        \n",
    "        model_templates[cache_key] = {\n",
    "            'model': model,\n",
    "            'initial_weights': initial_weights,\n",
    "            'input_shape': input_shape\n",
    "        }\n",
    "\n",
    "print(f\"Created {len(model_templates)} model templates.\")\n",
    "\n",
    "# Training Loop \n",
    "batch_results = []\n",
    "\n",
    "for ticker in tqdm(valid_tickers, desc=\"Processing Additional Tickers\"):\n",
    "    \n",
    "    for set_name, feature_cols in feature_sets.items():\n",
    "        \n",
    "        data = prepare_data_for_ticker(\n",
    "            ticker, df, feature_cols, \n",
    "            sequence_length=sequence_length\n",
    "        )\n",
    "        \n",
    "        if data is None:\n",
    "            continue\n",
    "        \n",
    "        X_train = data['X_train']\n",
    "        X_test = data['X_test']\n",
    "        y_train = data['y_train']\n",
    "        y_test = data['y_test']\n",
    "        \n",
    "        if len(X_test) < 5 or X_train.shape[1] != sequence_length:\n",
    "            continue\n",
    "        \n",
    "        for model_name in model_builders.keys():\n",
    "            try:\n",
    "                cache_key = f\"{model_name}_{set_name}\"\n",
    "                template = model_templates[cache_key]\n",
    "                model = template['model']\n",
    "                \n",
    "                model.set_weights(template['initial_weights'])\n",
    "                \n",
    "                history = model.fit(\n",
    "                    X_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stop],\n",
    "                    verbose=0,\n",
    "                    validation_split=0.1\n",
    "                )\n",
    "                \n",
    "                y_pred = model.predict(X_test, verbose=0).flatten()\n",
    "                \n",
    "                metrics = calculate_metrics(\n",
    "                    y_test, y_pred, \n",
    "                    model_name, ticker, set_name\n",
    "                )\n",
    "                \n",
    "                if metrics:\n",
    "                    batch_results.append(metrics)\n",
    "                \n",
    "                del history\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    # Clear session every 10 tickers\n",
    "    if (valid_tickers.index(ticker) + 1) % 10 == 0:\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "print(f\"\\nBatch 2 Training Complete. New results: {len(batch_results)}\")\n",
    "\n",
    "# Cleanup\n",
    "for template in model_templates.values():\n",
    "    del template['model']\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Append to existing results\n",
    "if batch_results:\n",
    "    new_results_df = pd.DataFrame(batch_results)\n",
    "    new_results_df['timestamp'] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Combine with existing\n",
    "    combined_df = pd.concat([existing_results_df, new_results_df], ignore_index=True)\n",
    "    \n",
    "    # Save updated CSV\n",
    "    combined_df.to_csv(output_dir / \"03B_Deep_Learning_Performance.csv\", index=False)\n",
    "    print(f\"Updated results file. Total results: {len(combined_df)}\")\n",
    "    \n",
    "    # Update database\n",
    "    try:\n",
    "        combined_df.to_sql(\n",
    "            'results_deep_learning_models',\n",
    "            con=engine,\n",
    "            if_exists='replace',\n",
    "            index=False,\n",
    "            chunksize=1000\n",
    "        )\n",
    "        print(\"Successfully updated database!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Database update error: {e}\")\n",
    "else:\n",
    "    print(\"No new results generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33146661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to fix database connection and upload existing results...\n",
      "Old engine disposed.\n",
      "New database engine created.\n",
      "Loaded 2352 rows from CSV.\n",
      "SUCCESS: All results successfully uploaded to the database.\n"
     ]
    }
   ],
   "source": [
    "# Cell: Fix Database Upload (Run this first)\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "print(\"Attempting to fix database connection and upload existing results...\")\n",
    "\n",
    "# 1. Force close the existing bad connection\n",
    "try:\n",
    "    engine.dispose()\n",
    "    print(\"Old engine disposed.\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 2. Re-create the engine\n",
    "db_url = f\"mysql+pymysql://{db_config['user']}:{db_config['password']}@{db_config['host']}/{db_config['database']}\"\n",
    "engine = create_engine(db_url)\n",
    "print(\"New database engine created.\")\n",
    "\n",
    "# 3. Load the CSV that was successfully saved on disk\n",
    "try:\n",
    "    csv_path = output_dir / \"03B_Deep_Learning_Performance.csv\"\n",
    "    current_results = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded {len(current_results)} rows from CSV.\")\n",
    "\n",
    "    # 4. Upload to Database\n",
    "    current_results.to_sql(\n",
    "        'results_deep_learning_models',\n",
    "        con=engine,\n",
    "        if_exists='replace',\n",
    "        index=False,\n",
    "        chunksize=1000\n",
    "    )\n",
    "    print(\"SUCCESS: All results successfully uploaded to the database.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR: Could not upload to database. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb80a4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Batch 3 Training.\n",
      "Currently trained on 196 tickers.\n",
      "Targeting next 100 tickers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bbe14a67c084039b73c01b3ed236ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Tickers:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid tickers to train: 97\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00cf7ac3fb84866939ed45628415da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batch 3:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3 Complete. Generated 1164 new results.\n",
      "CSV updated. Total rows: 3516\n",
      "Database successfully updated with Batch 3.\n"
     ]
    }
   ],
   "source": [
    "# Cell: Train Batch 3 (Tickers 200-300)\n",
    "print(\"Starting Batch 3 Training.\")\n",
    "\n",
    "# 1. Refresh the list of what is already finished\n",
    "existing_results_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
    "trained_tickers = set(existing_results_df['ticker'].unique())\n",
    "print(f\"Currently trained on {len(trained_tickers)} tickers.\")\n",
    "\n",
    "# 2. Calculate the next batch\n",
    "all_tickers = sorted(df['ticker'].unique())\n",
    "remaining_tickers = [t for t in all_tickers if t not in trained_tickers]\n",
    "\n",
    "# Grab the next 100\n",
    "BATCH_SIZE = 100\n",
    "next_batch = remaining_tickers[:BATCH_SIZE]\n",
    "print(f\"Targeting next {len(next_batch)} tickers.\")\n",
    "\n",
    "# 3. Pre-filter for data availability\n",
    "valid_tickers = []\n",
    "for ticker in tqdm(next_batch, desc=\"Validating Tickers\"):\n",
    "    ticker_df = df[df['ticker'] == ticker]\n",
    "    if len(ticker_df) >= sequence_length + 100:\n",
    "        valid_tickers.append(ticker)\n",
    "\n",
    "print(f\"Valid tickers to train: {len(valid_tickers)}\")\n",
    "\n",
    "# 4. Rebuild Templates (Fresh start for memory safety)\n",
    "tf.keras.backend.clear_session()\n",
    "model_templates = {}\n",
    "for set_name, feature_cols in feature_sets.items():\n",
    "    input_shape = (sequence_length, len(feature_cols))\n",
    "    for model_name, builder_func in model_builders.items():\n",
    "        model = builder_func(input_shape)\n",
    "        model_templates[f\"{model_name}_{set_name}\"] = {\n",
    "            'model': model,\n",
    "            'initial_weights': model.get_weights(),\n",
    "            'input_shape': input_shape\n",
    "        }\n",
    "\n",
    "# 5. Training Loop\n",
    "batch_results = []\n",
    "for ticker in tqdm(valid_tickers, desc=\"Training Batch 3\"):\n",
    "    for set_name, feature_cols in feature_sets.items():\n",
    "        data = prepare_data_for_ticker(ticker, df, feature_cols, sequence_length=sequence_length)\n",
    "        if data is None: continue\n",
    "        \n",
    "        # Skip if validation set is too small\n",
    "        if len(data['X_test']) < 5: continue\n",
    "\n",
    "        for model_name in model_builders.keys():\n",
    "            try:\n",
    "                cache_key = f\"{model_name}_{set_name}\"\n",
    "                template = model_templates[cache_key]\n",
    "                model = template['model']\n",
    "                model.set_weights(template['initial_weights']) # Reset weights\n",
    "                \n",
    "                history = model.fit(\n",
    "                    data['X_train'], data['y_train'],\n",
    "                    epochs=50, batch_size=32, callbacks=[early_stop],\n",
    "                    verbose=0, validation_split=0.1\n",
    "                )\n",
    "                \n",
    "                y_pred = model.predict(data['X_test'], verbose=0).flatten()\n",
    "                metrics = calculate_metrics(data['y_test'], y_pred, model_name, ticker, set_name)\n",
    "                \n",
    "                if metrics: batch_results.append(metrics)\n",
    "                del history\n",
    "            except: continue\n",
    "            \n",
    "    # Memory management\n",
    "    if (valid_tickers.index(ticker) + 1) % 10 == 0:\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "# 6. Save and Upload \n",
    "print(f\"Batch 3 Complete. Generated {len(batch_results)} new results.\")\n",
    "\n",
    "if batch_results:\n",
    "    # Append to CSV\n",
    "    new_results_df = pd.DataFrame(batch_results)\n",
    "    new_results_df['timestamp'] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Reload full CSV to be safe, append, and save back\n",
    "    full_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
    "    combined_df = pd.concat([full_df, new_results_df], ignore_index=True)\n",
    "    combined_df.to_csv(output_dir / \"03B_Deep_Learning_Performance.csv\", index=False)\n",
    "    print(f\"CSV updated. Total rows: {len(combined_df)}\")\n",
    "    \n",
    "    # Upload to DB\n",
    "    try:\n",
    "        # Re-dispose engine one last time to be safe\n",
    "        engine.dispose()\n",
    "        engine = create_engine(db_url)\n",
    "        \n",
    "        combined_df.to_sql(\n",
    "            'results_deep_learning_models',\n",
    "            con=engine,\n",
    "            if_exists='replace',\n",
    "            index=False,\n",
    "            chunksize=1000\n",
    "        )\n",
    "        print(\"Database successfully updated with Batch 3.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading Batch 3 to DB: {e}\")\n",
    "\n",
    "# Cleanup\n",
    "for template in model_templates.values(): del template['model']\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ce558e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Batch 4 Training...\n",
      "Currently trained on 293 tickers.\n",
      "Targeting next 100 tickers (Batch 4).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a490d1c2d0496286e982d9e29a50af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Batch 4 Tickers:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid tickers to train: 95\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e697abaac5e44c8ca7d0985070c10e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batch 4:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4 Complete. Generated 1140 new results.\n",
      "CSV updated. Total rows: 4656\n",
      "Database successfully updated with Batch 4.\n"
     ]
    }
   ],
   "source": [
    "# Cell: Train Batch 4 \n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Starting Batch 4 Training...\")\n",
    "\n",
    "# 1. Refresh the list of what is already finished\n",
    "# We reload the CSV to ensure we skip everything done in Batch 3\n",
    "existing_results_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
    "trained_tickers = set(existing_results_df['ticker'].unique())\n",
    "print(f\"Currently trained on {len(trained_tickers)} tickers.\")\n",
    "\n",
    "# 2. Calculate the next batch\n",
    "all_tickers = sorted(df['ticker'].unique())\n",
    "remaining_tickers = [t for t in all_tickers if t not in trained_tickers]\n",
    "\n",
    "if not remaining_tickers:\n",
    "    print(\"No tickers left to train!\")\n",
    "else:\n",
    "    # Grab the next 100\n",
    "    BATCH_SIZE = 100\n",
    "    next_batch = remaining_tickers[:BATCH_SIZE]\n",
    "    print(f\"Targeting next {len(next_batch)} tickers (Batch 4).\")\n",
    "\n",
    "    # 3. Pre-filter for data availability\n",
    "    valid_tickers = []\n",
    "    for ticker in tqdm(next_batch, desc=\"Validating Batch 4 Tickers\"):\n",
    "        ticker_df = df[df['ticker'] == ticker]\n",
    "        if len(ticker_df) >= sequence_length + 100:\n",
    "            valid_tickers.append(ticker)\n",
    "\n",
    "    print(f\"Valid tickers to train: {len(valid_tickers)}\")\n",
    "\n",
    "    # 4. Rebuild Templates (Clear session to prevent memory leaks)\n",
    "    tf.keras.backend.clear_session()\n",
    "    model_templates = {}\n",
    "    for set_name, feature_cols in feature_sets.items():\n",
    "        input_shape = (sequence_length, len(feature_cols))\n",
    "        for model_name, builder_func in model_builders.items():\n",
    "            model = builder_func(input_shape)\n",
    "            model_templates[f\"{model_name}_{set_name}\"] = {\n",
    "                'model': model,\n",
    "                'initial_weights': model.get_weights(),\n",
    "                'input_shape': input_shape\n",
    "            }\n",
    "\n",
    "    # 5. Training Loop\n",
    "    batch_results = []\n",
    "    for ticker in tqdm(valid_tickers, desc=\"Training Batch 4\"):\n",
    "        for set_name, feature_cols in feature_sets.items():\n",
    "            data = prepare_data_for_ticker(ticker, df, feature_cols, sequence_length=sequence_length)\n",
    "            if data is None: continue\n",
    "            \n",
    "            if len(data['X_test']) < 5: continue\n",
    "\n",
    "            for model_name in model_builders.keys():\n",
    "                try:\n",
    "                    cache_key = f\"{model_name}_{set_name}\"\n",
    "                    template = model_templates[cache_key]\n",
    "                    model = template['model']\n",
    "                    model.set_weights(template['initial_weights']) \n",
    "                    \n",
    "                    history = model.fit(\n",
    "                        data['X_train'], data['y_train'],\n",
    "                        epochs=50, batch_size=32, callbacks=[early_stop],\n",
    "                        verbose=0, validation_split=0.1\n",
    "                    )\n",
    "                    \n",
    "                    y_pred = model.predict(data['X_test'], verbose=0).flatten()\n",
    "                    metrics = calculate_metrics(data['y_test'], y_pred, model_name, ticker, set_name)\n",
    "                    \n",
    "                    if metrics: batch_results.append(metrics)\n",
    "                    del history\n",
    "                except: continue\n",
    "                \n",
    "        # Periodic memory clear\n",
    "        if (valid_tickers.index(ticker) + 1) % 10 == 0:\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "    # 6. Save and Upload\n",
    "    print(f\"Batch 4 Complete. Generated {len(batch_results)} new results.\")\n",
    "\n",
    "    if batch_results:\n",
    "        # Append new results to CSV\n",
    "        new_results_df = pd.DataFrame(batch_results)\n",
    "        new_results_df['timestamp'] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        # Combine with full dataset\n",
    "        full_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
    "        combined_df = pd.concat([full_df, new_results_df], ignore_index=True)\n",
    "        combined_df.to_csv(output_dir / \"03B_Deep_Learning_Performance.csv\", index=False)\n",
    "        print(f\"CSV updated. Total rows: {len(combined_df)}\")\n",
    "        \n",
    "        # Upload to DB (Re-initializing engine to prevent transaction errors)\n",
    "        try:\n",
    "            engine.dispose()\n",
    "            engine = create_engine(db_url)\n",
    "            \n",
    "            combined_df.to_sql(\n",
    "                'results_deep_learning_models',\n",
    "                con=engine,\n",
    "                if_exists='replace',\n",
    "                index=False,\n",
    "                chunksize=1000\n",
    "            )\n",
    "            print(\"Database successfully updated with Batch 4.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading Batch 4 to DB: {e}\")\n",
    "\n",
    "    # Final Cleanup\n",
    "    for template in model_templates.values(): del template['model']\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0cc7ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessing Deep Learning Model Performance so far...\n",
      "Loaded 4656 results from c:\\Users\\18kyu\\Desktop\\Unishit\\IR\\4_Results\\Deep_Learning_Results\\03B_Deep_Learning_Performance.csv\n",
      "Unique tickers processed: 388\n",
      "\n",
      "--- Performance by Model Type ---\n",
      "                               rmse                      mae         directional_accuracy                 \n",
      "                               mean      std     min    mean     std                 mean     std      max\n",
      "model                                                                                                     \n",
      "BiLSTM_Price_Base_Sentiment  4.6981  17.6065  0.0303  0.9050  2.1015              47.9624  4.8294  56.9201\n",
      "BiLSTM_Price_Full_Sentiment  5.1470  18.0117  0.0193  1.1342  3.3328              48.5957  4.5864  58.4559\n",
      "BiLSTM_Price_Only            4.3789  17.6115  0.0065  0.7133  2.0284              47.7899  5.4870  65.2778\n",
      "CNN1D_Price_Base_Sentiment   5.7546  19.8138  0.0034  1.2758  5.8093              48.2089  4.9700  63.8889\n",
      "CNN1D_Price_Full_Sentiment   5.5045  21.1216  0.0154  0.9597  5.2780              47.9680  4.7966  58.6745\n",
      "CNN1D_Price_Only             6.6284  22.9567  0.0033  1.5858  8.7997              48.2445  4.7223  61.1111\n",
      "GRU_Price_Base_Sentiment     4.8960  17.8297  0.0163  0.9688  2.4744              48.1561  4.7652  58.3333\n",
      "GRU_Price_Full_Sentiment     5.1683  17.8912  0.0170  1.1716  3.3963              48.4557  5.1876  60.1852\n",
      "GRU_Price_Only               4.4322  17.6910  0.0141  0.8108  2.5258              48.1942  5.4274  71.4286\n",
      "LSTM_Price_Base_Sentiment    4.6523  17.6252  0.0184  0.8351  2.0529              48.2244  4.9713  59.5588\n",
      "LSTM_Price_Full_Sentiment    4.7646  17.6496  0.0239  0.8350  1.9366              47.9831  5.0038  57.5049\n",
      "LSTM_Price_Only              4.2102  17.4915  0.0085  0.6089  1.2542              48.2203  5.1415  58.3333\n",
      "\n",
      "--- Top 15 Models by RMSE ---\n",
      "                     model ticker     rmse  directional_accuracy\n",
      "          CNN1D_Price_Only    BIS 0.003272             51.983299\n",
      "CNN1D_Price_Base_Sentiment    ANY 0.003387             55.151515\n",
      "CNN1D_Price_Base_Sentiment   ANTH 0.004441             43.650794\n",
      "         BiLSTM_Price_Only    BIS 0.006539             48.643006\n",
      "           LSTM_Price_Only    BIS 0.008534             47.390397\n",
      "          CNN1D_Price_Only   ACFN 0.008976             35.294118\n",
      "            GRU_Price_Only    ANY 0.014147             56.060606\n",
      "          CNN1D_Price_Only   ACUR 0.015357             42.544732\n",
      "CNN1D_Price_Full_Sentiment   AUMN 0.015394             32.812500\n",
      "  GRU_Price_Base_Sentiment   ANTH 0.016339             41.865079\n",
      "            GRU_Price_Only   ANTH 0.016780             41.269841\n",
      "  GRU_Price_Full_Sentiment   ANTH 0.016979             44.642857\n",
      "CNN1D_Price_Full_Sentiment    AGQ 0.018065             52.436647\n",
      " LSTM_Price_Base_Sentiment   ANTH 0.018361             45.833333\n",
      "            GRU_Price_Only    BIS 0.018759             47.807933\n",
      "\n",
      "--- Comparison with Traditional Models ---\n",
      "Metric                    | Deep Learning   | Traditional    \n",
      "------------------------------------------------------------\n",
      "Best RMSE                 | 0.0033          | 0.0002         \n",
      "Avg Directional Acc %     | 48.17           | 49.71          \n",
      "Best Directional Acc %    | 71.43           | 94.03          \n"
     ]
    }
   ],
   "source": [
    "# Cell: Assess Performance (Current Progress ~400 tickers)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Assessing Deep Learning Model Performance so far...\")\n",
    "\n",
    "# 1. Load Results from CSV\n",
    "try:\n",
    "    results_path = output_dir / \"03B_Deep_Learning_Performance.csv\"\n",
    "    results_df = pd.read_csv(results_path)\n",
    "    print(f\"Loaded {len(results_df)} results from {results_path}\")\n",
    "    print(f\"Unique tickers processed: {results_df['ticker'].nunique()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading results: {e}\")\n",
    "    results_df = pd.DataFrame()\n",
    "\n",
    "if not results_df.empty:\n",
    "    # 2. Summary Statistics by Model Type\n",
    "    # We group by model to see which architecture performs best on average\n",
    "    summary = results_df.groupby('model').agg({\n",
    "        'rmse': ['mean', 'std', 'min'],\n",
    "        'mae': ['mean', 'std'],\n",
    "        'directional_accuracy': ['mean', 'std', 'max']\n",
    "    }).round(4)\n",
    "    \n",
    "    print(\"\\n--- Performance by Model Type ---\")\n",
    "    print(summary.to_string())\n",
    "    \n",
    "    # 3. Top 15 Models by RMSE\n",
    "    print(\"\\n--- Top 15 Models by RMSE ---\")\n",
    "    top_models = results_df.nsmallest(15, 'rmse')[['model', 'ticker', 'rmse', 'directional_accuracy']]\n",
    "    print(top_models.to_string(index=False))\n",
    "    \n",
    "    # 4. Comparison with Traditional Models (if available in DB)\n",
    "    try:\n",
    "        trad_results = pd.read_sql(\"SELECT * FROM results_ir_models\", con=engine)\n",
    "        \n",
    "        print(\"\\n--- Comparison with Traditional Models ---\")\n",
    "        print(f\"{'Metric':<25} | {'Deep Learning':<15} | {'Traditional':<15}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # RMSE Comparison\n",
    "        dl_min_rmse = results_df['rmse'].min()\n",
    "        trad_min_rmse = trad_results['rmse'].min()\n",
    "        print(f\"{'Best RMSE':<25} | {dl_min_rmse:<15.4f} | {trad_min_rmse:<15.4f}\")\n",
    "        \n",
    "        # Directional Accuracy Comparison\n",
    "        dl_avg_acc = results_df['directional_accuracy'].mean()\n",
    "        trad_avg_acc = trad_results['directional_accuracy'].mean()\n",
    "        print(f\"{'Avg Directional Acc %':<25} | {dl_avg_acc:<15.2f} | {trad_avg_acc:<15.2f}\")\n",
    "        \n",
    "        dl_best_acc = results_df['directional_accuracy'].max()\n",
    "        trad_best_acc = trad_results['directional_accuracy'].max()\n",
    "        print(f\"{'Best Directional Acc %':<25} | {dl_best_acc:<15.2f} | {trad_best_acc:<15.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n(Could not load traditional model results for comparison: {e})\")\n",
    "\n",
    "else:\n",
    "    print(\"No results found to analyze.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77d5f5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Batch 5 Training (Targeting next 200 tickers).\n",
      "Currently trained on 388 tickers.\n",
      "Targeting next 200 tickers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b7880013214b318416e4de9fa9737b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Tickers:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid tickers to train: 193\n",
      "WARNING:tensorflow:From C:\\Users\\18kyu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918a39512a2d461295b85ca3bd57169c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batch 5:   0%|          | 0/193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5 Complete. Generated 2316 new results.\n",
      "CSV updated. Total rows: 6972\n",
      "Database successfully updated.\n"
     ]
    }
   ],
   "source": [
    "# Cell: Train Batch 5 (Next 200 tickers -> Target Total ~600)\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "sequence_length = 60\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True, verbose=0)\n",
    "print(\"Starting Batch 5 Training (Targeting next 200 tickers).\")\n",
    "\n",
    "# 1. Identify what is already done\n",
    "try:\n",
    "    existing_results_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
    "    trained_tickers = set(existing_results_df['ticker'].unique())\n",
    "except:\n",
    "    trained_tickers = set()\n",
    "    existing_results_df = pd.DataFrame()\n",
    "\n",
    "print(f\"Currently trained on {len(trained_tickers)} tickers.\")\n",
    "\n",
    "# 2. Select next batch\n",
    "all_tickers = sorted(df['ticker'].unique())\n",
    "remaining_tickers = [t for t in all_tickers if t not in trained_tickers]\n",
    "\n",
    "if not remaining_tickers:\n",
    "    print(\"All tickers have been trained!\")\n",
    "else:\n",
    "    BATCH_SIZE = 200  # Training 200 tickers in this batch\n",
    "    next_batch = remaining_tickers[:BATCH_SIZE]\n",
    "    print(f\"Targeting next {len(next_batch)} tickers.\")\n",
    "\n",
    "    # 3. Validate Data Availability\n",
    "    valid_tickers = []\n",
    "    for ticker in tqdm(next_batch, desc=\"Validating Tickers\"):\n",
    "        ticker_df = df[df['ticker'] == ticker]\n",
    "        # Check if enough data exists (Sequence length + buffer)\n",
    "        if len(ticker_df) >= sequence_length + 100:\n",
    "            valid_tickers.append(ticker)\n",
    "\n",
    "    print(f\"Valid tickers to train: {len(valid_tickers)}\")\n",
    "\n",
    "    # 4. Rebuild Model Templates (Fresh session)\n",
    "    tf.keras.backend.clear_session()\n",
    "    model_templates = {}\n",
    "    for set_name, feature_cols in feature_sets.items():\n",
    "        input_shape = (sequence_length, len(feature_cols))\n",
    "        for model_name, builder_func in model_builders.items():\n",
    "            model = builder_func(input_shape)\n",
    "            model_templates[f\"{model_name}_{set_name}\"] = {\n",
    "                'model': model,\n",
    "                'initial_weights': model.get_weights(),\n",
    "                'input_shape': input_shape\n",
    "            }\n",
    "\n",
    "    # 5. Training Loop\n",
    "    batch_results = []\n",
    "    for ticker in tqdm(valid_tickers, desc=\"Training Batch 5\"):\n",
    "        for set_name, feature_cols in feature_sets.items():\n",
    "            data = prepare_data_for_ticker(ticker, df, feature_cols, sequence_length=sequence_length)\n",
    "            if data is None: continue\n",
    "            if len(data['X_test']) < 5: continue # Skip small test sets\n",
    "\n",
    "            for model_name in model_builders.keys():\n",
    "                try:\n",
    "                    cache_key = f\"{model_name}_{set_name}\"\n",
    "                    template = model_templates[cache_key]\n",
    "                    model = template['model']\n",
    "                    model.set_weights(template['initial_weights']) # Reset\n",
    "                    \n",
    "                    history = model.fit(\n",
    "                        data['X_train'], data['y_train'],\n",
    "                        epochs=50, batch_size=32, callbacks=[early_stop],\n",
    "                        verbose=0, validation_split=0.1\n",
    "                    )\n",
    "                    \n",
    "                    y_pred = model.predict(data['X_test'], verbose=0).flatten()\n",
    "                    metrics = calculate_metrics(data['y_test'], y_pred, model_name, ticker, set_name)\n",
    "                    \n",
    "                    if metrics: batch_results.append(metrics)\n",
    "                    del history\n",
    "                except: continue\n",
    "        \n",
    "        # Memory Cleanup (every 10 tickers)\n",
    "        if (valid_tickers.index(ticker) + 1) % 10 == 0:\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "    # 6. Save Results\n",
    "    print(f\"Batch 5 Complete. Generated {len(batch_results)} new results.\")\n",
    "    \n",
    "    if batch_results:\n",
    "        new_results_df = pd.DataFrame(batch_results)\n",
    "        new_results_df['timestamp'] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        # Merge and Save CSV\n",
    "        full_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
    "        combined_df = pd.concat([full_df, new_results_df], ignore_index=True)\n",
    "        combined_df.to_csv(output_dir / \"03B_Deep_Learning_Performance.csv\", index=False)\n",
    "        print(f\"CSV updated. Total rows: {len(combined_df)}\")\n",
    "        \n",
    "        # Upload to DB (Re-init engine to act as a fresh transaction)\n",
    "        try:\n",
    "            engine.dispose()\n",
    "            engine = create_engine(db_url)\n",
    "            combined_df.to_sql(\n",
    "                'results_deep_learning_models', \n",
    "                con=engine, \n",
    "                if_exists='replace', \n",
    "                index=False, \n",
    "                chunksize=1000\n",
    "            )\n",
    "            print(\"Database successfully updated.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading to DB: {e}\")\n",
    "\n",
    "    # Final Cleanup\n",
    "    for template in model_templates.values(): del template['model']\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14cf6430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Batch 6 Training (Targeting next 200 tickers).\n",
      "Currently trained on 581 tickers.\n",
      "Targeting next 200 tickers (Batch 6).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0336e65f73b04cbeb68f055466485a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Batch 6 Tickers:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid tickers to train: 192\n",
      "WARNING:tensorflow:From C:\\Users\\18kyu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418b5c5316ef432cbbac182db6e939c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batch 6:   0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6 Complete. Generated 2304 new results.\n",
      "CSV updated. Total rows: 9276\n",
      "Database successfully updated.\n"
     ]
    }
   ],
   "source": [
    "# Cell: Train Batch 6 (Next 200 tickers)\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Configuration\n",
    "sequence_length = 60\n",
    "BATCH_SIZE = 200\n",
    "early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True, verbose=0)\n",
    "\n",
    "print(\"Starting Batch 6 Training (Targeting next 200 tickers).\")\n",
    "\n",
    "# 1. Identify what is already done\n",
    "try:\n",
    "    existing_results_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
    "    trained_tickers = set(existing_results_df['ticker'].unique())\n",
    "except Exception as e:\n",
    "    print(f\"Could not load existing results: {e}\")\n",
    "    trained_tickers = set()\n",
    "    existing_results_df = pd.DataFrame()\n",
    "\n",
    "print(f\"Currently trained on {len(trained_tickers)} tickers.\")\n",
    "\n",
    "# 2. Select next batch\n",
    "all_tickers = sorted(df['ticker'].unique())\n",
    "remaining_tickers = [t for t in all_tickers if t not in trained_tickers]\n",
    "\n",
    "if not remaining_tickers:\n",
    "    print(\"All tickers have been trained! No more batches needed.\")\n",
    "else:\n",
    "    next_batch = remaining_tickers[:BATCH_SIZE]\n",
    "    print(f\"Targeting next {len(next_batch)} tickers (Batch 6).\")\n",
    "\n",
    "    # 3. Validate Data Availability\n",
    "    valid_tickers = []\n",
    "    for ticker in tqdm(next_batch, desc=\"Validating Batch 6 Tickers\"):\n",
    "        ticker_df = df[df['ticker'] == ticker]\n",
    "        if len(ticker_df) >= sequence_length + 100:\n",
    "            valid_tickers.append(ticker)\n",
    "\n",
    "    print(f\"Valid tickers to train: {len(valid_tickers)}\")\n",
    "\n",
    "    # 4. Rebuild Model Templates (Fresh session to clear memory)\n",
    "    tf.keras.backend.clear_session()\n",
    "    model_templates = {}\n",
    "    for set_name, feature_cols in feature_sets.items():\n",
    "        input_shape = (sequence_length, len(feature_cols))\n",
    "        for model_name, builder_func in model_builders.items():\n",
    "            model = builder_func(input_shape)\n",
    "            model_templates[f\"{model_name}_{set_name}\"] = {\n",
    "                'model': model,\n",
    "                'initial_weights': model.get_weights(),\n",
    "                'input_shape': input_shape\n",
    "            }\n",
    "\n",
    "    # 5. Training Loop\n",
    "    batch_results = []\n",
    "    for ticker in tqdm(valid_tickers, desc=\"Training Batch 6\"):\n",
    "        for set_name, feature_cols in feature_sets.items():\n",
    "            data = prepare_data_for_ticker(ticker, df, feature_cols, sequence_length=sequence_length)\n",
    "            if data is None: continue\n",
    "            if len(data['X_test']) < 5: continue\n",
    "\n",
    "            for model_name in model_builders.keys():\n",
    "                try:\n",
    "                    cache_key = f\"{model_name}_{set_name}\"\n",
    "                    template = model_templates[cache_key]\n",
    "                    model = template['model']\n",
    "                    \n",
    "                    # Reset weights to initial state\n",
    "                    model.set_weights(template['initial_weights'])\n",
    "                    \n",
    "                    history = model.fit(\n",
    "                        data['X_train'], data['y_train'],\n",
    "                        epochs=50, batch_size=32, callbacks=[early_stop],\n",
    "                        verbose=0, validation_split=0.1\n",
    "                    )\n",
    "                    \n",
    "                    y_pred = model.predict(data['X_test'], verbose=0).flatten()\n",
    "                    metrics = calculate_metrics(data['y_test'], y_pred, model_name, ticker, set_name)\n",
    "                    \n",
    "                    if metrics: batch_results.append(metrics)\n",
    "                    del history\n",
    "                except: continue\n",
    "        \n",
    "        # Memory Cleanup (every 10 tickers)\n",
    "        if (valid_tickers.index(ticker) + 1) % 10 == 0:\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "    # 6. Save Results\n",
    "    print(f\"Batch 6 Complete. Generated {len(batch_results)} new results.\")\n",
    "    \n",
    "    if batch_results:\n",
    "        new_results_df = pd.DataFrame(batch_results)\n",
    "        new_results_df['timestamp'] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        # Merge and Save CSV\n",
    "        try:\n",
    "            full_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
    "            combined_df = pd.concat([full_df, new_results_df], ignore_index=True)\n",
    "            combined_df.to_csv(output_dir / \"03B_Deep_Learning_Performance.csv\", index=False)\n",
    "            print(f\"CSV updated. Total rows: {len(combined_df)}\")\n",
    "            \n",
    "            # Upload to DB (Re-init engine to act as a fresh transaction)\n",
    "            engine.dispose()\n",
    "            engine = create_engine(db_url)\n",
    "            combined_df.to_sql(\n",
    "                'results_deep_learning_models', \n",
    "                con=engine, \n",
    "                if_exists='replace', \n",
    "                index=False, \n",
    "                chunksize=1000\n",
    "            )\n",
    "            print(\"Database successfully updated.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving/uploading results: {e}\")\n",
    "\n",
    "    # Final Cleanup\n",
    "    for template in model_templates.values(): del template['model']\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f254db27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Batch 7 Training (Targeting next 200 tickers).\n",
      "Currently trained on 773 tickers.\n",
      "Targeting next 200 tickers (Batch 7).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491d8bf50b33467bb43296ec06b45505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Batch 7 Tickers:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid tickers to train: 190\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15265a105a34dadba933557bd2a288b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batch 7:   0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7 Complete. Generated 2280 new results.\n",
      "CSV updated. Total rows: 11556\n",
      "Database successfully updated.\n"
     ]
    }
   ],
   "source": [
    "# Cell: Train Batch 7 (Tickers 800-1000)\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Configuration\n",
    "sequence_length = 60\n",
    "BATCH_SIZE = 200  # Target tickers 800-1000\n",
    "early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True, verbose=0)\n",
    "\n",
    "print(\"Starting Batch 7 Training (Targeting next 200 tickers).\")\n",
    "\n",
    "# 1. Identify what is already done\n",
    "try:\n",
    "    existing_results_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
    "    trained_tickers = set(existing_results_df['ticker'].unique())\n",
    "except Exception as e:\n",
    "    print(f\"Could not load existing results: {e}\")\n",
    "    trained_tickers = set()\n",
    "    existing_results_df = pd.DataFrame()\n",
    "\n",
    "print(f\"Currently trained on {len(trained_tickers)} tickers.\")\n",
    "\n",
    "# 2. Select next batch\n",
    "all_tickers = sorted(df['ticker'].unique())\n",
    "remaining_tickers = [t for t in all_tickers if t not in trained_tickers]\n",
    "\n",
    "if not remaining_tickers:\n",
    "    print(\"All tickers have been trained! No more batches needed.\")\n",
    "else:\n",
    "    next_batch = remaining_tickers[:BATCH_SIZE]\n",
    "    print(f\"Targeting next {len(next_batch)} tickers (Batch 7).\")\n",
    "\n",
    "    # 3. Validate Data Availability\n",
    "    valid_tickers = []\n",
    "    for ticker in tqdm(next_batch, desc=\"Validating Batch 7 Tickers\"):\n",
    "        ticker_df = df[df['ticker'] == ticker]\n",
    "        if len(ticker_df) >= sequence_length + 100:\n",
    "            valid_tickers.append(ticker)\n",
    "\n",
    "    print(f\"Valid tickers to train: {len(valid_tickers)}\")\n",
    "\n",
    "    # 4. Rebuild Model Templates (Fresh session to clear memory)\n",
    "    tf.keras.backend.clear_session()\n",
    "    model_templates = {}\n",
    "    for set_name, feature_cols in feature_sets.items():\n",
    "        input_shape = (sequence_length, len(feature_cols))\n",
    "        for model_name, builder_func in model_builders.items():\n",
    "            model = builder_func(input_shape)\n",
    "            model_templates[f\"{model_name}_{set_name}\"] = {\n",
    "                'model': model,\n",
    "                'initial_weights': model.get_weights(),\n",
    "                'input_shape': input_shape\n",
    "            }\n",
    "\n",
    "    # 5. Training Loop\n",
    "    batch_results = []\n",
    "    for ticker in tqdm(valid_tickers, desc=\"Training Batch 7\"):\n",
    "        for set_name, feature_cols in feature_sets.items():\n",
    "            data = prepare_data_for_ticker(ticker, df, feature_cols, sequence_length=sequence_length)\n",
    "            if data is None: continue\n",
    "            if len(data['X_test']) < 5: continue\n",
    "\n",
    "            for model_name in model_builders.keys():\n",
    "                try:\n",
    "                    cache_key = f\"{model_name}_{set_name}\"\n",
    "                    template = model_templates[cache_key]\n",
    "                    model = template['model']\n",
    "                    \n",
    "                    # Reset weights to initial state\n",
    "                    model.set_weights(template['initial_weights'])\n",
    "                    \n",
    "                    history = model.fit(\n",
    "                        data['X_train'], data['y_train'],\n",
    "                        epochs=50, batch_size=32, callbacks=[early_stop],\n",
    "                        verbose=0, validation_split=0.1\n",
    "                    )\n",
    "                    \n",
    "                    y_pred = model.predict(data['X_test'], verbose=0).flatten()\n",
    "                    metrics = calculate_metrics(data['y_test'], y_pred, model_name, ticker, set_name)\n",
    "                    \n",
    "                    if metrics: batch_results.append(metrics)\n",
    "                    del history\n",
    "                except: continue\n",
    "        \n",
    "        # Memory Cleanup (every 10 tickers)\n",
    "        if (valid_tickers.index(ticker) + 1) % 10 == 0:\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "    # 6. Save Results\n",
    "    print(f\"Batch 7 Complete. Generated {len(batch_results)} new results.\")\n",
    "    \n",
    "    if batch_results:\n",
    "        new_results_df = pd.DataFrame(batch_results)\n",
    "        new_results_df['timestamp'] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        # Merge and Save CSV\n",
    "        try:\n",
    "            full_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
    "            combined_df = pd.concat([full_df, new_results_df], ignore_index=True)\n",
    "            combined_df.to_csv(output_dir / \"03B_Deep_Learning_Performance.csv\", index=False)\n",
    "            print(f\"CSV updated. Total rows: {len(combined_df)}\")\n",
    "            \n",
    "            # Upload to DB (Re-init engine to act as a fresh transaction)\n",
    "            engine.dispose()\n",
    "            engine = create_engine(db_url)\n",
    "            combined_df.to_sql(\n",
    "                'results_deep_learning_models', \n",
    "                con=engine, \n",
    "                if_exists='replace', \n",
    "                index=False, \n",
    "                chunksize=1000\n",
    "            )\n",
    "            print(\"Database successfully updated.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving/uploading results: {e}\")\n",
    "\n",
    "    # Final Cleanup\n",
    "    for template in model_templates.values(): del template['model']\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f51c6a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Batch 8 Training (Targeting next 200 tickers).\n",
      "Currently trained on 963 tickers.\n",
      "Targeting next 200 tickers (Batch 8).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805d8a336e054cde868b5ae4ed39ae3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Batch 8 Tickers:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid tickers to train: 190\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f7e65c2bc34b70857b12905936af04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batch 8:   0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8 Complete. Generated 2280 new results.\n",
      "CSV updated. Total rows: 13836\n",
      "Database successfully updated.\n"
     ]
    }
   ],
   "source": [
    "# Cell: Train Batch 8 (Tickers 1000-1200)\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Configuration\n",
    "sequence_length = 60\n",
    "BATCH_SIZE = 200  # Target next 200 tickers\n",
    "early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True, verbose=0)\n",
    "\n",
    "print(\"Starting Batch 8 Training (Targeting next 200 tickers).\")\n",
    "\n",
    "# 1. Identify what is already done\n",
    "try:\n",
    "    existing_results_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
    "    trained_tickers = set(existing_results_df['ticker'].unique())\n",
    "except Exception as e:\n",
    "    print(f\"Could not load existing results: {e}\")\n",
    "    trained_tickers = set()\n",
    "    existing_results_df = pd.DataFrame()\n",
    "\n",
    "print(f\"Currently trained on {len(trained_tickers)} tickers.\")\n",
    "\n",
    "# 2. Select next batch\n",
    "all_tickers = sorted(df['ticker'].unique())\n",
    "remaining_tickers = [t for t in all_tickers if t not in trained_tickers]\n",
    "\n",
    "if not remaining_tickers:\n",
    "    print(\"All tickers have been trained! No more batches needed.\")\n",
    "else:\n",
    "    next_batch = remaining_tickers[:BATCH_SIZE]\n",
    "    print(f\"Targeting next {len(next_batch)} tickers (Batch 8).\")\n",
    "\n",
    "    # 3. Validate Data Availability\n",
    "    valid_tickers = []\n",
    "    for ticker in tqdm(next_batch, desc=\"Validating Batch 8 Tickers\"):\n",
    "        ticker_df = df[df['ticker'] == ticker]\n",
    "        if len(ticker_df) >= sequence_length + 100:\n",
    "            valid_tickers.append(ticker)\n",
    "\n",
    "    print(f\"Valid tickers to train: {len(valid_tickers)}\")\n",
    "\n",
    "    # 4. Rebuild Model Templates (Fresh session to clear memory)\n",
    "    tf.keras.backend.clear_session()\n",
    "    model_templates = {}\n",
    "    for set_name, feature_cols in feature_sets.items():\n",
    "        input_shape = (sequence_length, len(feature_cols))\n",
    "        for model_name, builder_func in model_builders.items():\n",
    "            model = builder_func(input_shape)\n",
    "            model_templates[f\"{model_name}_{set_name}\"] = {\n",
    "                'model': model,\n",
    "                'initial_weights': model.get_weights(),\n",
    "                'input_shape': input_shape\n",
    "            }\n",
    "\n",
    "    # 5. Training Loop\n",
    "    batch_results = []\n",
    "    for ticker in tqdm(valid_tickers, desc=\"Training Batch 8\"):\n",
    "        for set_name, feature_cols in feature_sets.items():\n",
    "            data = prepare_data_for_ticker(ticker, df, feature_cols, sequence_length=sequence_length)\n",
    "            if data is None: continue\n",
    "            if len(data['X_test']) < 5: continue\n",
    "\n",
    "            for model_name in model_builders.keys():\n",
    "                try:\n",
    "                    cache_key = f\"{model_name}_{set_name}\"\n",
    "                    template = model_templates[cache_key]\n",
    "                    model = template['model']\n",
    "                    \n",
    "                    # Reset weights to initial state\n",
    "                    model.set_weights(template['initial_weights'])\n",
    "                    \n",
    "                    history = model.fit(\n",
    "                        data['X_train'], data['y_train'],\n",
    "                        epochs=50, batch_size=32, callbacks=[early_stop],\n",
    "                        verbose=0, validation_split=0.1\n",
    "                    )\n",
    "                    \n",
    "                    y_pred = model.predict(data['X_test'], verbose=0).flatten()\n",
    "                    metrics = calculate_metrics(data['y_test'], y_pred, model_name, ticker, set_name)\n",
    "                    \n",
    "                    if metrics: batch_results.append(metrics)\n",
    "                    del history\n",
    "                except: continue\n",
    "        \n",
    "        # Memory Cleanup (every 10 tickers)\n",
    "        if (valid_tickers.index(ticker) + 1) % 10 == 0:\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "    # 6. Save Results\n",
    "    print(f\"Batch 8 Complete. Generated {len(batch_results)} new results.\")\n",
    "    \n",
    "    if batch_results:\n",
    "        new_results_df = pd.DataFrame(batch_results)\n",
    "        new_results_df['timestamp'] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        # Merge and Save CSV\n",
    "        try:\n",
    "            full_df = pd.read_csv(output_dir / \"03B_Deep_Learning_Performance.csv\")\n",
    "            combined_df = pd.concat([full_df, new_results_df], ignore_index=True)\n",
    "            combined_df.to_csv(output_dir / \"03B_Deep_Learning_Performance.csv\", index=False)\n",
    "            print(f\"CSV updated. Total rows: {len(combined_df)}\")\n",
    "            \n",
    "            # Upload to DB (Re-init engine to act as a fresh transaction)\n",
    "            engine.dispose()\n",
    "            engine = create_engine(db_url)\n",
    "            combined_df.to_sql(\n",
    "                'results_deep_learning_models', \n",
    "                con=engine, \n",
    "                if_exists='replace', \n",
    "                index=False, \n",
    "                chunksize=1000\n",
    "            )\n",
    "            print(\"Database successfully updated.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving/uploading results: {e}\")\n",
    "\n",
    "    # Final Cleanup\n",
    "    for template in model_templates.values(): del template['model']\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cde3aab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
